{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2v2: LightGBM with Robust Time-Series Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pdb\n",
    "import os\n",
    "import gc; gc.enable()\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Function for loading h5py file\n",
    "def load_h5py(fname):\n",
    "    with h5py.File(fname, 'r') as handle:\n",
    "        return handle['data'][:]\n",
    "# Function for loading pickle file\n",
    "def load_pickle(fname):\n",
    "    with open(fname, 'rb') as handle:\n",
    "        return pickle.load(handle)\n",
    "\n",
    "\n",
    "# Function for setting up\n",
    "def get_input(debug=False):\n",
    "    '''\n",
    "    Function for loading either debug or full datasets\n",
    "    '''\n",
    "    os.chdir('../data/compressed/')\n",
    "    print os.getcwd()\n",
    "    pkl_files = ['train_id.pickle', 'trainidx.pickle', 'target.pickle', 'test_id.pickle', 'testidx.pickle']\n",
    "    if debug:\n",
    "        print 'Loading debug train and test datasets...'\n",
    "        # h5py files\n",
    "        train = load_h5py('debug_train.h5')\n",
    "        test = load_h5py('debug_test.h5')\n",
    "        # pickle files\n",
    "        id_train, train_idx, target, id_test, test_idx = [load_pickle('debug_%s'%f) for f in pkl_files]\n",
    "    else:\n",
    "        print 'Loading original train and test datasets...'\n",
    "        # h5py files\n",
    "        train = load_h5py('full_train.h5')\n",
    "        test = load_h5py('full_test.h5')\n",
    "        # pickle files\n",
    "        id_train, train_idx, target, id_test, test_idx = [load_pickle('full_%s'%f) for f in pkl_files]\n",
    "    # Load feature names\n",
    "    fnames = load_pickle('feature_names.pickle')\n",
    "    # Find shape of loaded datasets\n",
    "    print('Shape of training dataset: {} Rows, {} Columns'.format(*train.shape))\n",
    "    print('Shape of test dataset: {} Rows, {} Columns'.format(*test.shape))\n",
    "    os.chdir('../../scripts/')\n",
    "    print os.getcwd()\n",
    "    return fnames, train, id_train, train_idx, target, test, id_test, test_idx\n",
    "\n",
    "\n",
    "# Function for getting datasets in dataframe format\n",
    "def get_dataframes(debug=False):\n",
    "    # Load data\n",
    "    fnames, train, id_train, train_idx, target, test, id_test, test_idx = get_input(debug)\n",
    "    # Format data\n",
    "    train_df = pd.DataFrame(data=train, index=train_idx, columns=fnames)\n",
    "    train_df['ID'] = id_train\n",
    "    train_df['target'] = target\n",
    "    test_df = pd.DataFrame(data=test, index=test_idx, columns=fnames)\n",
    "    test_df['ID'] = id_test\n",
    "    \n",
    "    print('\\nShape of training dataframe: {} Rows, {} Columns'.format(*train_df.shape))\n",
    "    print('Shape of test dataframe: {} Rows, {} Columns'.format(*test_df.shape))\n",
    "    return fnames, train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading leaks\n",
    "def load_leaks(leak_val):\n",
    "    leak_dir = './time_series/stats/'\n",
    "    \n",
    "    train_leak_loc = leak_dir + 'train_leak_%s.csv'%leak_val\n",
    "    train_leak = pd.read_csv(train_leak_loc).compiled_leak\n",
    "    test_leak_loc = leak_dir + 'test_leak_%s.csv'%leak_val\n",
    "    test_leak = pd.read_csv(test_leak_loc).compiled_leak\n",
    "    \n",
    "    return train_leak, test_leak\n",
    "\n",
    "\n",
    "# Function for applying statistical transformations to data\n",
    "def calculate_metadata(df):\n",
    "    '''\n",
    "    Function for calculating metadata across pandas dataframe row\n",
    "    '''\n",
    "    meta = pd.DataFrame()\n",
    "    # Calculations that disregard zeros\n",
    "    meta['nz_mean'] = df.apply(lambda x: x[x!=0].mean(), axis=1)\n",
    "    meta['nz_log_mean_exp'] = df.apply(lambda x: np.expm1(np.mean(np.log1p(x[x!=0]))), axis=1)\n",
    "    meta['nz_median'] = df.apply(lambda x: x[x!=0].median(), axis=1)\n",
    "    meta['nz_std'] = df.apply(lambda x: x[x!=0].std(), axis=1)\n",
    "    meta['nz_kurtosis'] = df.apply(lambda x: x[x!=0].kurtosis(), axis=1)\n",
    "    meta['nz_min'] = df.apply(lambda x: np.min(x[x!=0]), axis=1)\n",
    "    \n",
    "    # Calculations independent of zeros\n",
    "    meta['sum'] = df.apply(lambda x: np.sum(x), axis=1)\n",
    "    meta['max'] = df.apply(lambda x: np.max(x), axis=1)\n",
    "    \n",
    "    # Calculations factoring in zeros\n",
    "    meta['zero_count'] = df.apply(lambda x: np.count_nonzero(x==0), axis=1)\n",
    "    meta['mean'] = df.apply(lambda x: x.mean(), axis=1)\n",
    "    meta['log_mean_exp'] = df.apply(lambda x: np.expm1(np.mean(np.log1p(x))), axis=1)\n",
    "    meta['median'] = df.apply(lambda x: x.median(), axis=1)\n",
    "    meta['std'] = df.apply(lambda x: x.std(), axis=1)\n",
    "    meta['kurtosis'] = df.apply(lambda x: x.kurtosis(), axis=1)\n",
    "    \n",
    "    return meta \n",
    "\n",
    "\n",
    "# Function for feature engineering\n",
    "def format_for_training_v1(train, test, f, trn_leak, tst_leak, lagval=38):\n",
    "    '''\n",
    "    - Formats train and test dataframes for training\n",
    "    '''\n",
    "    tmp_trn = train.copy(deep=True)\n",
    "    tmp_trn['leak'] = trn_leak\n",
    "    tmp_trn['log_leak'] = np.log1p(tmp_trn['leak'])\n",
    "    \n",
    "    tmp_tst = test.copy(deep=True)\n",
    "    tmp_tst['leak'] = tst_leak\n",
    "    tmp_tst['log_leak'] = np.log1p(tmp_tst['leak'])\n",
    "    \n",
    "    score_name = './model_data/model_2v2_featscores_%s.csv'%lagval\n",
    "    print 'Loading file:\\n', score_name\n",
    "    score_df = pd.read_csv(score_name, index_col=0)\n",
    "    \n",
    "    # Select good features\n",
    "    if lagval==36:\n",
    "        threshold = 1.756\n",
    "    elif lagval==37:\n",
    "        threshold = 0.6255\n",
    "    else:\n",
    "        threshold = 0.625  # lag 38\n",
    "#     good_features = score_df.loc[score_df['rmse']<=threshold].index\n",
    "    feature_list = load_pickle('./time_series/aaron_test_v0.pickle')\n",
    "    good_features = []\n",
    "    for i in range(55):\n",
    "        good_features += feature_list[i]\n",
    "    \n",
    "    print '\\nLoading metadata for training set...'\n",
    "    trn_meta = pd.read_csv('./model_data/train_meta.csv')\n",
    "    print 'Loading metadata for test set...'\n",
    "    tst_meta = pd.read_csv('./model_data/test_meta.csv')\n",
    "    \n",
    "    # Format training and test datasets\n",
    "    cols = ['ID'] + list(good_features) + list(trn_meta.columns.values) + ['log_leak']\n",
    "    tmp_trn = pd.concat([tmp_trn, trn_meta], axis=1)\n",
    "    tmp_tst = pd.concat([tmp_tst, tst_meta], axis=1)\n",
    "    \n",
    "    return tmp_trn[cols], tmp_tst[cols]\n",
    "\n",
    "\n",
    "# Function for scaling datasets \n",
    "def scale_for_training(train, test):\n",
    "    print 'Scaling data...'\n",
    "    tmp_trn = train.copy(deep=True)\n",
    "    tmp_trn.replace(np.nan, 0, inplace=True)\n",
    "    tmp_tst = test.copy(deep=True)\n",
    "    tmp_tst.replace(np.nan, 0, inplace=True)\n",
    "    \n",
    "    tmp_trn.drop(labels=['ID'], axis=1, inplace=True)\n",
    "    tmp_tst.drop(labels=['ID'], axis=1, inplace=True)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaled_train = scaler.fit_transform(tmp_trn)\n",
    "    scaled_test = scaler.transform(tmp_tst)\n",
    "    \n",
    "    del tmp_trn, tmp_tst; gc.collect();\n",
    "    return scaled_train, scaled_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Script\n",
    "try:\n",
    "    del fnames, train, test\n",
    "    print 'Clearing loaded dataframes from memory...\\n'\n",
    "except:\n",
    "    pass\n",
    "fnames, train, test = get_dataframes(debug=False)\n",
    "\n",
    "# Load leaks\n",
    "leak_val = 38\n",
    "print '\\nLoading train and test leaks...'\n",
    "train_leak, test_leak = load_leaks(leak_val)\n",
    "print 'Nonzero elements in train:', np.count_nonzero(train_leak)\n",
    "print 'Nonzero elements in test:', np.count_nonzero(test_leak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and test in format for training booster\n",
    "# Format target variable\n",
    "target = train['target'].values\n",
    "target_log = np.log1p(target)\n",
    "\n",
    "pp_flag = False\n",
    "# train_name = './model_data/btrain_%s.csv'%leak_val\n",
    "# test_name = './model_data/btest_%s.csv'%leak_val\n",
    "\n",
    "train_name = './model_data/btrain_fullfeat_%s.csv'%leak_val\n",
    "test_name = './model_data/btest_fullfeat_%s.csv'%leak_val\n",
    "if pp_flag:\n",
    "    btrain, btest = format_for_training_v1(train, test, fnames, train_leak, test_leak, leak_val)\n",
    "    print '\\nSaving generated datasets...'\n",
    "    btrain.to_csv(train_name, index=False)\n",
    "    btest.to_csv(test_name, index=False)\n",
    "else:\n",
    "    print '\\nLoading generated datasets...'\n",
    "    btrain = pd.read_csv(train_name)\n",
    "    btest = pd.read_csv(test_name)\n",
    "\n",
    "# Scale dataset for booster training\n",
    "boost_train, boost_test = scale_for_training(btrain, btest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom evaluation metric for LGBM\n",
    "def RMSLE(preds, train_data):\n",
    "    return 'RMSLE', np.sqrt(mean_squared_error(train_data.get_label(), preds)), False\n",
    "\n",
    "\n",
    "# Function for evaluating mean errors\n",
    "def average_best_scores(scores):\n",
    "    s_cols = [c for c in scores.columns if c!='index']\n",
    "    errs = scores[s_cols].apply(lambda x: np.min(x[x!=np.nan]), axis=0)\n",
    "    return np.mean(errs)\n",
    "\n",
    "\n",
    "# Function for plotting RMSLE averaged over all iterations\n",
    "def plot_scores(t_scores, v_scores):\n",
    "    s_cols = [c for c in t_scores.columns if c!='index']\n",
    "    t_mean = np.mean(t_scores[s_cols], axis=1)\n",
    "    t_mean = t_mean[~np.isnan(t_mean)]\n",
    "    v_mean = np.mean(v_scores[s_cols], axis=1)\n",
    "    v_mean = v_mean[~np.isnan(v_mean)]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(v_mean, label='validation', c='orange', alpha=0.7)\n",
    "    plt.plot(t_mean, label='training', c='orange', alpha=0.7)\n",
    "    plt.title('Averaged Training and Validation Error')\n",
    "    plt.xlabel('Training Iteration')\n",
    "    plt.ylabel('RMSLE')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "\n",
    "# Function for training a LGB Regressor\n",
    "def train_lgb_regressor(train, target, test, params, n_boost=500):\n",
    "    num_boosting = n_boost\n",
    "    valid_result = pd.DataFrame(data=np.arange(num_boosting), columns=['index'])\n",
    "    train_result = pd.DataFrame(data=np.arange(num_boosting), columns=['index'])\n",
    "    test_predictions = np.zeros(test.shape[0])\n",
    "    train_predictions = np.zeros(train.shape[0])\n",
    "    kfold = KFold(n_splits=4)\n",
    "    \n",
    "    for i, (trn, val) in enumerate(kfold.split(train)):\n",
    "        print '\\nTraining on fold:', i\n",
    "        round_name = 'round_%s'%i\n",
    "        xtrain = train[trn, :]\n",
    "        ytrain = target[trn]\n",
    "\n",
    "        xval = train[val, :]\n",
    "        yval = target[val]\n",
    "\n",
    "        lgb_train = lgb.Dataset(xtrain, ytrain)\n",
    "        lgb_eval = lgb.Dataset(xval, yval, reference=lgb_train)\n",
    "\n",
    "        evals_result = {}\n",
    "        gbm = lgb.train(params,\n",
    "                        lgb_train,\n",
    "                        num_boost_round=num_boosting,\n",
    "                        valid_sets= (lgb_train, lgb_eval),\n",
    "                        verbose_eval=500,\n",
    "                        feval=RMSLE, \n",
    "                        evals_result=evals_result,\n",
    "                        early_stopping_rounds=100)\n",
    "\n",
    "        # Get evaluation results\n",
    "        valid_res_df = pd.DataFrame.from_dict(evals_result['valid_1'], orient='columns')\n",
    "        valid_res_df.rename(columns={'RMSLE': round_name}, inplace=True)\n",
    "        valid_res_df.reset_index(inplace=True)\n",
    "        train_res_df = pd.DataFrame.from_dict(evals_result['training'], orient='columns')\n",
    "        train_res_df.rename(columns={'RMSLE': round_name}, inplace=True)\n",
    "        train_res_df.reset_index(inplace=True)\n",
    "\n",
    "        valid_result = pd.merge(valid_result, valid_res_df, how='outer', on='index')\n",
    "        train_result = pd.merge(train_result, train_res_df, how='outer', on='index')\n",
    "\n",
    "        # Make predictions\n",
    "        test_predictions += (gbm.predict(test, num_iteration=gbm.best_iteration))/kfold.n_splits\n",
    "        train_predictions += (gbm.predict(train, num_iteration=gbm.best_iteration))/kfold.n_splits\n",
    "    \n",
    "    return train_result, valid_result, test_predictions, train_predictions\n",
    "\n",
    "\n",
    "# Function for evaluating trouble samples\n",
    "trouble_idx = np.where(train_leak==0)[0]\n",
    "def selective_eval(preds, target, idx):\n",
    "    pred_subset = preds[idx]\n",
    "    target_subset = target[idx]\n",
    "    return mean_squared_error(pred_subset, target_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGB Regressor Parameters\n",
    "params = {'task': 'train',\n",
    "          'boosting': 'gbdt',\n",
    "          'objective': 'regression',\n",
    "          'metric': 'RMSLE',\n",
    "          'num_leaves': 18,\n",
    "          'learning_rate': 0.05,\n",
    "          'max_depth': -1,\n",
    "          'feature_fraction': 0.9,\n",
    "          'bagging_fraction': 0.8,\n",
    "          'bagging_freq': 5,\n",
    "          'min_sum_hessian_in_leaf': 1e-3,\n",
    "          'lambda_l2': 10,\n",
    "          'verbose': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune LGB Regressor Hyper-Parameters\n",
    "tune_flag = False\n",
    "if tune_flag:\n",
    "    cv_results = {}\n",
    "\n",
    "#     cv_entry = 'num_leaves'\n",
    "#     cv_set = [23, 24]\n",
    "\n",
    "#     cv_entry = 'learning_rate'\n",
    "#     cv_set = [0.0005, 0.001]\n",
    "\n",
    "#     cv_entry = 'max_depth'\n",
    "#     cv_set = [-1, 9, 10, 11]\n",
    "\n",
    "#     cv_entry = 'feature_fraction'\n",
    "#     cv_set = [0.8, 0.99]\n",
    "\n",
    "#     cv_entry = 'bagging_fraction'\n",
    "#     cv_set = [0.70, 0.73, 0.75]\n",
    "\n",
    "#     cv_entry = 'min_sum_hessian_in_leaf'\n",
    "#     cv_set = [1e-4, 1e-3, 1e-2]\n",
    "\n",
    "#     cv_entry = 'lambda_l2'\n",
    "#     cv_set = [27, 30, 33]\n",
    "\n",
    "    for cv_val in cv_set:\n",
    "        print '\\nCross validating with %s: %s'%(cv_entry, cv_val)\n",
    "        params[cv_entry] = cv_val\n",
    "        train_res, valid_res, tst_preds, trn_preds = train_lgb_regressor(train=boost_train, \n",
    "                                                                         target=target_log, \n",
    "                                                                         test=boost_test, \n",
    "                                                                         params=params, \n",
    "                                                                         n_boost=30000)\n",
    "        cv_results[cv_entry+'miss_train_val_%s'%cv_val] = (selective_eval(trn_preds, target_log, trouble_idx),\n",
    "                                                           average_best_scores(train_res), \n",
    "                                                           average_best_scores(valid_res))\n",
    "        \n",
    "    # Show cv results\n",
    "    print '\\nCV Results:'\n",
    "    for key in cv_results.keys():\n",
    "        print key, cv_results[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm current parameter settings\n",
    "train_res, valid_res, tst_preds, trn_preds = train_lgb_regressor(train=boost_train, \n",
    "                                                                 target=target_log, \n",
    "                                                                 test=boost_test, \n",
    "                                                                 params=params, \n",
    "                                                                 n_boost=30000)\n",
    "train_score = average_best_scores(train_res)\n",
    "valid_score = average_best_scores(valid_res)\n",
    "print '\\nTraining score:', train_score\n",
    "print 'Validation score:', valid_score\n",
    "print '\\nTrouble score:', selective_eval(trn_preds, target_log, trouble_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target_v1(leak, preds):\n",
    "    exp_preds = np.expm1(preds)\n",
    "    \n",
    "    fill_idx = np.where(leak==0)[0]\n",
    "    \n",
    "    tmp_leak = leak.copy()\n",
    "    tmp_leak[fill_idx] = exp_preds[fill_idx]\n",
    "    return tmp_leak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make submission\n",
    "save_flag=True\n",
    "\n",
    "tst_target = make_target_v1(test_leak, tst_preds)\n",
    "\n",
    "ori_target = np.expm1(tst_preds)\n",
    "trn_target = np.expm1(trn_preds)\n",
    "\n",
    "sub_name_tst = '../submissions/tstest_lgb_2v2_lag%s_fullfeat_submit.csv'%leak_val\n",
    "\n",
    "sub_name_ori = './model_data/tstest_lgb_2v2_lag%s_fullfeat_test.csv'%leak_val\n",
    "sub_name_trn = './model_data/tstest_lgb_2v2_lag%s_fullfeat_train.csv'%leak_val\n",
    "\n",
    "tst_df = pd.DataFrame()\n",
    "ori_df = pd.DataFrame()\n",
    "trn_df = pd.DataFrame()\n",
    "\n",
    "tst_df['ID'] = test['ID']\n",
    "ori_df['ID'] = test['ID']\n",
    "trn_df['ID'] = train['ID']\n",
    "\n",
    "tst_df['target'] = tst_target\n",
    "ori_df['target'] = ori_target\n",
    "trn_df['target'] = trn_target\n",
    "\n",
    "if save_flag:\n",
    "    tst_df.to_csv(sub_name_tst, index=False)\n",
    "    ori_df.to_csv(sub_name_ori, index=False)\n",
    "    trn_df.to_csv(sub_name_trn, index=False)\n",
    "\n",
    "tst_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_log_diff(sub1, sub2):\n",
    "    return np.sqrt(np.mean(np.square(np.subtract(np.log1p(sub1), np.log1p(sub2)))))\n",
    "\n",
    "fri_df = pd.read_csv('../submissions/tstest_lgb_2v2_lag36_submit_0.52.csv')\n",
    "sat_df = pd.read_csv('../submissions/tstest_lgb_2v2_lag38_bad_submit_0.52.csv')\n",
    "sun_df = pd.read_csv('../submissions/tstest_lgb_2v2_lag38_good_submit.csv')\n",
    "\n",
    "right_idx = np.where(test_leak!=0)[0]\n",
    "wrong_idx = np.where(test_leak==0)[0]\n",
    "\n",
    "sub_right = tst_df['target'].values[right_idx]\n",
    "sun_right = sun_df['target'].values[right_idx]\n",
    "fri_right = fri_df['target'].values[right_idx]\n",
    "sat_right = sat_df['target'].values[right_idx]\n",
    "\n",
    "sub_wrong = tst_df['target'].values[wrong_idx]\n",
    "sun_wrong = sun_df['target'].values[wrong_idx]\n",
    "fri_wrong = fri_df['target'].values[wrong_idx]\n",
    "sat_wrong = sat_df['target'].values[wrong_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print mean_log_diff(sun_right, sub_right)\n",
    "print mean_log_diff(sun_wrong, sub_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexicons():\n",
    "    lexi_train = load_pickle('./model_data/lexi_train.pickle')\n",
    "    lexi_test = load_pickle('./model_data/lexi_test.pickle')\n",
    "    return lexi_train, lexi_test\n",
    "\n",
    "\n",
    "def preds_to_lexicon(preds, lexicon):\n",
    "    new_preds = np.zeros(preds.shape[0])\n",
    "    \n",
    "    lex_matrix = lexicon * np.ones((preds.shape[0], lexicon.shape[0]))\n",
    "    \n",
    "    diff = (lex_matrix.T - preds).T\n",
    "    diff = np.abs(diff)\n",
    "    mins = np.argmin(diff, axis=1)\n",
    "    \n",
    "    return lexicon[mins]\n",
    "\n",
    "\n",
    "def train_lexicon(data, target, leak, preds, lexi):\n",
    "    lexi_transform = preds_to_lexicon(preds, lexi)\n",
    "    \n",
    "    lexi_df = pd.DataFrame()\n",
    "    lexi_df['ID'] = data['ID']\n",
    "    lexi_df['target'] = target\n",
    "    lexi_df['leak'] = leak\n",
    "    lexi_df['pred'] = preds\n",
    "    lexi_df['lexi_pred'] = lexi_transform\n",
    "    \n",
    "    lexi_df['SLE'] = np.square(np.subtract(np.log1p(lexi_df['target']), np.log1p(lexi_df['pred'])))\n",
    "    lexi_df['lexi_SLE'] = np.square(np.subtract(np.log1p(lexi_df['target']), np.log1p(lexi_df['lexi_pred'])))\n",
    "    lexi_df['SLE_diff'] = np.abs(np.subtract(lexi_df['SLE'], lexi_df['lexi_SLE']))\n",
    "    \n",
    "    return lexi_df\n",
    "\n",
    "\n",
    "def test_lexicon(data, leak, preds, lexi):\n",
    "    lexi_transform = preds_to_lexicon(preds, lexi)\n",
    "    \n",
    "    lexi_df = pd.DataFrame()\n",
    "    lexi_df['ID'] = data['ID']\n",
    "    lexi_df['leak'] = leak\n",
    "    lexi_df['pred'] = preds\n",
    "    lexi_df['lexi_pred'] = lexi_transform\n",
    "    \n",
    "    lexi_df['pred_abs_diff'] = np.abs(np.subtract(lexi_df['pred'], lexi_df['lexi_red']))\n",
    "    \n",
    "    return lexi_df \n",
    "\n",
    "\n",
    "def get_errors(lexi):\n",
    "    zero_idx = lexi['leak']==0\n",
    "    \n",
    "    pred_sle_missed = lexi.loc[zero_idx, 'SLE']\n",
    "    lexi_sle_missed = lexi.loc[zero_idx, 'lexi_SLE']\n",
    "    \n",
    "    print 'Trouble error w/o lexicon:', np.sqrt(np.mean(pred_sle_missed))\n",
    "    print 'Trouble error w/ lexicon:', np.sqrt(np.mean(lexi_sle_missed))\n",
    "    return None\n",
    "\n",
    "\n",
    "# lexi_train, lexi_test = load_lexicons()\n",
    "\n",
    "# train_lexi_eval = train_lexicon(train, target, train_leak, np.expm1(trn_preds), lexi_train)\n",
    "\n",
    "# get_errors(train_lexi_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
