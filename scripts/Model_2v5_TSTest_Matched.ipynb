{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2v5:  Match-Focused Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pdb\n",
    "import os\n",
    "import gc; gc.enable()\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Function for loading h5py file\n",
    "def load_h5py(fname):\n",
    "    with h5py.File(fname, 'r') as handle:\n",
    "        return handle['data'][:]\n",
    "# Function for loading pickle file\n",
    "def load_pickle(fname):\n",
    "    with open(fname, 'rb') as handle:\n",
    "        return pickle.load(handle)\n",
    "\n",
    "\n",
    "# Function for setting up\n",
    "def get_input(debug=False):\n",
    "    '''\n",
    "    Function for loading either debug or full datasets\n",
    "    '''\n",
    "    os.chdir('../data/compressed/')\n",
    "    print os.getcwd()\n",
    "    pkl_files = ['train_id.pickle', 'trainidx.pickle', 'target.pickle', 'test_id.pickle', 'testidx.pickle']\n",
    "    if debug:\n",
    "        print 'Loading debug train and test datasets...'\n",
    "        # h5py files\n",
    "        train = load_h5py('debug_train.h5')\n",
    "        test = load_h5py('debug_test.h5')\n",
    "        # pickle files\n",
    "        id_train, train_idx, target, id_test, test_idx = [load_pickle('debug_%s'%f) for f in pkl_files]\n",
    "    else:\n",
    "        print 'Loading original train and test datasets...'\n",
    "        # h5py files\n",
    "        train = load_h5py('full_train.h5')\n",
    "        test = load_h5py('full_test.h5')\n",
    "        # pickle files\n",
    "        id_train, train_idx, target, id_test, test_idx = [load_pickle('full_%s'%f) for f in pkl_files]\n",
    "    # Load feature names\n",
    "    fnames = load_pickle('feature_names.pickle')\n",
    "    # Find shape of loaded datasets\n",
    "    print('Shape of training dataset: {} Rows, {} Columns'.format(*train.shape))\n",
    "    print('Shape of test dataset: {} Rows, {} Columns'.format(*test.shape))\n",
    "    os.chdir('../../scripts/')\n",
    "    print os.getcwd()\n",
    "    return fnames, train, id_train, train_idx, target, test, id_test, test_idx\n",
    "\n",
    "\n",
    "# Function for getting datasets in dataframe format\n",
    "def get_dataframes(debug=False):\n",
    "    # Load data\n",
    "    fnames, train, id_train, train_idx, target, test, id_test, test_idx = get_input(debug)\n",
    "    # Format data\n",
    "    train_df = pd.DataFrame(data=train, index=train_idx, columns=fnames)\n",
    "    train_df['ID'] = id_train\n",
    "    train_df['target'] = target\n",
    "    test_df = pd.DataFrame(data=test, index=test_idx, columns=fnames)\n",
    "    test_df['ID'] = id_test\n",
    "    \n",
    "    print('\\nShape of training dataframe: {} Rows, {} Columns'.format(*train_df.shape))\n",
    "    print('Shape of test dataframe: {} Rows, {} Columns'.format(*test_df.shape))\n",
    "    return fnames, train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading leaks\n",
    "def load_leaks(leak_val):\n",
    "    leak_dir = './time_series/stats/'\n",
    "    \n",
    "    train_leak_loc = leak_dir + 'train_leak_%s.csv'%leak_val\n",
    "    train_leak = pd.read_csv(train_leak_loc).compiled_leak\n",
    "    test_leak_loc = leak_dir + 'test_leak_%s.csv'%leak_val\n",
    "    test_leak = pd.read_csv(test_leak_loc).compiled_leak\n",
    "    \n",
    "    return train_leak, test_leak\n",
    "\n",
    "\n",
    "# Function for applying statistical transformations to data\n",
    "def calculate_metadata(df):\n",
    "    '''\n",
    "    Function for calculating metadata across pandas dataframe row\n",
    "    '''\n",
    "    meta = pd.DataFrame()\n",
    "    # Calculations that disregard zeros\n",
    "    meta['nz_mean'] = df.apply(lambda x: x[x!=0].mean(), axis=1)\n",
    "    meta['nz_log_mean_exp'] = df.apply(lambda x: np.expm1(np.mean(np.log1p(x[x!=0]))), axis=1)\n",
    "    meta['nz_median'] = df.apply(lambda x: x[x!=0].median(), axis=1)\n",
    "    meta['nz_std'] = df.apply(lambda x: x[x!=0].std(), axis=1)\n",
    "    meta['nz_kurtosis'] = df.apply(lambda x: x[x!=0].kurtosis(), axis=1)\n",
    "    meta['nz_min'] = df.apply(lambda x: np.min(x[x!=0]), axis=1)\n",
    "    \n",
    "    # Calculations independent of zeros\n",
    "    meta['sum'] = df.apply(lambda x: np.sum(x), axis=1)\n",
    "    meta['max'] = df.apply(lambda x: np.max(x), axis=1)\n",
    "    \n",
    "    # Calculations factoring in zeros\n",
    "    meta['zero_count'] = df.apply(lambda x: np.count_nonzero(x==0), axis=1)\n",
    "    meta['mean'] = df.apply(lambda x: x.mean(), axis=1)\n",
    "    meta['log_mean_exp'] = df.apply(lambda x: np.expm1(np.mean(np.log1p(x))), axis=1)\n",
    "    meta['median'] = df.apply(lambda x: x.median(), axis=1)\n",
    "    meta['std'] = df.apply(lambda x: x.std(), axis=1)\n",
    "    meta['kurtosis'] = df.apply(lambda x: x.kurtosis(), axis=1)\n",
    "    \n",
    "    return meta\n",
    "\n",
    "\n",
    "# Function for feature engineering\n",
    "def format_for_training(train, test, f, leak_train, leak_test, lagval, trn_res, tst_res):\n",
    "    '''\n",
    "    - Formats train and test dataframes for training\n",
    "    - Splits the training and test datasets into those found and missed\n",
    "    from the time-series reconstruction (for a total of 4 groups)\n",
    "    - Redefines training and test set as:\n",
    "        - Training: All those that were found in t-s reconstruction\n",
    "        - Test: All those that were missed in t-s reconstruction\n",
    "    '''\n",
    "    # Hit and miss indexes\n",
    "    trn_leak_idx = np.where(leak_train!=0)[0]\n",
    "    trn_miss_idx = np.where(leak_train==0)[0]\n",
    "    \n",
    "    tst_leak_idx = np.where(leak_test!=0)[0]\n",
    "    tst_miss_idx = np.where(leak_test==0)[0]\n",
    "    \n",
    "    print 'Loading metadata for training set...'\n",
    "    trn_meta = pd.read_csv('./model_data/train_meta.csv')\n",
    "    print 'Loading metadata for test set...'\n",
    "    tst_meta = pd.read_csv('./model_data/test_meta.csv')\n",
    "    \n",
    "    score_name = './model_data/model_2v2_featscores_%s.csv'%lagval\n",
    "    print '\\nLoading file:\\n', score_name\n",
    "    score_df = pd.read_csv(score_name, index_col=0)\n",
    "    \n",
    "    # Select good features\n",
    "    if lagval==36:\n",
    "        threshold = 1.756\n",
    "    elif lagval==37:\n",
    "        threshold = 0.6255\n",
    "    else:\n",
    "        threshold = 0.625  # lag 38\n",
    "    good_features = score_df.loc[score_df['rmse']<=threshold].index\n",
    "    \n",
    "    # Format boosting training and test sets\n",
    "    print '\\nFormatting datasets...'\n",
    "    \n",
    "    # Incorporate predictions as log leak\n",
    "    tmp_trn = train.copy(deep=True)\n",
    "    tmp_trn['log_leak'] = np.log1p(trn_res['target'].values)\n",
    "    \n",
    "    tmp_tst = test.copy(deep=True)\n",
    "    tmp_tst['log_leak'] = np.log1p(tst_res['target'].values)\n",
    "    \n",
    "    tmp_trn = pd.concat([tmp_trn, trn_meta], axis=1)\n",
    "    tmp_tst = pd.concat([tmp_tst, tst_meta], axis=1)\n",
    "    \n",
    "    cols = ['ID'] + list(good_features) + list(trn_meta.columns.values) + ['log_leak']\n",
    "    leaked = pd.concat([tmp_trn.loc[trn_leak_idx, cols], tmp_tst.loc[tst_leak_idx, cols]], \n",
    "                       axis=0).reset_index(drop=True)\n",
    "    missed = pd.concat([tmp_trn.loc[trn_miss_idx, cols], tmp_tst.loc[tst_miss_idx, cols]], \n",
    "                       axis=0, ignore_index=True).reset_index(drop=True)\n",
    "    \n",
    "    # Define new target\n",
    "    target = stitch_target(train_leak, test_leak, trn_leak_idx, tst_leak_idx)\n",
    "    \n",
    "    del tmp_trn, tmp_tst; gc.collect()\n",
    "    return leaked, missed, target\n",
    "\n",
    "\n",
    "# Function for stitching new target together\n",
    "def stitch_target(train_leak, test_leak, trn_leak_idx, tst_leak_idx):\n",
    "    target_trn = train_leak[trn_leak_idx]\n",
    "    target_tst = test_leak[tst_leak_idx]\n",
    "    return np.concatenate([target_trn, target_tst])\n",
    "\n",
    "\n",
    "# Function for scaling datasets \n",
    "def scale_for_training(train, test):\n",
    "    print 'Scaling data...'\n",
    "    tmp_trn = train.copy(deep=True)\n",
    "    tmp_trn.replace(np.nan, 0, inplace=True)\n",
    "    tmp_tst = test.copy(deep=True)\n",
    "    tmp_tst.replace(np.nan, 0, inplace=True)\n",
    "    \n",
    "    tmp_trn.drop(labels=['ID'], axis=1, inplace=True)\n",
    "    tmp_tst.drop(labels=['ID'], axis=1, inplace=True)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaled_train = scaler.fit_transform(tmp_trn)\n",
    "    scaled_test = scaler.transform(tmp_tst)\n",
    "    \n",
    "    del tmp_trn, tmp_tst; gc.collect();\n",
    "    return scaled_train, scaled_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Script\n",
    "try:\n",
    "    del fnames, train, test\n",
    "    print 'Clearing loaded dataframes from memory...\\n'\n",
    "except:\n",
    "    pass\n",
    "fnames, train, test = get_dataframes(debug=False)\n",
    "\n",
    "# Load leaks\n",
    "leak_val = 38\n",
    "print '\\nLoading train and test leaks...'\n",
    "train_leak, test_leak = load_leaks(leak_val)\n",
    "print 'Nonzero elements in train:', np.count_nonzero(train_leak)\n",
    "print 'Nonzero elements in test:', np.count_nonzero(test_leak)\n",
    "\n",
    "# Load good result\n",
    "print '\\nLoading good results for train and test predictions...'\n",
    "tst_res_name = './model_data/tstest_lgb_2v2_lag38_good_test.csv'\n",
    "trn_res_name = './model_data/tstest_lgb_2v2_lag38_good_train.csv'\n",
    "tst_res = pd.read_csv(tst_res_name)\n",
    "trn_res = pd.read_csv(trn_res_name)\n",
    "print 'Shape of test results import:', tst_res.shape\n",
    "print 'Shape of train results import:', trn_res.shape\n",
    "\n",
    "# Find hit and miss indexes\n",
    "trn_leak_idx = np.where(train_leak!=0)[0]\n",
    "trn_miss_idx = np.where(train_leak==0)[0]\n",
    "\n",
    "tst_leak_idx = np.where(test_leak!=0)[0]\n",
    "tst_miss_idx = np.where(test_leak==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_flag = False\n",
    "train_name = './model_data/btrain_2v5_%s.csv'%leak_val\n",
    "test_name = './model_data/btest_2v5_%s.csv'%leak_val\n",
    "if pp_flag:\n",
    "    btrain, btest, target = format_for_training(train, test, fnames, train_leak, test_leak, leak_val, \n",
    "                                                trn_res, tst_res)\n",
    "    print '\\nSaving generated datasets...'\n",
    "    btrain.to_csv(train_name, index=False)\n",
    "    btest.to_csv(test_name, index=False)\n",
    "else:\n",
    "    print '\\nLoading generated datasets...'\n",
    "    btrain = pd.read_csv(train_name)\n",
    "    btest = pd.read_csv(test_name)\n",
    "    target = target = stitch_target(train_leak, test_leak, trn_leak_idx, tst_leak_idx)\n",
    "    \n",
    "# Format target variable\n",
    "target_log = np.log1p(target)\n",
    "\n",
    "# Real train set target\n",
    "y = train['target'].values\n",
    "train_target = np.log1p(y)\n",
    "\n",
    "# Create dataset for booster training\n",
    "boost_train, boost_test = scale_for_training(btrain, btest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom evaluation metric for LGBM\n",
    "def RMSLE(preds, train_data):\n",
    "    return 'RMSLE', np.sqrt(mean_squared_error(train_data.get_label(), preds)), False\n",
    "\n",
    "\n",
    "# Function for evaluating mean errors\n",
    "def average_best_scores(scores):\n",
    "    s_cols = [c for c in scores.columns if c!='index']\n",
    "    errs = scores[s_cols].apply(lambda x: np.min(x[x!=np.nan]), axis=0)\n",
    "    return np.mean(errs)\n",
    "\n",
    "\n",
    "# Function for plotting RMSLE averaged over all iterations\n",
    "def plot_scores(t_scores, v_scores):\n",
    "    s_cols = [c for c in t_scores.columns if c!='index']\n",
    "    t_mean = np.mean(t_scores[s_cols], axis=1)\n",
    "    t_mean = t_mean[~np.isnan(t_mean)]\n",
    "    v_mean = np.mean(v_scores[s_cols], axis=1)\n",
    "    v_mean = v_mean[~np.isnan(v_mean)]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(v_mean, label='validation', c='orange', alpha=0.7)\n",
    "    plt.plot(t_mean, label='training', c='orange', alpha=0.7)\n",
    "    plt.title('Averaged Training and Validation Error')\n",
    "    plt.xlabel('Training Iteration')\n",
    "    plt.ylabel('RMSLE')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "\n",
    "# Function for training a LGB Regressor\n",
    "def train_lgb_regressor(train, target, test, params, n_boost=500):\n",
    "    num_boosting = n_boost\n",
    "    valid_result = pd.DataFrame(data=np.arange(num_boosting), columns=['index'])\n",
    "    train_result = pd.DataFrame(data=np.arange(num_boosting), columns=['index'])\n",
    "    test_predictions = np.zeros(test.shape[0])\n",
    "    train_predictions = np.zeros(train.shape[0])\n",
    "    kfold = KFold(n_splits=4)\n",
    "    \n",
    "    for i, (trn, val) in enumerate(kfold.split(train)):\n",
    "        print '\\nTraining on fold:', i\n",
    "        round_name = 'round_%s'%i\n",
    "        xtrain = train[trn, :]\n",
    "        ytrain = target[trn]\n",
    "\n",
    "        xval = train[val, :]\n",
    "        yval = target[val]\n",
    "\n",
    "        lgb_train = lgb.Dataset(xtrain, ytrain)\n",
    "        lgb_eval = lgb.Dataset(xval, yval, reference=lgb_train)\n",
    "\n",
    "        evals_result = {}\n",
    "        gbm = lgb.train(params,\n",
    "                        lgb_train,\n",
    "                        num_boost_round=num_boosting,\n",
    "                        valid_sets= (lgb_train, lgb_eval),\n",
    "                        verbose_eval=500,\n",
    "                        feval=RMSLE, \n",
    "                        evals_result=evals_result,\n",
    "                        early_stopping_rounds=50)\n",
    "\n",
    "        # Get evaluation results\n",
    "        valid_res_df = pd.DataFrame.from_dict(evals_result['valid_1'], orient='columns')\n",
    "        valid_res_df.rename(columns={'RMSLE': round_name}, inplace=True)\n",
    "        valid_res_df.reset_index(inplace=True)\n",
    "        train_res_df = pd.DataFrame.from_dict(evals_result['training'], orient='columns')\n",
    "        train_res_df.rename(columns={'RMSLE': round_name}, inplace=True)\n",
    "        train_res_df.reset_index(inplace=True)\n",
    "\n",
    "        valid_result = pd.merge(valid_result, valid_res_df, how='outer', on='index')\n",
    "        train_result = pd.merge(train_result, train_res_df, how='outer', on='index')\n",
    "\n",
    "        # Make predictions\n",
    "        test_predictions += (gbm.predict(test, num_iteration=gbm.best_iteration))/kfold.n_splits\n",
    "        train_predictions += (gbm.predict(train, num_iteration=gbm.best_iteration))/kfold.n_splits\n",
    "    \n",
    "    return train_result, valid_result, test_predictions, train_predictions\n",
    "\n",
    "\n",
    "# Function for calculating real scores for training set\n",
    "def get_real_scores(trn_preds, tst_preds, train_target, trn_leak_idx, trn_miss_idx):\n",
    "    '''\n",
    "    Splits predictions into training leak and miss\n",
    "    Gets scores for both sets\n",
    "    '''\n",
    "    train_leak_preds = trn_preds[:trn_leak_idx.shape[0]]\n",
    "    train_miss_preds = tst_preds[:trn_miss_idx.shape[0]]\n",
    "    \n",
    "    stitch_preds = np.zeros(train_target.shape[0])\n",
    "    stitch_preds[trn_leak_idx] = train_leak_preds\n",
    "    stitch_preds[trn_miss_idx] = train_miss_preds\n",
    "    \n",
    "    train_target_miss = train_target[trn_miss_idx]\n",
    "    \n",
    "    # Calculate scores\n",
    "    full_score = np.sqrt(mean_squared_error(stitch_preds, train_target))\n",
    "    miss_score = np.sqrt(mean_squared_error(train_miss_preds, train_target_miss))\n",
    "    \n",
    "    return full_score, miss_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGB Regressor Parameters\n",
    "params = {'task': 'train',\n",
    "          'boosting': 'gbdt',\n",
    "          'objective': 'regression',\n",
    "          'metric': 'RMSLE',\n",
    "          'num_leaves': 5,\n",
    "          'learning_rate': 0.03,\n",
    "          'max_depth': 10,\n",
    "          'feature_fraction': 0.7,\n",
    "          'bagging_fraction': 0.53,\n",
    "          'bagging_freq': 5,\n",
    "          'min_sum_hessian_in_leaf': 1e-3,\n",
    "          'lambda_l2': 23,\n",
    "          'verbose': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune LGB Regressor Hyper-Parameters\n",
    "tune_flag = False\n",
    "if tune_flag:\n",
    "    cv_results = {}\n",
    "\n",
    "#     cv_entry = 'num_leaves'\n",
    "#     cv_set = [5, 6, 7]\n",
    "\n",
    "    cv_entry = 'learning_rate'\n",
    "    cv_set = [0.01, 0.03, 0.4]\n",
    "\n",
    "#     cv_entry = 'max_depth'\n",
    "#     cv_set = [5, 10, 15]\n",
    "\n",
    "#     cv_entry = 'feature_fraction'\n",
    "#     cv_set = [0.6, 0.7, 0.75]\n",
    "\n",
    "#     cv_entry = 'bagging_fraction'\n",
    "#     cv_set = [.52, .53, .55]\n",
    "\n",
    "#     cv_entry = 'min_sum_hessian_in_leaf'\n",
    "#     cv_set = [1e-4, 1e-3, 1e-2]\n",
    "\n",
    "#     cv_entry = 'lambda_l2'\n",
    "#     cv_set = [22, 23, 24]\n",
    "\n",
    "    for cv_val in cv_set:\n",
    "        print '\\nCross validating with %s: %s'%(cv_entry, cv_val)\n",
    "        params[cv_entry] = cv_val\n",
    "        # Get predictions\n",
    "        train_res, valid_res, tst_preds, trn_preds = train_lgb_regressor(train=boost_train, \n",
    "                                                                         target=target_log, \n",
    "                                                                         test=boost_test, \n",
    "                                                                         params=params, \n",
    "                                                                         n_boost=30000)\n",
    "        # Get real and train / val scores\n",
    "        trn_full, trn_miss = get_real_scores(trn_preds=trn_preds,\n",
    "                                             tst_preds=tst_preds,\n",
    "                                             train_target=train_target,\n",
    "                                             trn_leak_idx=trn_leak_idx,\n",
    "                                             trn_miss_idx=trn_miss_idx)\n",
    "        train_res_score = average_best_scores(train_res)\n",
    "        valid_res_score = average_best_scores(valid_res)\n",
    "        # Compile all scores\n",
    "        cv_results[cv_entry+'_full_miss_train_val_%s'%cv_val] = (trn_full, trn_miss,\n",
    "                                                                 train_res_score, valid_res_score)\n",
    "        \n",
    "    # Show cv results\n",
    "    print '\\nCV Results:'\n",
    "    for key in cv_results.keys():\n",
    "        print key, cv_results[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm current parameter settings\n",
    "train_res, valid_res, tst_preds, trn_preds = train_lgb_regressor(train=boost_train, \n",
    "                                                                 target=target_log, \n",
    "                                                                 test=boost_test, \n",
    "                                                                 params=params, \n",
    "                                                                 n_boost=30000)\n",
    "# Get real and train / val scores\n",
    "train_res_score = average_best_scores(train_res)\n",
    "valid_res_score = average_best_scores(valid_res)\n",
    "\n",
    "trn_full, trn_miss = get_real_scores(trn_preds=trn_preds,\n",
    "                                     tst_preds=tst_preds,\n",
    "                                     train_target=train_target,\n",
    "                                     trn_leak_idx=trn_leak_idx,\n",
    "                                     trn_miss_idx=trn_miss_idx)\n",
    "\n",
    "print '\\nTraining score:', train_res_score\n",
    "print 'Validation score:', valid_res_score\n",
    "\n",
    "print '\\nReal full training score:', trn_full\n",
    "print 'Real training missed score:', trn_miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_target(tst_preds, trn_preds, test_leak, tst_leak_idx, tst_miss_idx):\n",
    "    tst_preds = np.expm1(tst_preds)\n",
    "    trn_preds = np.expm1(trn_preds)\n",
    "    \n",
    "    test_leak_vals = test_leak[tst_leak_idx]\n",
    "    test_leak_pred = trn_preds[-tst_leak_idx.shape[0]:]\n",
    "    test_miss_pred = tst_preds[-tst_miss_idx.shape[0]:]\n",
    "    \n",
    "    stitch_ori = np.zeros(test_leak.shape[0])\n",
    "    stitch_ori[tst_leak_idx] = test_leak_pred\n",
    "    stitch_ori[tst_miss_idx] = test_miss_pred\n",
    "    \n",
    "    stitch_test = np.zeros(test_leak.shape[0])\n",
    "    stitch_test[tst_leak_idx] = test_leak_vals\n",
    "    stitch_test[tst_miss_idx] = test_miss_pred\n",
    "    \n",
    "    return stitch_test, stitch_ori\n",
    "\n",
    "\n",
    "def make_train_target(tst_preds, trn_preds, trn_leak_idx, trn_miss_idx):\n",
    "    tst_preds = np.expm1(tst_preds)\n",
    "    trn_preds = np.expm1(trn_preds)\n",
    "    \n",
    "    train_leak_pred = trn_preds[:trn_leak_idx.shape[0]]\n",
    "    train_miss_pred = tst_preds[:trn_miss_idx.shape[0]]\n",
    "    \n",
    "    stitch_train = np.zeros(train_leak.shape[0])\n",
    "    stitch_train[trn_leak_idx] = train_leak_pred\n",
    "    stitch_train[trn_miss_idx] = train_miss_pred\n",
    "    \n",
    "    return stitch_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make submission\n",
    "\n",
    "save_flag=True\n",
    "\n",
    "tst_target, ori_target = make_test_target(tst_preds, trn_preds, test_leak, tst_leak_idx, tst_miss_idx)\n",
    "trn_target = make_train_target(tst_preds, trn_preds, trn_leak_idx, trn_miss_idx)\n",
    "\n",
    "sub_name_tst = '../submissions/tstest_lgb_2v5_lag%s_submit.csv'%leak_val\n",
    "\n",
    "sub_name_ori = './model_data/tstest_lgb_2v5_lag%s_test.csv'%leak_val\n",
    "sub_name_trn = './model_data/tstest_lgb_2v5_lag%s_train.csv'%leak_val\n",
    "\n",
    "tst_df = pd.DataFrame()\n",
    "ori_df = pd.DataFrame()\n",
    "trn_df = pd.DataFrame()\n",
    "\n",
    "tst_df['ID'] = test['ID']\n",
    "ori_df['ID'] = test['ID']\n",
    "trn_df['ID'] = train['ID']\n",
    "\n",
    "tst_df['target'] = tst_target\n",
    "ori_df['target'] = ori_target\n",
    "trn_df['target'] = trn_target\n",
    "\n",
    "if save_flag:\n",
    "    tst_df.to_csv(sub_name_tst, index=False)\n",
    "    ori_df.to_csv(sub_name_ori, index=False)\n",
    "    trn_df.to_csv(sub_name_trn, index=False)\n",
    "\n",
    "tst_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_square_log_diff(sub1, sub2):\n",
    "    return np.sqrt(np.mean(np.square(np.subtract(np.log1p(sub1), np.log1p(sub2)))))\n",
    "\n",
    "def sum_abs_diff(sub1, sub2):\n",
    "    return np.sum(np.abs(np.subtract(sub1, sub2)))\n",
    "\n",
    "fri_df = pd.read_csv('../submissions/tstest_lgb_2v2_lag36_submit_0.52.csv')\n",
    "sat_df = pd.read_csv('../submissions/tstest_lgb_2v2_lag38_bad_submit_0.52.csv')\n",
    "sun_df = pd.read_csv('../submissions/tstest_lgb_2v2_lag38_good_submit.csv')\n",
    "\n",
    "right_idx = np.where(test_leak!=0)[0]\n",
    "wrong_idx = np.where(test_leak==0)[0]\n",
    "\n",
    "sub_right = tst_df['target'].values[right_idx]\n",
    "sun_right = sun_df['target'].values[right_idx]\n",
    "fri_right = fri_df['target'].values[right_idx]\n",
    "sat_right = sat_df['target'].values[right_idx]\n",
    "\n",
    "sub_wrong = tst_df['target'].values[wrong_idx]\n",
    "sun_wrong = sun_df['target'].values[wrong_idx]\n",
    "fri_wrong = fri_df['target'].values[wrong_idx]\n",
    "sat_wrong = sat_df['target'].values[wrong_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Sum abs diff:'\n",
    "print sum_abs_diff(sub_right, sun_right)\n",
    "print sum_abs_diff(sub_wrong, sun_wrong)\n",
    "\n",
    "print '\\nRoot mean square log diff:'\n",
    "print root_mean_square_log_diff(sub_right, sun_right)\n",
    "print root_mean_square_log_diff(sub_wrong, sun_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Sum abs diff:'\n",
    "print sum_abs_diff(sub_right, fri_right)\n",
    "print sum_abs_diff(sub_wrong, fri_wrong)\n",
    "\n",
    "print '\\nRoot mean square log diff:'\n",
    "print root_mean_square_log_diff(sub_right, fri_right)\n",
    "print root_mean_square_log_diff(sub_wrong, fri_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
