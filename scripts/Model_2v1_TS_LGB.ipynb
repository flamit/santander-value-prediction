{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2v1: Time Series Sequencing with Light GBM\n",
    "This model is inspired by the public kernel: https://www.kaggle.com/the1owl/love-is-the-answer\n",
    "\n",
    "Model 2v1 is a combination of original machine learning along with a sequence of weighted blending with top public leaderboard results. As a disclaimer, the public results I've chosen differ from the ones used in the public kernel linked above. The scores I will be blending together are:\n",
    "1. *ts_lgb_2v0_submit_0.66.csv* - **PL Score: 0.66** (Created from my own Model 2v0)\n",
    "2. *best_pub_blend.csv* - **PL Score: 0.63** (Taken from: https://www.kaggle.com/ashishpatel26/blending)\n",
    "3. *seq_ext_blend.csv* - **PL Score: 0.63** (Taken from: https://www.kaggle.com/prashantkikani/santad-label-is-present-in-row)\n",
    "\n",
    "At its core, the machine learning portion of Model 2v1 is an extension of Model 2v0's results. In Model 2v0, a \"feature scoring\" algorithm using XGBoost was used to determine the predictive value of each individual feature in the Santander dataset. Model 2v1 employs a different strategy. Model 2v1's strategy for choosing important features to include in the LightGBM training phase utilizes two concepts:\n",
    "1. Prioritizing by number of values that a feature shares with the target variable\n",
    "2. Checking that all the values within a candidate feature are within a 5% offset from their respective target variable value\n",
    "\n",
    "This second concept's idea of \"sequencing\" is conducted column-wise. Additionally, however, Model 2v0 also incorporates a notion of sequencing row-wise by adding additional features to the training set to capture the relative positions of the samples within the dataset (factoring in the **default lag value of 2**).\n",
    "\n",
    "Another point of deviation between Model 2v0 and Model 2v1 (apart from the blending involved in Model 2v1) is that Model 2v1 trains an LGB model on the entirety of the training and test sets rather than the train itself. The consequence of this training method is something to watch out for in this model.\n",
    "\n",
    "By itself (ie without the usage of blending), it may not be immediately guaranteed that Model 2v1 will outperform Model 2v0. The import feature selection process in 2v1 is largely based on a looser criteria than 2v0 which relied entirely upon XGBoost results. However, with the inclusion of a cascaded blending procedure, Model 2v1 should be expected to comfortably outperform Model 2v0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pdb\n",
    "import os\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug flag\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading h5py file\n",
    "def load_h5py(fname):\n",
    "    with h5py.File(fname, 'r') as handle:\n",
    "        return handle['data'][:]\n",
    "# Function for loading pickle file\n",
    "def load_pickle(fname):\n",
    "    with open(fname, 'rb') as handle:\n",
    "        return pickle.load(handle)\n",
    "# Function for saving pickle file\n",
    "def save_pickle(fname, data):\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for setting up\n",
    "def get_input(debug=False):\n",
    "    '''\n",
    "    Function for loading either debug or full datasets\n",
    "    '''\n",
    "    os.chdir('../data/compressed/')\n",
    "    print os.getcwd()\n",
    "    pkl_files = ['train_id.pickle', 'trainidx.pickle', 'target.pickle', 'test_id.pickle', 'testidx.pickle']\n",
    "    if debug:\n",
    "        print 'Loading debug train and test datasets...'\n",
    "        # h5py files\n",
    "        train = load_h5py('debug_train.h5')\n",
    "        test = load_h5py('debug_test.h5')\n",
    "        # pickle files\n",
    "        id_train, train_idx, target, id_test, test_idx = [load_pickle('debug_%s'%f) for f in pkl_files]\n",
    "    else:\n",
    "        print 'Loading original train and test datasets...'\n",
    "        # h5py files\n",
    "        train = load_h5py('full_train.h5')\n",
    "        test = load_h5py('full_test.h5')\n",
    "        # pickle files\n",
    "        id_train, train_idx, target, id_test, test_idx = [load_pickle('full_%s'%f) for f in pkl_files]\n",
    "    # Load feature names\n",
    "    fnames = load_pickle('feature_names.pickle')\n",
    "    # Find shape of loaded datasets\n",
    "    print('Shape of training dataset: {} Rows, {} Columns'.format(*train.shape))\n",
    "    print('Shape of test dataset: {} Rows, {} Columns'.format(*test.shape))\n",
    "    os.chdir('../../scripts/')\n",
    "    print os.getcwd()\n",
    "    return fnames, train, id_train, train_idx, target, test, id_test, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting datasets in dataframe format\n",
    "def get_dataframes(debug=False):\n",
    "    # Load data\n",
    "    fnames, train, id_train, train_idx, target, test, id_test, test_idx = get_input(debug)\n",
    "    # Format data\n",
    "    train_df = pd.DataFrame(data=train, index=train_idx, columns=fnames)\n",
    "    train_df['ID'] = id_train\n",
    "    train_df['target'] = target\n",
    "    test_df = pd.DataFrame(data=test, index=test_idx, columns=fnames)\n",
    "    test_df['ID'] = id_test\n",
    "    \n",
    "    print('\\nShape of training dataframe: {} Rows, {} Columns'.format(*train_df.shape))\n",
    "    print('Shape of test dataframe: {} Rows, {} Columns'.format(*test_df.shape))\n",
    "    return fnames, train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del fnames, train, test\n",
    "    print 'Clearing loaded dataframes from memory...\\n'\n",
    "except:\n",
    "    pass\n",
    "fnames, train, test = get_dataframes(debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model 2v0's results\n",
    "model_2v0_name = '../submissions/ts_lgb_2v0_submit_0.66.csv'\n",
    "res_2v0 = pd.read_csv(model_2v0_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find important columns\n",
    "important_cols = []\n",
    "for col in fnames:\n",
    "    crit1 = np.sum(train[col]==train['target']).astype(int)\n",
    "    crit2 = np.sum(((train[col]-train['target'])/train['target'])<0.05).astype(int)\n",
    "    if crit1>30 and crit2>3500:\n",
    "        important_cols.append(col)\n",
    "print '\\nNumber of important features: %s\\n'%len(important_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite train and test data\n",
    "cols = important_cols\n",
    "train_data = train[cols + ['ID', 'target']]\n",
    "test_data = test[cols + ['ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating row-wise metadata\n",
    "def get_meta(df, cols):\n",
    "    df['nz_mean'] = df[cols].apply(lambda x: x[x!=0].mean(), axis=1)\n",
    "    df['nz_max'] = df[cols].apply(lambda x: x[x!=0].max(), axis=1)\n",
    "    df['nz_min'] = df[cols].apply(lambda x: x[x!=0].min(), axis=1)\n",
    "    df['num_zero'] = df[cols].apply(lambda x: len(x[x==0]), axis=1)\n",
    "    df['mean'] = df[cols].apply(lambda x: x.mean(), axis=1)\n",
    "    df['max'] = df[cols].apply(lambda x: x.max(), axis=1)\n",
    "    df['min'] = df[cols].apply(lambda x: x.min(), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add metadata to train and test data\n",
    "print 'Adding metadata to train set...'\n",
    "train_data = get_meta(train_data, important_cols)\n",
    "print 'Adding metadata to test set...'\n",
    "test_data = get_meta(test_data, important_cols)\n",
    "# Update column list\n",
    "cols += ['nz_mean', 'nz_max', 'nz_min', 'num_zero', 'mean', 'max', 'min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in sequencing information row-wise\n",
    "for i in range(2, 100):\n",
    "    train_data['index_%s'%str(i)] = ((train_data.index + 2)%i == 0).astype(int)\n",
    "    test_data['index_%s'%str(i)] = ((test_data.index + 2)%i == 0).astype(int)\n",
    "    cols.append('index_%s'%str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add leak to test set\n",
    "test_data = pd.merge(test_data, res_2v0, how='left', on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format datasets for training and predicting\n",
    "train_data.replace(0, np.nan, inplace=True)\n",
    "test_data.replace(0, np.nan, inplace=True)\n",
    "# Concat the two dataframes together\n",
    "train_data = pd.concat((train_data, test_data), axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LightGBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "folds = KFold(n_splits=num_folds, shuffle=True, random_state=0)\n",
    "\n",
    "test_pred = np.zeros(test_data.shape[0])\n",
    "train_pred = np.zeros(train_data.shape[0])\n",
    "# Begin iteration over folds\n",
    "for i, (trn, val) in enumerate(folds.split(train_data)):\n",
    "    print 'Iterating on fold %s...'%i\n",
    "    # Define parameters\n",
    "    params = {'learning_rate': 0.02, \n",
    "              'max_depth': 7, \n",
    "              'boosting': 'gbdt', \n",
    "              'objective': 'regression', \n",
    "              'metric': 'rmse', \n",
    "              'is_training_metric': True, \n",
    "              'feature_fraction': 0.9, \n",
    "              'bagging_fraction': 0.8, \n",
    "              'bagging_freq': 5, \n",
    "              'seed': i}\n",
    "    # Define LGB datasets\n",
    "    dtrain = lgb.Dataset(train_data[cols].iloc[trn], np.log1p(train_data.target.values[trn]))\n",
    "    dval = lgb.Dataset(train_data[cols].iloc[val], np.log1p(train_data.target.values[val]))\n",
    "    # Train model\n",
    "    model = lgb.train(params=params,\n",
    "                      train_set = dtrain,\n",
    "                      valid_sets = dval,\n",
    "                      num_boost_round = 3000,\n",
    "                      early_stopping_rounds = 100,\n",
    "                      verbose_eval = 200)\n",
    "    # Update predictions\n",
    "    test_pred += np.expm1(model.predict(test_data[cols], num_iteration = model.best_iteration))\n",
    "    train_pred[val] = model.predict(train_data[cols].iloc[val], num_iteration = model.best_iteration)\n",
    "    # Find validation error\n",
    "    val_error = np.sqrt(mean_squared_error(np.log1p(train_data.target.values[val]), train_pred[val]))\n",
    "    print 'Validation error for %s fold is: %f'%(i, val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Cascaded Blending:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import leak values\n",
    "leak_path = './time_series/stats/'\n",
    "path_test_leak = leak_path + 'test_leak.csv'\n",
    "# Add test leak\n",
    "test_leak = pd.read_csv(path_test_leak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import files for blending\n",
    "best_pub_name = '../submissions/public/best_pub_blend.csv'\n",
    "seq_ext_name = '../submissions/public/seq_ext_blend.csv'\n",
    "res_best = pd.read_csv(best_pub_name)\n",
    "res_seq = pd.read_csv(seq_ext_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format LightGBM results\n",
    "test_data['target'] = test_pred\n",
    "test_data['target'] /= folds.n_splits\n",
    "# Replace predictions with time-series results from imported leak\n",
    "test_data.loc[test_leak['compiled_leak'].notnull(), 'target'] = test_leak.loc[test_leak['compiled_leak'].notnull(), \n",
    "                                                                              'compiled_leak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataframe of targets\n",
    "sub_df = pd.DataFrame()\n",
    "sub_df['ID'] = test_data['ID']\n",
    "sub_df['res_ml'] = test_data['target']\n",
    "sub_df['res_2v0'] = res_2v0['target']\n",
    "sub_df['res_seq'] = res_seq['target']\n",
    "sub_df['res_best'] = res_best['target']\n",
    "# Start blending\n",
    "sub_df['blend1'] = 0.8*sub_df.res_2v0 + 0.2*sub_df.res_ml\n",
    "sub_df['blend2'] = 0.8*sub_df.res_seq + 0.2*sub_df.blend1\n",
    "sub_df['blend3'] = 0.5*sub_df.res_seq + 0.5*sub_df.blend2\n",
    "sub_df['blend4'] = 0.6*sub_df.res_best + 0.4*sub_df.blend3\n",
    "# Define target variable as latest blend\n",
    "sub_df['target'] = sub_df.blend4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final results\n",
    "sub_name = '../submissions/ts_lgb_2v1_submit.csv'\n",
    "sub_df[['ID', 'target']].to_csv(sub_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
