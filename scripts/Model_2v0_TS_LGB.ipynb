{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2v0: Feature Scoring and LGBM with Time Series\n",
    "This model is based off the public kernel: https://www.kaggle.com/ogrellier/feature-scoring-vs-zeros\n",
    "\n",
    "This model utilizes machine learning to bolster the accuracy of time-series predictions. A key feature to this model is that it replaces all predictions generated via LightGBM by time-series predictions obtained from a time-series reconstruction process. By doing so, this approach prioritizes the time-series reconstruction's results and simply fills in values that the time-series reconstruction was unable to obtain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pdb\n",
    "import os\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug flag\n",
    "debug = False\n",
    "# Get feature scores flag\n",
    "get_scores = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading h5py file\n",
    "def load_h5py(fname):\n",
    "    with h5py.File(fname, 'r') as handle:\n",
    "        return handle['data'][:]\n",
    "# Function for loading pickle file\n",
    "def load_pickle(fname):\n",
    "    with open(fname, 'rb') as handle:\n",
    "        return pickle.load(handle)\n",
    "# Function for saving pickle file\n",
    "def save_pickle(fname, data):\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for setting up\n",
    "def get_input(debug=False):\n",
    "    '''\n",
    "    Function for loading either debug or full datasets\n",
    "    '''\n",
    "    os.chdir('../data/compressed/')\n",
    "    print os.getcwd()\n",
    "    pkl_files = ['train_id.pickle', 'trainidx.pickle', 'target.pickle', 'test_id.pickle', 'testidx.pickle']\n",
    "    if debug:\n",
    "        print 'Loading debug train and test datasets...'\n",
    "        # h5py files\n",
    "        train = load_h5py('debug_train.h5')\n",
    "        test = load_h5py('debug_test.h5')\n",
    "        # pickle files\n",
    "        id_train, train_idx, target, id_test, test_idx = [load_pickle('debug_%s'%f) for f in pkl_files]\n",
    "    else:\n",
    "        print 'Loading original train and test datasets...'\n",
    "        # h5py files\n",
    "        train = load_h5py('full_train.h5')\n",
    "        test = load_h5py('full_test.h5')\n",
    "        # pickle files\n",
    "        id_train, train_idx, target, id_test, test_idx = [load_pickle('full_%s'%f) for f in pkl_files]\n",
    "    # Load feature names\n",
    "    fnames = load_pickle('feature_names.pickle')\n",
    "    # Find shape of loaded datasets\n",
    "    print('Shape of training dataset: {} Rows, {} Columns'.format(*train.shape))\n",
    "    print('Shape of test dataset: {} Rows, {} Columns'.format(*test.shape))\n",
    "    os.chdir('../../scripts/')\n",
    "    print os.getcwd()\n",
    "    return fnames, train, id_train, train_idx, target, test, id_test, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting datasets in dataframe format\n",
    "def get_dataframes(debug=False):\n",
    "    # Load data\n",
    "    fnames, train, id_train, train_idx, target, test, id_test, test_idx = get_input(debug)\n",
    "    # Format data\n",
    "    train_df = pd.DataFrame(data=train, index=train_idx, columns=fnames)\n",
    "    train_df['ID'] = id_train\n",
    "    train_df['target'] = target\n",
    "    test_df = pd.DataFrame(data=test, index=test_idx, columns=fnames)\n",
    "    test_df['ID'] = id_test\n",
    "    \n",
    "    print('\\nShape of training dataframe: {} Rows, {} Columns'.format(*train_df.shape))\n",
    "    print('Shape of test dataframe: {} Rows, {} Columns'.format(*test_df.shape))\n",
    "    return fnames, train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating ROOT mean squared error\n",
    "def rmse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/cheng-haotai/Projects_Data/santander-value-prediction/data/compressed\n",
      "Loading debug train and test datasets...\n",
      "Shape of training dataset: 100 Rows, 4991 Columns\n",
      "Shape of test dataset: 200 Rows, 4991 Columns\n",
      "/Users/cheng-haotai/Projects_Data/santander-value-prediction/scripts\n",
      "\n",
      "Shape of training dataframe: 100 Rows, 4993 Columns\n",
      "Shape of test dataframe: 200 Rows, 4992 Columns\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del fnames, train, test\n",
    "    print 'Clearing loaded dataframes from memory...\\n'\n",
    "except:\n",
    "    pass\n",
    "fnames, train, test = get_dataframes(debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load leak values\n",
    "leak_path = './time_series/stats/'\n",
    "path_train_leak = leak_path + 'train_leak.csv' \n",
    "path_test_leak = leak_path + 'test_leak.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add train leak\n",
    "train_leak = pd.read_csv(path_train_leak)\n",
    "train['leak'] = train_leak['compiled_leak'].replace(np.nan, 0.0)\n",
    "train['log_leak'] = np.log1p(train['leak'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add test leak\n",
    "test_leak = pd.read_csv(path_test_leak)\n",
    "test['leak'] = test_leak['compiled_leak'].replace(np.nan, 0.0)\n",
    "test['log_leak'] = np.log1p(test['leak'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate and format target\n",
    "target = np.log1p(train['target'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scoring using XGBoost with Leak Feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for finding feature scores\n",
    "def feature_score(num_splits=5):\n",
    "    # Initialize XGBRegressor object\n",
    "    reg = xgb.XGBRegressor(n_estimators=1000)\n",
    "\n",
    "    folds = KFold(n_splits=num_splits, shuffle=True, random_state=0)\n",
    "    fold_idx = [(trn, val) for trn, val in folds.split(train)]\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for idx, f in enumerate(fnames):\n",
    "        feat_set = ['log_leak', f]\n",
    "        score = 0\n",
    "        for trn, val in fold_idx:\n",
    "            reg.fit(X = train[feat_set].iloc[trn], \n",
    "                    y = target[trn], \n",
    "                    eval_set = [(train[feat_set].iloc[val], target[val])],\n",
    "                    eval_metric = 'rmse',\n",
    "                    early_stopping_rounds = 50, \n",
    "                    verbose=False)\n",
    "            score += rmse(target[val], reg.predict(data=train[feat_set].iloc[val], \n",
    "                                                   ntree_limit=reg.best_ntree_limit)) / folds.n_splits\n",
    "        scores.append((f, score))\n",
    "    \n",
    "    return scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_name = './model_data/model_2v0_featscores.pickle'\n",
    "if get_scores:\n",
    "    # Get scores\n",
    "    scores = feature_score(num_splits=5)\n",
    "    # Save scores\n",
    "    save_pickle(score_name, scores)\n",
    "else:\n",
    "    # Load scores\n",
    "    scores = load_pickle(score_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from scores\n",
    "score_df = pd.DataFrame(data=scores, columns=['feature', 'rmse']).set_index('feature')\n",
    "score_df.sort_values(by='rmse', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select good features\n",
    "threshold = 0.7925\n",
    "good_features = score_df.loc[score_df['rmse']<=threshold].index\n",
    "good_rmse = score_df.loc[score_df['rmse']<=threshold, 'rmse'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LightGBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating row-wise metadata\n",
    "def add_metadata(df):\n",
    "    df.replace(0, np.nan, inplace=True)\n",
    "    # Calculate new metadata\n",
    "    df['log_of_mean'] = np.log1p(df.loc[:, fnames].mean(axis=1))\n",
    "    df['mean_of_log'] = np.mean(np.log1p(df.loc[:, fnames]), axis=1)\n",
    "    df['log_of_median'] = np.log1p(df.loc[:, fnames].median(axis=1))\n",
    "    df['num_nans'] = df.loc[:, fnames].isnull().sum(axis=1)\n",
    "    df['sum'] = df.loc[:, fnames].sum(axis=1)\n",
    "    df['std'] = df.loc[:, fnames].std(axis=1)\n",
    "    df['kurtosis'] = df.loc[:, fnames].kurtosis(axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add row-wise metadata to train and test sets\n",
    "data_train = add_metadata(train)\n",
    "data_test = add_metadata(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add target column to test set\n",
    "data_test['target'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features to be used in training LGBM\n",
    "flist = good_features.tolist() + ['log_of_mean', 'mean_of_log', 'log_of_median', \n",
    "                                   'num_nans', 'sum', 'std', 'kurtosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Dataset at 0x10f85c890>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define LGBM training set\n",
    "dtrain = lgb.Dataset(data=data_train[flist], \n",
    "                     label=target, free_raw_data=True)\n",
    "dtrain.construct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.5719734219023358\n",
      "Validation error: 1.1177347788650847\n",
      "Validation error: 1.058724414861202\n",
      "Validation error: 1.1432669430060374\n",
      "Validation error: 1.3099142971335864\n"
     ]
    }
   ],
   "source": [
    "# Train LightGBM\n",
    "lgb_params = {\n",
    "        'objective': 'regression',\n",
    "        'num_leaves': 58,\n",
    "        'subsample': 0.6143,\n",
    "        'colsample_bytree': 0.6453,\n",
    "        'min_split_gain': np.power(10, -2.5988),\n",
    "        'reg_alpha': np.power(10, -2.2887),\n",
    "        'reg_lambda': np.power(10, 1.7570),\n",
    "        'min_child_weight': np.power(10, -0.1477),\n",
    "        'verbose': -1,\n",
    "        'seed': 3,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'max_depth': -1,\n",
    "        'learning_rate': 0.05,\n",
    "        'metric': 'l2',\n",
    "            }\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "train_pred = np.zeros(data_train.shape[0])\n",
    "\n",
    "for trn, val in folds.split(data_train):\n",
    "    reg = lgb.train(params=lgb_params,\n",
    "                    train_set = dtrain.subset(trn),\n",
    "                    valid_sets = dtrain.subset(val), \n",
    "                    num_boost_round = 10000,\n",
    "                    early_stopping_rounds = 100,\n",
    "                    verbose_eval = 100)\n",
    "    \n",
    "    # Get training predictions\n",
    "    train_pred[val] = reg.predict(data_train[flist].iloc[val])\n",
    "    # Get test predictions\n",
    "    test['target'] = reg.predict(data_test[flist]) / folds.n_splits\n",
    "    # Print validation error\n",
    "    print 'Validation error:', mean_squared_error(train_pred[val], target[val])**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Results and Save Submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 1.2541342536172482\n",
      "Train score with leak: 0.6001457309052213\n"
     ]
    }
   ],
   "source": [
    "# Evaluate training results\n",
    "data_train['predictions'] = train_pred\n",
    "data_train.loc[data_train['leak'].notnull(), 'predictions'] = np.log1p(data_train.loc[data_train['leak'].notnull(), \n",
    "                                                                                      'leak'])\n",
    "print 'Train score:', mean_squared_error(target, train_pred)**.5\n",
    "print 'Train score with leak:', mean_squared_error(target, data_train['predictions'])**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test submission\n",
    "sub_name = '../submissions/ts_lgb_2v0_submit.csv'\n",
    "\n",
    "data_test['target'] = np.expm1(test['target'])\n",
    "data_test.loc[data_test['leak'].notnull(), 'target'] = data_test.loc[data_test['leak'].notnull(), 'leak']\n",
    "data_test[['ID', 'target']].to_csv(sub_name, index=False, float_format='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
