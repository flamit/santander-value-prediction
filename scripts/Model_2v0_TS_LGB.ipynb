{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2v0: Feature Scoring and LGBM with Time Series\n",
    "This model is based off the public kernel: https://www.kaggle.com/ogrellier/feature-scoring-vs-zeros\n",
    "\n",
    "This model utilizes machine learning to bolster the accuracy of time-series predictions. A key feature to this model is that it replaces all predictions generated via LightGBM by time-series predictions obtained from a time-series reconstruction process. By doing so, this approach prioritizes the time-series reconstruction's results and simply fills in values that the time-series reconstruction was unable to obtain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pdb\n",
    "import os\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug flag\n",
    "debug = False\n",
    "# Get feature scores flag\n",
    "get_scores = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading h5py file\n",
    "def load_h5py(fname):\n",
    "    with h5py.File(fname, 'r') as handle:\n",
    "        return handle['data'][:]\n",
    "# Function for loading pickle file\n",
    "def load_pickle(fname):\n",
    "    with open(fname, 'rb') as handle:\n",
    "        return pickle.load(handle)\n",
    "# Function for saving pickle file\n",
    "def save_pickle(fname, data):\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for setting up\n",
    "def get_input(debug=False):\n",
    "    '''\n",
    "    Function for loading either debug or full datasets\n",
    "    '''\n",
    "    os.chdir('../data/compressed/')\n",
    "    print os.getcwd()\n",
    "    pkl_files = ['train_id.pickle', 'trainidx.pickle', 'target.pickle', 'test_id.pickle', 'testidx.pickle']\n",
    "    if debug:\n",
    "        print 'Loading debug train and test datasets...'\n",
    "        # h5py files\n",
    "        train = load_h5py('debug_train.h5')\n",
    "        test = load_h5py('debug_test.h5')\n",
    "        # pickle files\n",
    "        id_train, train_idx, target, id_test, test_idx = [load_pickle('debug_%s'%f) for f in pkl_files]\n",
    "    else:\n",
    "        print 'Loading original train and test datasets...'\n",
    "        # h5py files\n",
    "        train = load_h5py('full_train.h5')\n",
    "        test = load_h5py('full_test.h5')\n",
    "        # pickle files\n",
    "        id_train, train_idx, target, id_test, test_idx = [load_pickle('full_%s'%f) for f in pkl_files]\n",
    "    # Load feature names\n",
    "    fnames = load_pickle('feature_names.pickle')\n",
    "    # Find shape of loaded datasets\n",
    "    print('Shape of training dataset: {} Rows, {} Columns'.format(*train.shape))\n",
    "    print('Shape of test dataset: {} Rows, {} Columns'.format(*test.shape))\n",
    "    os.chdir('../../scripts/')\n",
    "    print os.getcwd()\n",
    "    return fnames, train, id_train, train_idx, target, test, id_test, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting datasets in dataframe format\n",
    "def get_dataframes(debug=False):\n",
    "    # Load data\n",
    "    fnames, train, id_train, train_idx, target, test, id_test, test_idx = get_input(debug)\n",
    "    # Format data\n",
    "    train_df = pd.DataFrame(data=train, index=train_idx, columns=fnames)\n",
    "    train_df['ID'] = id_train\n",
    "    train_df['target'] = target\n",
    "    test_df = pd.DataFrame(data=test, index=test_idx, columns=fnames)\n",
    "    test_df['ID'] = id_test\n",
    "    \n",
    "    print('\\nShape of training dataframe: {} Rows, {} Columns'.format(*train_df.shape))\n",
    "    print('Shape of test dataframe: {} Rows, {} Columns'.format(*test_df.shape))\n",
    "    return fnames, train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating ROOT mean squared error\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/cheng-haotai/Projects_Data/santander-value-prediction/data/compressed\n",
      "Loading original train and test datasets...\n",
      "Shape of training dataset: 4459 Rows, 4991 Columns\n",
      "Shape of test dataset: 49342 Rows, 4991 Columns\n",
      "/Users/cheng-haotai/Projects_Data/santander-value-prediction/scripts\n",
      "\n",
      "Shape of training dataframe: 4459 Rows, 4993 Columns\n",
      "Shape of test dataframe: 49342 Rows, 4992 Columns\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del fnames, train, test\n",
    "    print 'Clearing loaded dataframes from memory...\\n'\n",
    "except:\n",
    "    pass\n",
    "fnames, train, test = get_dataframes(debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load leak values\n",
    "leak_path = './time_series/stats/'\n",
    "path_train_leak = leak_path + 'train_leak.csv' \n",
    "path_test_leak = leak_path + 'test_leak.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add train leak\n",
    "train_leak = pd.read_csv(path_train_leak)\n",
    "train['leak'] = train_leak['compiled_leak'].replace(np.nan, 0.0)\n",
    "train['log_leak'] = np.log1p(train['leak'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add test leak\n",
    "test_leak = pd.read_csv(path_test_leak)\n",
    "test['leak'] = test_leak['compiled_leak'].replace(np.nan, 0.0)\n",
    "test['log_leak'] = np.log1p(test['leak'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate and format target\n",
    "target = np.log1p(train['target'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scoring using XGBoost with Leak Feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for finding feature scores\n",
    "def feature_score(num_splits=5):\n",
    "    # Initialize XGBRegressor object\n",
    "    reg = xgb.XGBRegressor(n_estimators=1000)\n",
    "\n",
    "    folds = KFold(n_splits=num_splits, shuffle=True, random_state=0)\n",
    "    fold_idx = [(trn, val) for trn, val in folds.split(train)]\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for idx, f in enumerate(fnames):\n",
    "        feat_set = ['log_leak', f]\n",
    "        score = 0\n",
    "        for trn, val in fold_idx:\n",
    "            reg.fit(X = train[feat_set].iloc[trn], \n",
    "                    y = target[trn], \n",
    "                    eval_set = [(train[feat_set].iloc[val], target[val])],\n",
    "                    eval_metric = 'rmse',\n",
    "                    early_stopping_rounds = 50, \n",
    "                    verbose=False)\n",
    "            score += rmse(target[val], reg.predict(data=train[feat_set].iloc[val], \n",
    "                                                   ntree_limit=reg.best_ntree_limit)) / folds.n_splits\n",
    "        scores.append((f, score))\n",
    "    \n",
    "    return scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_name = './model_data/model_2v0_featscores.pickle'\n",
    "if get_scores:\n",
    "    # Get scores\n",
    "    scores = feature_score(num_splits=5)\n",
    "    # Save scores\n",
    "    save_pickle(score_name, scores)\n",
    "else:\n",
    "    # Load scores\n",
    "    scores = load_pickle(score_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from scores\n",
    "score_df = pd.DataFrame(data=scores, columns=['feature', 'rmse']).set_index('feature')\n",
    "score_df.sort_values(by='rmse', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select good features\n",
    "threshold = 0.7925\n",
    "good_features = score_df.loc[score_df['rmse']<=threshold].index\n",
    "good_rmse = score_df.loc[score_df['rmse']<=threshold, 'rmse'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = ['6eef030c1', 'ba42e41fa', '703885424', 'eeb9cd3aa', '3f4a39818',\n",
    "       '371da7669', 'b98f3e0d7', 'fc99f9426', '2288333b4', '324921c7b',\n",
    "       '66ace2992', '84d9d1228', '491b9ee45', 'de4e75360', '9fd594eec',\n",
    "       'f190486d6', '62e59a501', '20aa07010', 'c47340d97', '1931ccfdd',\n",
    "       'c2dae3a5a', 'e176a204a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.intersect1d(good_features, tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(good_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LightGBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating row-wise metadata\n",
    "def add_metadata(df):\n",
    "    df.replace(0, np.nan, inplace=True)\n",
    "    # Calculate new metadata\n",
    "    df['log_of_mean'] = np.log1p(df[fnames].replace(0, np.nan).mean(axis=1))\n",
    "    df['mean_of_log'] = np.log1p(df[fnames]).replace(0, np.nan).mean(axis=1)\n",
    "    df['log_of_median'] = np.log1p(df[fnames].replace(0, np.nan).median(axis=1))\n",
    "    df['num_nans'] = df[fnames].isnull().sum(axis=1)\n",
    "    df['sum'] = np.log1p(df[fnames].sum(axis=1))\n",
    "    df['std'] = df[fnames].std(axis=1)\n",
    "    df['kurtosis'] = df[fnames].kurtosis(axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding metadata to train set...\n",
      "Adding metadata to test set...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add row-wise metadata to train and test sets\n",
    "print '\\nAdding metadata to train set...'\n",
    "data_train = add_metadata(train.copy(deep=True))\n",
    "print 'Adding metadata to test set...\\n'\n",
    "data_test = add_metadata(test.copy(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add target column to test set\n",
    "data_test['target'] = 0\n",
    "# Define features to be used in training LGBM\n",
    "flist = good_features.tolist() + ['log_leak', 'log_of_mean', 'mean_of_log', 'log_of_median', \n",
    "                                  'num_nans', 'sum', 'std', 'kurtosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training and evaluating LightGBM\n",
    "def run_lgb(lgb_params):\n",
    "    folds = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    # Initialize placeholder prediction vectors\n",
    "    test_pred = np.zeros(data_test.shape[0])\n",
    "    train_pred = np.zeros(data_train.shape[0])\n",
    "    val_error = np.zeros(folds.n_splits)\n",
    "    \n",
    "    # Iterate through folds\n",
    "    for i, (trn, val) in enumerate(folds.split(data_train)):\n",
    "        # Define train and val for current fold\n",
    "        lgb_train = lgb.Dataset(data_train[flist].iloc[trn], label=target[trn])\n",
    "        lgb_eval = lgb.Dataset(data_train[flist].iloc[val], label=target[val])\n",
    "        \n",
    "        # Train regressor\n",
    "        reg = lgb.train(params=lgb_params,\n",
    "                        train_set = lgb_train,\n",
    "                        valid_sets = lgb_eval, \n",
    "                        num_boost_round = 10000,\n",
    "                        early_stopping_rounds = 100,\n",
    "                        verbose_eval = 0)\n",
    "\n",
    "        # Get training predictions\n",
    "        train_pred[val] = reg.predict(data_train[flist].iloc[val])\n",
    "        # Get test predictions\n",
    "        test_pred += reg.predict(data_test[flist]) / folds.n_splits\n",
    "        # Get validation error\n",
    "        val_error[i] = np.sqrt(mean_squared_error(train_pred[val], target[val]))\n",
    "        print 'Validation error for fold %s is: %f'%(i, val_error[i])\n",
    "        \n",
    "    # Print validation error\n",
    "    print 'Overall validation error:', val_error.mean()\n",
    "    \n",
    "    return test_pred, train_pred, val_error.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error for fold 0 is: 0.647939\n",
      "Validation error for fold 1 is: 0.681281\n",
      "Validation error for fold 2 is: 0.620527\n",
      "Validation error for fold 3 is: 0.729893\n",
      "Validation error for fold 4 is: 0.605220\n",
      "Overall validation error: 0.6569718036541922\n"
     ]
    }
   ],
   "source": [
    "# Define LGBM training set\n",
    "dtrain = lgb.Dataset(data=data_train[flist], \n",
    "                     label=target, free_raw_data=True)\n",
    "\n",
    "# Train LightGBM\n",
    "lgb_params = {'objective': 'regression',\n",
    "              'metric': 'mean_squared_error',\n",
    "              'boosting_type': 'gbdt',\n",
    "              'random_seed': 3,\n",
    "              'verbose': -1,\n",
    "              'learning_rate': 0.05,\n",
    "              'num_leaves': 58,\n",
    "              'subsample': 0.6143,\n",
    "              'colsample_bytree': 0.6453,\n",
    "              'min_split_gain': np.power(10, -2.5988),\n",
    "              'reg_alpha': np.power(10, -2.2887),\n",
    "              'reg_lambda': np.power(10, 1.7570),\n",
    "              'min_child_weight': np.power(10, -0.1477),\n",
    "              'max_depth': -1}\n",
    "\n",
    "\n",
    "test_pred, train_pred, v_acc = run_lgb(lgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Results and Save Submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.6585031116467885\n",
      "Train score with leak: 0.651826372560223\n"
     ]
    }
   ],
   "source": [
    "# Evaluate training results\n",
    "data_train['predictions'] = train_pred\n",
    "data_train.loc[data_train['leak'].notnull(), 'predictions'] = np.log1p(data_train.loc[data_train['leak'].notnull(), \n",
    "                                                                                      'leak'])\n",
    "print 'Train score:', mean_squared_error(target, train_pred)**.5\n",
    "print 'Train score with leak:', mean_squared_error(target, data_train['predictions'])**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test submission\n",
    "sub_name = '../submissions/ts_lgb_2v0_submit.csv'\n",
    "\n",
    "data_test['target'] = np.expm1(test_pred)\n",
    "data_test.loc[data_test['leak'].notnull(), 'target'] = data_test.loc[data_test['leak'].notnull(), 'leak']\n",
    "data_test[['ID', 'target']].to_csv(sub_name, index=False, float_format='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
