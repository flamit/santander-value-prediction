{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras model history class\n",
    "class history(object):\n",
    "    def __init__(self, fname):\n",
    "        '''Defines main parameters for history object'''\n",
    "        self.fname = fname\n",
    "        if os.path.exists(self.fname):\n",
    "            print 'Loading existing history file...'\n",
    "            with open(self.fname, 'r') as handle:\n",
    "                self.history = json.load(handle)\n",
    "        else:\n",
    "            print 'Creating new history file...'\n",
    "            self.history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
    "            with open(self.fname, 'w') as handle:\n",
    "                json.dump(self.history, handle)\n",
    "    \n",
    "    \n",
    "    def append_history(self, new_history):\n",
    "        '''Updates history object with new training history'''\n",
    "        for key, value in new_history.iteritems():\n",
    "            for v in value:\n",
    "                self.history[key].append(v)\n",
    "        print 'Saving updated history file...'\n",
    "        with open(self.fname, 'w') as handle:\n",
    "            json.dump(self.history, handle)\n",
    "    \n",
    "    \n",
    "    def visualize_accuracy(self):\n",
    "        '''Plots visualization of training and validation accuracies'''\n",
    "        t_acc = self.history['acc']\n",
    "        v_acc = self.history['val_acc']\n",
    "        # Make plot\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.plot(t_acc, label='training_acc')\n",
    "        plt.plot(v_acc, label='validation_acc')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.title('Training and Validation Accuracies vs. Epoch Count')\n",
    "        plt.xlabel('Epoch Count')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current epoch\n",
    "def get_current_epoch(n_prefix):\n",
    "    files = sorted([f for f in os.listdir(weights_path) if n_prefix in f])\n",
    "    if len(files) == 0:\n",
    "        print 'No epochs have yet been run'\n",
    "        return(-1)\n",
    "    else:\n",
    "        latest_idx = int(files[-1].split(weights_suffix)[0].split('_')[-1])-1\n",
    "        print 'Latest epoch:', latest_idx\n",
    "        return(latest_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format current epoch\n",
    "def format_epoch(enum):\n",
    "    strnum = str(enum)\n",
    "    fill_val = 3 - len(strnum)\n",
    "    if fill_val > 0:\n",
    "        return(fill_val*str(0)+strnum)\n",
    "    else:\n",
    "        return(strnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating stacked autoencoders\n",
    "def get_stacked_models(input_size, num_ae):\n",
    "    encoded_input = Input(shape=(input_size,))\n",
    "    autoencoder_list = []\n",
    "    encoder_list = []\n",
    "    for n in range(num_ae):\n",
    "        encoding_dim = int(input_size/(2**(n+1)))  # Size of bottleneck layer\n",
    "        decoding_dim = int(input_size/(2**n))  # Size of reconstruction\n",
    "        # Define autoencoder layers\n",
    "        encoded = Dense(encoding_dim, activation='relu', name='encode_%s'%str(n))(encoded_input)\n",
    "        decoded = Dense(decoding_dim, activation='relu', name='decode_%s'%str(n))(encoded)\n",
    "        # Define and compile models\n",
    "        autoencoder = Model(encoded_input, decoded)\n",
    "        encoder = Model(encoded_input, encoded)\n",
    "        opt = Adam(lr=0.0001)\n",
    "        autoencoder.compile(optimizer=opt, loss='mean_squared_error', metrics=['accuracy'])\n",
    "        autoencoder_list.append(autoencoder)\n",
    "        encoder_list.append(encoder)\n",
    "        # Reset encoded input to smaller size\n",
    "        encoded_input = Input(shape=(encoding_dim,))\n",
    "    return encoder_list, autoencoder_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug flag:\n",
    "debug=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data:\n",
    "data_path = '../../data/'\n",
    "if debug:\n",
    "    print 'Using debugging datasets'\n",
    "    train_path = data_path + 'train_debug.csv'\n",
    "    test_path = data_path + 'test_debug.csv'\n",
    "    epoch_num = 5\n",
    "else:\n",
    "    print 'Using full datasets'\n",
    "    train_path = data_path + 'train.csv'\n",
    "    test_path = data_path + 'test.csv'\n",
    "    epoch_num = 400\n",
    "print 'Loading training and test dataframes...'\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "print 'Training and test dataframes loaded!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_path = os.getcwd() + '/'\n",
    "# Autoencoder weight files metadata\n",
    "weights_path = parent_path + 'weights/'\n",
    "weights_prefix = 'ae_weight_'\n",
    "weights_suffix = '.hdf5'\n",
    "# Autoencoder transformed data metadata\n",
    "trans_path = parent_path + 'data/'\n",
    "trans_prefix = trans_path + 'transformed_'\n",
    "trans_suffix = '.csv'\n",
    "# History file metadata\n",
    "history_prefix = weights_path + 'history_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make folders if they don't exist\n",
    "if not os.path.exists(weights_path):\n",
    "    print 'Creating weights folder...'\n",
    "    os.mkdir(weights_path)\n",
    "if not os.path.exists(trans_path):\n",
    "    print 'Creating transformed data folder...'\n",
    "    os.mkdir(trans_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop training labels\n",
    "train_df.drop(columns=['target'], inplace=True, axis=1)\n",
    "print('Shape of training dataset: {} Rows, {} Columns'.format(*train_df.shape))\n",
    "print('Shape of test dataset: {} Rows, {} Columns'.format(*test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create aggregate train + test dataframe\n",
    "print 'Scaling data and assembling master dataframe...'\n",
    "all_data = pd.concat([train_df, test_df], axis=0)\n",
    "all_data.set_index('ID', inplace=True)\n",
    "# Get columns\n",
    "all_cols = all_data.columns.values\n",
    "all_idx = all_data.index.values\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(all_data)\n",
    "scaled_data = pd.DataFrame(data=scaled_data, columns=all_cols, index=all_idx)\n",
    "scaled_data.index.name = 'ID'\n",
    "print 'Master dataframe assembled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked autoencoder parameters\n",
    "num_ae = 5\n",
    "batch_size = 100\n",
    "num_train = train_df.shape[0]\n",
    "input_size = all_data.shape[1]\n",
    "print 'Autoencoder input size:', input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create autoencoder and encoder models\n",
    "encoder_list, autoencoder_list = get_stacked_models(input_size, num_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train stacked autoencoders\n",
    "current_data = scaled_data\n",
    "history_list = []  # List of history objects\n",
    "\n",
    "visualize_flag = False\n",
    "epoch_idx_params = dict()\n",
    "# Iterate through stacks \n",
    "for n, autoencoder in enumerate(autoencoder_list):\n",
    "    print 'Training %s autoencoder stack...'%n\n",
    "    print('Shape of current data: {} Rows, {} Columns'.format(*current_data.shape))\n",
    "    n_prefix = '%s_%s'%(str(n), weights_prefix)\n",
    "    # Initialize history file\n",
    "    epoch_history = history(history_prefix + str(n) + '.json')\n",
    "    # Load old weights (if exists)\n",
    "    latest_idx = get_current_epoch(n_prefix)\n",
    "    prior_weight_fname = weights_path + n_prefix + format_epoch(latest_idx) + weights_suffix\n",
    "    if os.path.exists(prior_weight_fname):\n",
    "        print 'Loading existing model weight...\\n', prior_weight_fname\n",
    "        autoencoder.load_weights(prior_weight_fname, by_name=True)\n",
    "    # Define performance tracking template\n",
    "    weight_fname = weights_path + n_prefix + '{epoch:03d}' + weights_suffix\n",
    "    checkpoint = ModelCheckpoint(weight_fname, monitor='val_loss', verbose=0, save_best_only=False, \n",
    "                                 save_weights_only=True)\n",
    "    # Train model\n",
    "    train_history = autoencoder.fit(current_data, current_data, epochs=epoch_num, batch_size=batch_size, verbose=1, \n",
    "                              shuffle=True, validation_data=(current_data, current_data), callbacks=[checkpoint], \n",
    "                              initial_epoch=latest_idx+1)\n",
    "    # Update history file with new training history\n",
    "    epoch_history.append_history(train_history.history)\n",
    "    history_list.append(epoch_history)\n",
    "    if visualize_flag:\n",
    "        print 'Visualizing the training and validation accuracies for stage %s'%n\n",
    "        epoch_history.visualize_accuracy()\n",
    "        plt.clf()\n",
    "        current_epoch = int(input('Please specify which epoch to use for predicting (max %s)'%(epoch_num-1)))\n",
    "        encoder_list[n].load_weights(weights_path + n_prefix + format_epoch(current_epoch) + weights_suffix, \n",
    "                                     by_name=True)\n",
    "        epoch_idx_params['param%s'%n] = current_epoch\n",
    "    else:\n",
    "        epoch_idx_params['param%s'%n] = get_current_epoch(n_prefix)\n",
    "    # Transform current data with current encoder layer\n",
    "    current_data = encoder_list[n].predict(current_data)\n",
    "    print('Shape of current data: {} Rows, {} Columns\\n'.format(*current_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformations\n",
    "current_data = scaled_data\n",
    "for i, encoder in enumerate(encoder_list):\n",
    "    n_prefix = '%s_%s'%(str(n), weights_prefix)\n",
    "    # Option to select specific epoch\n",
    "    if visualize_flag:\n",
    "        current_epoch = epoch_idx_params['param%s'%i]\n",
    "    else:  # Set latest epoch as current epoch \n",
    "        current_epoch = get_current_epoch(n_prefix)\n",
    "    # Load old weights (if exists)\n",
    "    prior_weight_fname = weights_path + n_prefix + format_epoch(current_epoch) + weights_suffix\n",
    "    if os.path.exists(prior_weight_fname):\n",
    "        print 'Loading existing model weight...\\n', prior_weight_fname\n",
    "        encoder.load_weights(prior_weight_fname, by_name=True)\n",
    "        print 'Reducing dimensionality for autoencoder layer %s'%i\n",
    "        current_data = encoder.predict(current_data)\n",
    "        # Save transformed data\n",
    "        transformed_data = pd.DataFrame(data=current_data, index=all_idx)\n",
    "        print 'Saving transformed dataset for AE layer %s'%i\n",
    "        transformed_data.to_csv(trans_path + 'data_' + str(transformed_data.shape[1]) + trans_suffix, index=None)\n",
    "    else:\n",
    "        print 'No trained epochs exist for autoencoder layer %s'%i\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
