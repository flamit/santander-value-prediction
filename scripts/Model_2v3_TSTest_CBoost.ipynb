{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2v3: Time-Series Reconstruction with CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pdb\n",
    "import os\n",
    "import gc; gc.enable()\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Function for loading h5py file\n",
    "def load_h5py(fname):\n",
    "    with h5py.File(fname, 'r') as handle:\n",
    "        return handle['data'][:]\n",
    "# Function for loading pickle file\n",
    "def load_pickle(fname):\n",
    "    with open(fname, 'rb') as handle:\n",
    "        return pickle.load(handle)\n",
    "\n",
    "\n",
    "# Function for setting up\n",
    "def get_input(debug=False):\n",
    "    '''\n",
    "    Function for loading either debug or full datasets\n",
    "    '''\n",
    "    os.chdir('../data/compressed/')\n",
    "    print os.getcwd()\n",
    "    pkl_files = ['train_id.pickle', 'trainidx.pickle', 'target.pickle', 'test_id.pickle', 'testidx.pickle']\n",
    "    if debug:\n",
    "        print 'Loading debug train and test datasets...'\n",
    "        # h5py files\n",
    "        train = load_h5py('debug_train.h5')\n",
    "        test = load_h5py('debug_test.h5')\n",
    "        # pickle files\n",
    "        id_train, train_idx, target, id_test, test_idx = [load_pickle('debug_%s'%f) for f in pkl_files]\n",
    "    else:\n",
    "        print 'Loading original train and test datasets...'\n",
    "        # h5py files\n",
    "        train = load_h5py('full_train.h5')\n",
    "        test = load_h5py('full_test.h5')\n",
    "        # pickle files\n",
    "        id_train, train_idx, target, id_test, test_idx = [load_pickle('full_%s'%f) for f in pkl_files]\n",
    "    # Load feature names\n",
    "    fnames = load_pickle('feature_names.pickle')\n",
    "    # Find shape of loaded datasets\n",
    "    print('Shape of training dataset: {} Rows, {} Columns'.format(*train.shape))\n",
    "    print('Shape of test dataset: {} Rows, {} Columns'.format(*test.shape))\n",
    "    os.chdir('../../scripts/')\n",
    "    print os.getcwd()\n",
    "    return fnames, train, id_train, train_idx, target, test, id_test, test_idx\n",
    "\n",
    "\n",
    "# Function for getting datasets in dataframe format\n",
    "def get_dataframes(debug=False):\n",
    "    # Load data\n",
    "    fnames, train, id_train, train_idx, target, test, id_test, test_idx = get_input(debug)\n",
    "    # Format data\n",
    "    train_df = pd.DataFrame(data=train, index=train_idx, columns=fnames)\n",
    "    train_df['ID'] = id_train\n",
    "    train_df['target'] = target\n",
    "    test_df = pd.DataFrame(data=test, index=test_idx, columns=fnames)\n",
    "    test_df['ID'] = id_test\n",
    "    \n",
    "    print('\\nShape of training dataframe: {} Rows, {} Columns'.format(*train_df.shape))\n",
    "    print('Shape of test dataframe: {} Rows, {} Columns'.format(*test_df.shape))\n",
    "    return fnames, train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading leaks\n",
    "def load_leaks(leak_val):\n",
    "    leak_dir = './time_series/stats/'\n",
    "    \n",
    "    train_leak_loc = leak_dir + 'train_leak_%s.csv'%leak_val\n",
    "    train_leak = pd.read_csv(train_leak_loc).compiled_leak\n",
    "    test_leak_loc = leak_dir + 'test_leak_%s.csv'%leak_val\n",
    "    test_leak = pd.read_csv(test_leak_loc).compiled_leak\n",
    "    \n",
    "    return train_leak, test_leak\n",
    "\n",
    "\n",
    "# Function for applying statistical transformations to data\n",
    "def calculate_metadata(df):\n",
    "    '''\n",
    "    Function for calculating metadata across pandas dataframe row\n",
    "    '''\n",
    "    meta = pd.DataFrame()\n",
    "    # Calculations that disregard zeros\n",
    "    meta['nz_mean'] = df.apply(lambda x: x[x!=0].mean(), axis=1)\n",
    "    meta['nz_log_mean_exp'] = df.apply(lambda x: np.expm1(np.mean(np.log1p(x[x!=0]))), axis=1)\n",
    "    meta['nz_median'] = df.apply(lambda x: x[x!=0].median(), axis=1)\n",
    "    meta['nz_std'] = df.apply(lambda x: x[x!=0].std(), axis=1)\n",
    "    meta['nz_kurtosis'] = df.apply(lambda x: x[x!=0].kurtosis(), axis=1)\n",
    "    meta['nz_min'] = df.apply(lambda x: np.min(x[x!=0]), axis=1)\n",
    "    \n",
    "    # Calculations independent of zeros\n",
    "    meta['sum'] = df.apply(lambda x: np.sum(x), axis=1)\n",
    "    meta['max'] = df.apply(lambda x: np.max(x), axis=1)\n",
    "    \n",
    "    # Calculations factoring in zeros\n",
    "    meta['zero_count'] = df.apply(lambda x: np.count_nonzero(x==0), axis=1)\n",
    "    meta['mean'] = df.apply(lambda x: x.mean(), axis=1)\n",
    "    meta['log_mean_exp'] = df.apply(lambda x: np.expm1(np.mean(np.log1p(x))), axis=1)\n",
    "    meta['median'] = df.apply(lambda x: x.median(), axis=1)\n",
    "    meta['std'] = df.apply(lambda x: x.std(), axis=1)\n",
    "    meta['kurtosis'] = df.apply(lambda x: x.kurtosis(), axis=1)\n",
    "    \n",
    "    return meta\n",
    "\n",
    "\n",
    "# Function for feature engineering\n",
    "def format_for_training_v1(train, test, f, trn_leak, tst_leak, trn_res, tst_res, lagval=38):\n",
    "    '''\n",
    "    - Formats train and test dataframes for training\n",
    "    '''\n",
    "    tmp_trn = train.copy(deep=True)\n",
    "    tmp_trn['leak'] = trn_res['target'].values\n",
    "    tmp_trn['log_leak'] = np.log1p(tmp_trn['leak'])\n",
    "    \n",
    "    tmp_tst = test.copy(deep=True)\n",
    "    tmp_tst['leak'] = tst_res['target'].values\n",
    "    tmp_tst['log_leak'] = np.log1p(tmp_tst['leak'])\n",
    "    \n",
    "    score_name = './model_data/model_2v2_featscores_%s.csv'%lagval\n",
    "    print 'Loading file:\\n', score_name\n",
    "    score_df = pd.read_csv(score_name, index_col=0)\n",
    "    \n",
    "    # Select good features\n",
    "    if lagval==36:\n",
    "        threshold = 1.756\n",
    "    elif lagval==37:\n",
    "        threshold = 0.6255\n",
    "    else:\n",
    "        threshold = 0.625  # lag 38\n",
    "#     good_features = score_df.loc[score_df['rmse']<=threshold].index\n",
    "    feature_list = load_pickle('./time_series/aaron_test_v0.pickle')\n",
    "    good_features = []\n",
    "    for i in range(55):\n",
    "        good_features += feature_list[i]\n",
    "    \n",
    "    print '\\nLoading metadata for training set...'\n",
    "    trn_meta = pd.read_csv('./model_data/train_meta.csv')\n",
    "    print 'Loading metadata for test set...'\n",
    "    tst_meta = pd.read_csv('./model_data/test_meta.csv')\n",
    "    \n",
    "    # Format training and test datasets\n",
    "    cols = ['ID'] + list(good_features) + list(trn_meta.columns.values) + ['log_leak']\n",
    "    tmp_trn = pd.concat([tmp_trn, trn_meta], axis=1)\n",
    "    tmp_tst = pd.concat([tmp_tst, tst_meta], axis=1)\n",
    "    \n",
    "    return tmp_trn[cols], tmp_tst[cols]\n",
    "\n",
    "\n",
    "\n",
    "# Function for scaling datasets \n",
    "def scale_for_training(train, test):\n",
    "    print 'Scaling data...'\n",
    "    tmp_trn = train.copy(deep=True)\n",
    "    tmp_trn.replace(np.nan, 0, inplace=True)\n",
    "    tmp_tst = test.copy(deep=True)\n",
    "    tmp_tst.replace(np.nan, 0, inplace=True)\n",
    "    \n",
    "    tmp_trn.drop(labels=['ID'], axis=1, inplace=True)\n",
    "    tmp_tst.drop(labels=['ID'], axis=1, inplace=True)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaled_train = scaler.fit_transform(tmp_trn)\n",
    "    scaled_test = scaler.transform(tmp_tst)\n",
    "    \n",
    "    del tmp_trn, tmp_tst; gc.collect();\n",
    "    return scaled_train, scaled_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Script\n",
    "try:\n",
    "    del fnames, train, test\n",
    "    print 'Clearing loaded dataframes from memory...\\n'\n",
    "except:\n",
    "    pass\n",
    "fnames, train, test = get_dataframes(debug=False)\n",
    "\n",
    "# Load leaks\n",
    "leak_val = 38\n",
    "print '\\nLoading train and test leaks...'\n",
    "train_leak, test_leak = load_leaks(leak_val)\n",
    "print 'Nonzero elements in train:', np.count_nonzero(train_leak)\n",
    "print 'Nonzero elements in test:', np.count_nonzero(test_leak)\n",
    "\n",
    "# Load good result\n",
    "print '\\nLoading good results for train and test predictions...'\n",
    "\n",
    "tst_res_name = './model_data/tstest_lgb_2v2_lag38_fullfeat_test.csv'\n",
    "trn_res_name = './model_data/tstest_lgb_2v2_lag38_fullfeat_train.csv'\n",
    "\n",
    "tst_res = pd.read_csv(tst_res_name)\n",
    "trn_res = pd.read_csv(trn_res_name)\n",
    "print 'Shape of test results import:', tst_res.shape\n",
    "print 'Shape of train results import:', trn_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and test in format for training booster\n",
    "# Format target variable\n",
    "target = train['target'].values\n",
    "target_log = np.log1p(target)\n",
    "\n",
    "pp_flag = True\n",
    "train_name = './model_data/btrain_fullfeat_%s.csv'%leak_val\n",
    "test_name = './model_data/btest_fullfeat_%s.csv'%leak_val\n",
    "if pp_flag:\n",
    "    btrain, btest = format_for_training_v1(train, test, fnames, train_leak, test_leak, trn_res, tst_res, leak_val)\n",
    "    print '\\nSaving generated datasets...'\n",
    "    btrain.to_csv(train_name, index=False)\n",
    "    btest.to_csv(test_name, index=False)\n",
    "else:\n",
    "    print '\\nLoading generated datasets...'\n",
    "    btrain = pd.read_csv(train_name)\n",
    "    btest = pd.read_csv(test_name)\n",
    "\n",
    "# Scale dataset for booster training\n",
    "boost_train, boost_test = scale_for_training(btrain, btest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training a CatBoost Regressor\n",
    "def train_catboost_regressor(train, target, test, params):\n",
    "    '''\n",
    "    Function for training a catboost regressor\n",
    "    '''\n",
    "    test_predictions = np.zeros(test.shape[0])\n",
    "    train_predictions = np.zeros(train.shape[0])\n",
    "    kfold = KFold(n_splits=4)\n",
    "    val_errors = np.zeros(kfold.n_splits)\n",
    "    trn_errors = np.zeros(kfold.n_splits)\n",
    "    \n",
    "    for i, (trn, val) in enumerate(kfold.split(train)):\n",
    "        params['random_seed'] = i\n",
    "        print '\\nTraining on fold:', i\n",
    "        round_name = 'round_%s'%i\n",
    "        xtrain = train[trn, :]\n",
    "        ytrain = target[trn]\n",
    "\n",
    "        xval = train[val, :]\n",
    "        yval = target[val]\n",
    "\n",
    "        train_pool = Pool(xtrain, ytrain)\n",
    "        validate_pool = Pool(xval, yval)\n",
    "        \n",
    "        model = CatBoostRegressor(**params)\n",
    "        model.fit(train_pool, eval_set=validate_pool)\n",
    "\n",
    "        # Make predictions\n",
    "        test_predictions += model.predict(test)\n",
    "        \n",
    "        # Get validation and train errors\n",
    "        valid_pred = model.predict(xval)\n",
    "        train_pred = model.predict(train)\n",
    "        train_predictions += train_pred\n",
    "        val_errors[i] = np.sqrt(mean_squared_error(valid_pred, yval))\n",
    "        trn_errors[i] = np.sqrt(mean_squared_error(train_pred, target))\n",
    "    \n",
    "    # Average over cv folds\n",
    "    test_predictions /= kfold.n_splits\n",
    "    train_predictions /= kfold.n_splits\n",
    "    \n",
    "    return trn_errors, val_errors, test_predictions, train_predictions\n",
    "\n",
    "\n",
    "# Function for evaluating trouble samples\n",
    "trouble_idx = np.where(train_leak==0)[0]\n",
    "def selective_eval(preds, target, idx):\n",
    "    pred_subset = preds[idx]\n",
    "    target_subset = target[idx]\n",
    "    return mean_squared_error(pred_subset, target_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost Regressor Parameters\n",
    "params = {'iterations': 30000,\n",
    "          'random_seed': 0,\n",
    "          'loss_function': 'RMSE',\n",
    "          'eval_metric': 'RMSE',\n",
    "          'od_type': 'Iter',\n",
    "          'od_wait': 20,\n",
    "          'learning_rate': 0.043,\n",
    "          'depth': 6,\n",
    "          'bagging_temperature': 9,\n",
    "          'l2_leaf_reg': 0.113,\n",
    "          'verbose': 1000,\n",
    "          'use_best_model': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Tune CatBoost Regressor Hyper-Parameters\n",
    "tune_flag = False\n",
    "if tune_flag:\n",
    "    cv_results = {}\n",
    "\n",
    "    cv_entry = 'l2_leaf_reg'\n",
    "    cv_set = [0.113, 0.115, 0.117]\n",
    "    \n",
    "#     cv_entry = 'learning_rate'\n",
    "#     cv_set = [0.042, 0.043, 0.044]\n",
    "\n",
    "#     cv_entry = 'depth'\n",
    "#     cv_set = [9, 10, 11]\n",
    "\n",
    "#     cv_entry = 'bagging_temperature'\n",
    "#     cv_set = [1, 5, 10]\n",
    "\n",
    "    for cv_val in cv_set:\n",
    "        print '\\nCross validating with %s: %s'%(cv_entry, cv_val)\n",
    "        params[cv_entry] = cv_val\n",
    "        train_err, valid_err, tst_preds, trn_preds = train_catboost_regressor(train=boost_train, \n",
    "                                                                              target=target_log, \n",
    "                                                                              test=boost_test, \n",
    "                                                                              params=params)\n",
    "        cv_results[cv_entry+'_miss_train_val_%s'%cv_val] = (selective_eval(trn_preds, target_log, trouble_idx),\n",
    "                                                            np.mean(train_err), \n",
    "                                                            np.mean(valid_err))\n",
    "        \n",
    "    # Show cv results\n",
    "    print '\\nCV Results:'\n",
    "    for key in cv_results.keys():\n",
    "        print key, cv_results[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_err, valid_err, tst_preds, trn_preds = train_catboost_regressor(train=boost_train, \n",
    "                                                                      target=target_log, \n",
    "                                                                      test=boost_test, \n",
    "                                                                      params=params)\n",
    "print 'Train score:', np.mean(train_err) \n",
    "print 'Validation score:', np.mean(valid_err)\n",
    "print '\\nTrouble score:', selective_eval(trn_preds, target_log, trouble_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target_v1(leak, preds):\n",
    "    exp_preds = np.expm1(preds)\n",
    "    \n",
    "    fill_idx = np.where(leak==0)[0]\n",
    "    \n",
    "    tmp_leak = leak.copy()\n",
    "    tmp_leak[fill_idx] = exp_preds[fill_idx]\n",
    "    return tmp_leak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make submission\n",
    "save_flag=True\n",
    "\n",
    "tst_target = make_target_v1(test_leak, tst_preds)\n",
    "\n",
    "ori_target = np.expm1(tst_preds)\n",
    "trn_target = np.expm1(trn_preds)\n",
    "\n",
    "sub_name_tst = '../submissions/tstest_cat_2v3_lag%s_fullfeat_submit.csv'%leak_val\n",
    "\n",
    "sub_name_ori = './model_data/tstest_cat_2v3_lag%s_fullfeat_test.csv'%leak_val\n",
    "sub_name_trn = './model_data/tstest_cat_2v3_lag%s_fullfeat_train.csv'%leak_val\n",
    "\n",
    "tst_df = pd.DataFrame()\n",
    "ori_df = pd.DataFrame()\n",
    "trn_df = pd.DataFrame()\n",
    "\n",
    "tst_df['ID'] = test['ID']\n",
    "ori_df['ID'] = test['ID']\n",
    "trn_df['ID'] = train['ID']\n",
    "\n",
    "tst_df['target'] = tst_target\n",
    "ori_df['target'] = ori_target\n",
    "trn_df['target'] = trn_target\n",
    "\n",
    "if save_flag:\n",
    "    tst_df.to_csv(sub_name_tst, index=False)\n",
    "    ori_df.to_csv(sub_name_ori, index=False)\n",
    "    trn_df.to_csv(sub_name_trn, index=False)\n",
    "\n",
    "tst_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_square_log_diff(sub1, sub2):\n",
    "    return np.sqrt(np.mean(np.square(np.subtract(np.log1p(sub1), np.log1p(sub2)))))\n",
    "\n",
    "def sum_abs_diff(sub1, sub2):\n",
    "    return np.sum(np.abs(np.subtract(sub1, sub2)))\n",
    "\n",
    "fri_df = pd.read_csv('../submissions/tstest_lgb_2v2_lag36_submit_0.52.csv')\n",
    "sat_df = pd.read_csv('../submissions/tstest_lgb_2v2_lag38_bad_submit_0.52.csv')\n",
    "sun_df = pd.read_csv('../submissions/tstest_lgb_2v2_lag38_good_submit.csv')\n",
    "tmp_df = pd.read_csv('../submissions/tstest_cat_2v3_lag38_submit.csv')\n",
    "\n",
    "right_idx = np.where(test_leak!=0)[0]\n",
    "wrong_idx = np.where(test_leak==0)[0]\n",
    "\n",
    "sub_right = tst_df['target'].values[right_idx]\n",
    "sun_right = sun_df['target'].values[right_idx]\n",
    "fri_right = fri_df['target'].values[right_idx]\n",
    "sat_right = sat_df['target'].values[right_idx]\n",
    "tmp_right = tmp_df['target'].values[right_idx]\n",
    "\n",
    "sub_wrong = tst_df['target'].values[wrong_idx]\n",
    "sun_wrong = sun_df['target'].values[wrong_idx]\n",
    "fri_wrong = fri_df['target'].values[wrong_idx]\n",
    "sat_wrong = sat_df['target'].values[wrong_idx]\n",
    "tmp_wrong = tmp_df['target'].values[wrong_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Sum abs diff:'\n",
    "print sum_abs_diff(sub_right, tmp_right)\n",
    "print sum_abs_diff(sub_wrong, tmp_wrong)\n",
    "\n",
    "print '\\nRoot mean square log diff:'\n",
    "print root_mean_square_log_diff(sub_right, tmp_right)\n",
    "print root_mean_square_log_diff(sub_wrong, tmp_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Sum abs diff:'\n",
    "print sum_abs_diff(sub_right, sun_right)\n",
    "print sum_abs_diff(sub_wrong, sun_wrong)\n",
    "\n",
    "print '\\nRoot mean square log diff:'\n",
    "print root_mean_square_log_diff(sub_right, sun_right)\n",
    "print root_mean_square_log_diff(sub_wrong, sun_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexicons():\n",
    "    lexi_train = load_pickle('./model_data/lexi_train.pickle')\n",
    "    lexi_test = load_pickle('./model_data/lexi_test.pickle')\n",
    "    return lexi_train, lexi_test\n",
    "\n",
    "\n",
    "def preds_to_lexicon(preds, lexicon):\n",
    "    new_preds = np.zeros(preds.shape[0])\n",
    "    \n",
    "    lex_matrix = lexicon * np.ones((preds.shape[0], lexicon.shape[0]))\n",
    "    \n",
    "    diff = (lex_matrix.T - preds).T\n",
    "    diff = np.abs(diff)\n",
    "    mins = np.argmin(diff, axis=1)\n",
    "    \n",
    "    return lexicon[mins]\n",
    "\n",
    "\n",
    "def evaluate_lexicon(data, target, leak, preds, lexi):\n",
    "    lexi_transform = preds_to_lexicon(preds, lexi)\n",
    "    \n",
    "    lexi_df = pd.DataFrame()\n",
    "    lexi_df['ID'] = data['ID']\n",
    "    lexi_df['target'] = target\n",
    "    lexi_df['leak'] = leak\n",
    "    lexi_df['pred'] = preds\n",
    "    lexi_df['lexi_pred'] = lexi_transform\n",
    "    \n",
    "    lexi_df['SLE'] = np.square(np.subtract(np.log1p(lexi_df['target']), np.log1p(lexi_df['pred'])))\n",
    "    lexi_df['lexi_SLE'] = np.square(np.subtract(np.log1p(lexi_df['target']), np.log1p(lexi_df['lexi_pred'])))\n",
    "    lexi_df['SLE_diff'] = np.abs(np.subtract(lexi_df['SLE'], lexi_df['lexi_SLE']))\n",
    "    \n",
    "    return lexi_df \n",
    "\n",
    "\n",
    "def get_errors(lexi):\n",
    "    zero_idx = lexi['leak']==0\n",
    "    \n",
    "    pred_sle_missed = lexi.loc[zero_idx, 'SLE']\n",
    "    lexi_sle_missed = lexi.loc[zero_idx, 'lexi_SLE']\n",
    "    \n",
    "    print 'Trouble error w/o lexicon:', np.sqrt(np.mean(pred_sle_missed))\n",
    "    print 'Trouble error w/ lexicon:', np.sqrt(np.mean(lexi_sle_missed))\n",
    "    return None\n",
    "\n",
    "\n",
    "lexi_train, lexi_test = load_lexicons()\n",
    "\n",
    "train_lexi_eval = evaluate_lexicon(train, target, train_leak, np.expm1(trn_preds), lexi_train)\n",
    "get_errors(train_lexi_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lexi_eval.sort_values(by='SLE_diff', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
