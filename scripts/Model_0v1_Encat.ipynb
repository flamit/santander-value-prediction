{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 0 Model: Encoding with CatBoost\n",
    "Inspired from: https://www.kaggle.com/tanreinama/catboost-stackedae-with-mxnet-meta-1-40lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
    "\n",
    "import time\n",
    "import os\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load h5py file\n",
    "def loadh5(fname, dname):\n",
    "    h5f = h5py.File(fname, 'r')\n",
    "    data = h5f[dname][:]\n",
    "    h5f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickle file\n",
    "def loadpickle(fname):\n",
    "    with open(fname, 'rb') as handle:\n",
    "        data = pkl.load(handle)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row-wise metadata generator\n",
    "def generate_meta(df):\n",
    "    meta = pd.DataFrame({\n",
    "        'nonsparse_count': (df[df==0]).fillna(1).sum(axis=1),\n",
    "        'sum': df[df!=0].sum(axis=1),\n",
    "        'mean': df[df!=0].mean(axis=1),\n",
    "        'std': df[df!=0].std(axis=1),\n",
    "        'median': df[df!=0].median(axis=1),\n",
    "        'max': df[df!=0].max(axis=1),\n",
    "        'min': df[df!=0].min(axis=1),\n",
    "        'var': df[df!=0].var(axis=1)})\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'\n",
    "train_dname = 'train_s0'\n",
    "test_dname = 'test_s0'\n",
    "f_ext = '_vanilla.h5'\n",
    "\n",
    "load_start = time.time()\n",
    "print 'Loading Stage 0 vanilla train and test datasets...'\n",
    "# Load h5py data\n",
    "train_data = loadh5(data_path + train_dname + f_ext, train_dname)\n",
    "test_data = loadh5(data_path + test_dname + f_ext, test_dname)\n",
    "# Load dataframe indexes\n",
    "train_idx = loadpickle(data_path + 'train_idx.pkl')\n",
    "test_idx = loadpickle(data_path + 'test_idx.pkl')\n",
    "# Load dataframe column names\n",
    "train_cols = loadpickle(data_path + 'train_cols.pkl')\n",
    "test_cols = loadpickle(data_path + 'test_cols.pkl')\n",
    "\n",
    "# Create dataframes\n",
    "train_df = pd.DataFrame(data=train_data, index=train_idx, columns=train_cols)\n",
    "test_df = pd.DataFrame(data=test_data, index=test_idx, columns=test_cols)\n",
    "\n",
    "print 'Loading completed in %s seconds'%(time.time()-load_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata data path\n",
    "meta_path = data_path + 'meta_stage0v1.csv'\n",
    "# Autoencoder data path\n",
    "auto_type = '155'\n",
    "autodir_path = './autoencoder/data/'\n",
    "autodata_path = autodir_path + 'data_' + auto_type + '.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format label values\n",
    "Y = np.log1p(train_df.target.values)\n",
    "train_df.drop(columns=['target'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create master dataset and perform scaling \n",
    "X = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "scaled_X = X.div(X.max(), axis='columns')\n",
    "# Scale labels\n",
    "y_min = np.min(Y)\n",
    "y_max = np.max(Y)\n",
    "scaled_Y = (Y - y_min)/(y_max - y_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get autoencoder and metadata results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder data\n",
    "autodata = pd.read_csv(autodata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row-wise metadata\n",
    "if not os.path.exists(meta_path):\n",
    "    print 'Generating row-wise metadata...'\n",
    "    meta_X = generate_meta(scaled_X)\n",
    "    meta_X.to_csv(meta_path, index=False)\n",
    "    print 'Row-wise metadata generated!'\n",
    "else:\n",
    "    print 'Loading row-wise metadata...'\n",
    "    meta_X = pd.read_csv(meta_path)\n",
    "    print 'Row-wise metadata loaded!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Encodings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=0\n",
    "num_clusters1 = 24\n",
    "num_clusters2 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Running Mini-Batch KMeans...'\n",
    "mbkm = MiniBatchKMeans(n_clusters=num_clusters2, random_state=random_state)\n",
    "mbkm_X = pd.DataFrame(mbkm.fit_transform(scaled_X))\n",
    "print 'Running PCA...'\n",
    "pca = PCA(n_components=num_clusters2, random_state=random_state)\n",
    "pca_X = pd.DataFrame(pca.fit_transform(scaled_X))\n",
    "print 'Running Truncated SVD...'\n",
    "tsvd = TruncatedSVD(n_components=num_clusters2, random_state=random_state)\n",
    "tsvd_X = pd.DataFrame(tsvd.fit_transform(scaled_X))\n",
    "print 'Running Gaussian Random Projection...'\n",
    "grp = GaussianRandomProjection(n_components=num_clusters1, eps=0.1, random_state=random_state)\n",
    "grp_X = pd.DataFrame(grp.fit_transform(scaled_X))\n",
    "print 'Running Sparse Random Projection...'\n",
    "srp = SparseRandomProjection(n_components=num_clusters1, dense_output=True, random_state=random_state)\n",
    "srp_X = pd.DataFrame(srp.fit_transform(scaled_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Catboost with Cross Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all data\n",
    "X_all = pd.concat([scaled_X, meta_X, autodata, mbkm_X, pca_X, tsvd_X, grp_X, srp_X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = []\n",
    "print 'Training Catboost model...'\n",
    "for fold_id, (IDX_train, IDX_test) in enumerate(KFold(n_splits=5, random_state=random_state, shuffle=False).split(scaled_Y)):\n",
    "    # Partition training and test sets\n",
    "    X_train = X_all.iloc[IDX_train].values\n",
    "    X_test = X_all.iloc[IDX_test].values\n",
    "    Y_train = scaled_Y[IDX_train]\n",
    "    Y_test = scaled_Y[IDX_test]\n",
    "    \n",
    "    # Define Catboost model\n",
    "    cb_reg = CatBoostRegressor(iterations=500,\n",
    "                               learning_rate=0.05, \n",
    "                               depth=10,\n",
    "                               eval_metric='RMSE',\n",
    "                               random_seed=fold_id, \n",
    "                               bagging_temperature=0.2,\n",
    "                               od_type='Iter', \n",
    "                               metric_period=50,  # Rounds to process before calculating objective\n",
    "                               od_wait=20)\n",
    "    cb_reg.fit(X_train, Y_train, eval_set=(X_test, Y_test), cat_features=[], use_best_model=True, verbose=True)\n",
    "    target_pred = cb_reg.predict(X_all.iloc[scaled_Y.shape[0]:])\n",
    "    # Rescale target predictions and append to submissions list\n",
    "    target_pred = target_pred * (y_max - y_min) + y_min\n",
    "    submissions.append(np.expm1(target_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format submissions\n",
    "mean_submissions = np.mean(submissions, axis=0)\n",
    "result = pd.DataFrame({'ID': test_idx, 'target': mean_submissions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submissions\n",
    "result_path = '../submissions/encat_0v1_submit.csv'\n",
    "result.to_csv(result_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
