{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correcting Covariate Shift\n",
    "An application of the Kullback-Leibler Importance Estimation Procedure on the Santander Value Prediction datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "\n",
    "import pdb\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-level parameters\n",
    "debug=True\n",
    "random_state=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions and Classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniqueTransformer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Class with fit and transform methods for removing duplicate columns from a dataset\n",
    "    **fit** finds the indexes of unique columns using numpy unique\n",
    "    **transform** returns the dataset with the indexes of unique columns\n",
    "    '''\n",
    "    def __init__(self, axis=1):\n",
    "        if axis==0:\n",
    "            raise NotImplementedError('Axis is 0! Not implemented!')\n",
    "        self.axis=axis\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print 'Finding unique indexes...'\n",
    "        _, self.unique_indexes_ = np.unique(X, axis=self.axis, return_index=True)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print 'Filtering for only unique columns...'\n",
    "        return X[:, self.unique_indexes_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for setting up\n",
    "def get_input(debug=False):\n",
    "    '''\n",
    "    Function for loading either debug or full datasets\n",
    "    '''\n",
    "    if debug:\n",
    "        print 'Loading debug train and test datasets...'\n",
    "        train = pd.read_csv('../../data/train_debug.csv')\n",
    "        test = pd.read_csv('../../data/test_debug.csv')\n",
    "    else:\n",
    "        print 'Loading original train and test datasets...'\n",
    "        train = pd.read_csv('../data/train.csv')\n",
    "        test = pd.read_csv('../data/test.csv')\n",
    "    y_train_log = np.log1p(train['target'])\n",
    "    id_test = test['ID']\n",
    "    # Drop unnecessary columns\n",
    "    train.drop(labels=['ID', 'target'], axis=1, inplace=True)\n",
    "    test.drop(labels=['ID'], axis=1, inplace=True)\n",
    "    # Find shape of loaded datasets\n",
    "    print('Shape of training dataset: {} Rows, {} Columns'.format(*train.shape))\n",
    "    print('Shape of test dataset: {} Rows, {} Columns'.format(*test.shape))\n",
    "\n",
    "    return train.values, y_train_log.values, test.values, id_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for retrieving width list\n",
    "def get_width(debug=False):\n",
    "    '''\n",
    "    Function for loading either debug or full width lists\n",
    "    '''\n",
    "    if debug:\n",
    "        return [10, 1000]\n",
    "    else:\n",
    "        return [0.1, 1, 10, 100, 1000, 10000, 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating Gaussian kernel value\n",
    "def calc_gaussian(x, center, width):\n",
    "    return np.exp(-(np.square(np.linalg.norm(np.subtract(x, center))))/(2*np.square(width)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating importance weights\n",
    "def get_importances(data, alpha, kc, kw):\n",
    "    importance_weights = np.zeros(len(data))\n",
    "    for i, row in enumerate(data):\n",
    "        kernel_sum = 0\n",
    "        for j, center in enumerate(kc):\n",
    "            kernel_sum += alpha[j]*calc_gaussian(row, center, kw)\n",
    "        importance_weights[i] = kernel_sum\n",
    "    return importance_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kullback-Leibler Importance Estimation Procedure training function\n",
    "def train_KLIEP(train, test, num_kernels=100, kernel_width=10, lr=0.001, a_val=1, stop=0.00001):\n",
    "    '''\n",
    "    Function for getting KLIEP weights for a given training and test set\n",
    "    '''\n",
    "    # Instantiate kernel centers\n",
    "    kernel_idx_bag = np.random.permutation(len(test))\n",
    "    kernel_idx = np.array([np.random.choice(kernel_idx_bag) for i in range(num_kernels)])\n",
    "    kernel_centers = test[kernel_idx, :]\n",
    "    # Compute A matrix\n",
    "    A = np.zeros(shape=(len(test), len(kernel_centers)))\n",
    "    for i, row in enumerate(test):\n",
    "        for j, center in enumerate(kernel_centers):\n",
    "            A[i, j] = calc_gaussian(row, center, kernel_width)\n",
    "    # Compute b vector\n",
    "    b = np.zeros(num_kernels)\n",
    "    for j, center in enumerate(kernel_centers):\n",
    "        temp_sum = 0\n",
    "        for row in train:\n",
    "            temp_sum += calc_gaussian(row, center, kernel_width)\n",
    "        b[j] = temp_sum/np.float16(len(train))\n",
    "    # Initialize alpha vector\n",
    "    alpha = a_val * np.ones(shape=num_kernels)\n",
    "    # Begin training\n",
    "    alpha_old = np.zeros(shape=num_kernels)\n",
    "    counter = 0\n",
    "    while True:\n",
    "        alpha = np.add(alpha, lr*np.matmul(A.T, np.divide(np.ones(len(test)), np.matmul(A, alpha))))\n",
    "        alpha = np.add(alpha, np.divide(np.multiply((1-np.dot(b, alpha)), b), np.dot(b, b)))\n",
    "        alpha = np.maximum(np.zeros(num_kernels), alpha)\n",
    "        alpha = np.divide(alpha, np.dot(b, alpha))\n",
    "        # Check convergence by average deviation\n",
    "        deviation = np.linalg.norm(np.subtract(alpha, alpha_old))\n",
    "        if deviation < stop*np.linalg.norm(alpha_old):\n",
    "            print 'Converged in %s iterations!'%counter\n",
    "            importance_weights = get_importances(data=test, alpha=alpha, kc=kernel_centers, kw=kernel_width)\n",
    "            return importance_weights, alpha, kernel_centers\n",
    "            break\n",
    "        else:\n",
    "            counter += 1\n",
    "            alpha_old = alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting the best model\n",
    "def get_best_KLIEP(train, test, width_list, n_splits=10, num_kernels=100, lr=0.001, a_val=1, stop=0.00001):\n",
    "    '''\n",
    "    Function for tuning KLIEP kernel performance\n",
    "    '''\n",
    "    # Split test set into disjoint subsets\n",
    "    split_sets = []\n",
    "    kf = KFold(n_splits=n_splits, shuffle=False, random_state=random_state)\n",
    "    print 'Splitting test set into %s disjoint subsets...'%n_splits\n",
    "    for _, test_idx in kf.split(test):\n",
    "        split_sets.append(test[test_idx, :])\n",
    "    # Evaluate each model\n",
    "    j_models = []\n",
    "    alpha_list = []\n",
    "    centers_list = []\n",
    "    for idx, w in enumerate(width_list):\n",
    "        print 'Working on split set %s'%idx\n",
    "        print 'Evaluating KLIEP model with Gaussian kernel width of %s...'%w\n",
    "        j_avglist = []\n",
    "        for s in split_sets:\n",
    "            importance, alpha, center = train_KLIEP(train=train, test=s, num_kernels=num_kernels, kernel_width=w, \n",
    "                                     lr=lr, a_val=a_val, stop=stop)\n",
    "            j_avglist.append(np.mean(np.log(importance)))\n",
    "        j_models.append(np.mean(j_avglist))\n",
    "        alpha_list.append(alpha)\n",
    "        centers_list.append(center)\n",
    "    # Use best model to evaluate train set KLIEP importances\n",
    "    best_idx = np.argmax(np.array(j_models))\n",
    "    print '\\nBest width was: %s'%width_list[best_idx]\n",
    "    importance_weights = get_importances(data=train, alpha=alpha_list[best_idx], kc=centers_list[best_idx], \n",
    "                                         kw=width_list[best_idx])\n",
    "    return width_list[best_idx], importance_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading debug train and test datasets...\n",
      "Shape of training dataset: 100 Rows, 4992 Columns\n",
      "Shape of test dataset: 200 Rows, 4992 Columns\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "xtrain, ytrain_log, xtest, id_test = get_input(debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load width list\n",
    "wlist = get_width(debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding unique indexes...\n",
      "Filtering for only unique columns...\n",
      "Filtering for only unique columns...\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate columns\n",
    "unique = UniqueTransformer()\n",
    "unique.fit(X=xtrain)\n",
    "xtrain = unique.transform(X=xtrain)\n",
    "xtest = unique.transform(X=xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "xdata = np.concatenate([xtrain, xtest], axis=0)\n",
    "scaler = StandardScaler()\n",
    "xdata_scaled = scaler.fit_transform(X=xdata)\n",
    "xtrain_scaled = xdata_scaled[:len(xtrain), :]\n",
    "xtest_scaled = xdata_scaled[len(xtrain):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting test set into 10 disjoint subsets...\n",
      "Working on split set 0\n",
      "Evaluating KLIEP model with Gaussian kernel width of 10...\n",
      "Converged in 4444 iterations!\n",
      "Converged in 3966 iterations!\n",
      "Converged in 9479 iterations!\n",
      "Converged in 5921 iterations!\n",
      "Converged in 7338 iterations!\n",
      "Converged in 4391 iterations!\n",
      "Converged in 4062 iterations!\n",
      "Converged in 6692 iterations!\n",
      "Converged in 4913 iterations!\n",
      "Converged in 5706 iterations!\n",
      "Working on split set 1\n",
      "Evaluating KLIEP model with Gaussian kernel width of 1000...\n",
      "Converged in 26689 iterations!\n",
      "Converged in 9096 iterations!\n",
      "Converged in 14156 iterations!\n",
      "Converged in 21934 iterations!\n",
      "Converged in 8535 iterations!\n",
      "Converged in 29425 iterations!\n",
      "Converged in 21646 iterations!\n",
      "Converged in 19804 iterations!\n",
      "Converged in 34565 iterations!\n",
      "Converged in 21938 iterations!\n",
      "\n",
      "Best width was: 10\n"
     ]
    }
   ],
   "source": [
    "# Get KLIEP weights\n",
    "best_width, importances = get_best_KLIEP(xtrain_scaled, xtest_scaled, wlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save KLIEP results\n",
    "save_name = 'kliep_weights_%s.pickle'%best_width\n",
    "with open(save_name, 'wb') as handle:\n",
    "    pkl.dump(obj=importances, file=handle, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
