{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correcting Covariate Shift\n",
    "An application of the Kullback-Leibler Importance Estimation Procedure on the Santander Value Prediction datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "import h5py\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-level parameters\n",
    "debug=True\n",
    "random_state=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions and Classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniqueTransformer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Class with fit and transform methods for removing duplicate columns from a dataset\n",
    "    **fit** finds the indexes of unique columns using numpy unique\n",
    "    **transform** returns the dataset with the indexes of unique columns\n",
    "    '''\n",
    "    def __init__(self, axis=1):\n",
    "        if axis==0:\n",
    "            raise NotImplementedError('Axis is 0! Not implemented!')\n",
    "        self.axis=axis\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print 'Finding unique indexes...'\n",
    "        _, self.unique_indexes_ = np.unique(X, axis=self.axis, return_index=True)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print 'Filtering for only unique columns...'\n",
    "        return X[:, self.unique_indexes_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading h5py file\n",
    "def load_h5py(fname):\n",
    "    with h5py.File(fname, 'r') as handle:\n",
    "        return handle['data'][:]\n",
    "# Function for loading pickle file\n",
    "def load_pickle(fname):\n",
    "    with open(fname, 'rb') as handle:\n",
    "        return pkl.load(handle)\n",
    "# Function for saving pickle file\n",
    "def save_pickle(fname, data):\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pkl.dump(obj=data, file=handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for setting up\n",
    "def get_input(debug=False):\n",
    "    '''\n",
    "    Function for loading either debug or full datasets\n",
    "    '''\n",
    "    if debug:\n",
    "        print 'Loading debug train and test datasets...'\n",
    "        train = load_h5py('../../data/compressed/debug_train.h5')\n",
    "        test = load_h5py('../../data/compressed/debug_test.h5')\n",
    "        id_test = load_pickle('../../data/compressed/debug_test_id.pickle')\n",
    "        y_train_log = np.log1p(load_pickle('../../data/compressed/debug_target.pickle'))\n",
    "    else:\n",
    "        print 'Loading original train and test datasets...'\n",
    "        train = load_h5py('../../data/compressed/full_train.h5')\n",
    "        test = load_h5py('../../data/compressed/full_test.h5')\n",
    "        id_test = load_pickle('../../data/compressed/full_test_id.pickle')\n",
    "        y_train_log = np.log1p(load_pickle('../../data/compressed/full_target.pickle'))\n",
    "    # Find shape of loaded datasets\n",
    "    print('Shape of training dataset: {} Rows, {} Columns'.format(*train.shape))\n",
    "    print('Shape of test dataset: {} Rows, {} Columns'.format(*test.shape))\n",
    "\n",
    "    return train, y_train_log, test, id_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating Gaussian kernel value\n",
    "def calc_gaussian(x, center, width):\n",
    "    return np.exp(-(np.square(np.linalg.norm(np.subtract(x, center))))/(2*np.square(width)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating importance weights\n",
    "def get_importances(data, alpha, kc, kw):\n",
    "    importance_weights = np.zeros(len(data))\n",
    "    for i, row in enumerate(data):\n",
    "        kernel_sum = 0\n",
    "        for j, center in enumerate(kc):\n",
    "            kernel_sum += alpha[j]*calc_gaussian(row, center, kw)\n",
    "        importance_weights[i] = kernel_sum\n",
    "    return importance_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kullback-Leibler Importance Estimation Procedure training function\n",
    "def train_KLIEP(train, test, num_kernels=100, kernel_width=10, lr=0.001, a_val=1, stop=0.00001):\n",
    "    '''\n",
    "    Function for getting KLIEP weights for a given training and test set\n",
    "    '''\n",
    "    # Instantiate kernel centers\n",
    "    kernel_idx_bag = np.random.permutation(len(test))\n",
    "    kernel_idx = np.array([np.random.choice(kernel_idx_bag) for i in range(num_kernels)])\n",
    "    kernel_centers = test[kernel_idx, :]\n",
    "    # Compute A matrix\n",
    "    A = np.zeros(shape=(len(test), len(kernel_centers)))\n",
    "    for i, row in enumerate(test):\n",
    "        for j, center in enumerate(kernel_centers):\n",
    "            A[i, j] = calc_gaussian(row, center, kernel_width)\n",
    "    # Compute b vector\n",
    "    b = np.zeros(num_kernels)\n",
    "    for j, center in enumerate(kernel_centers):\n",
    "        temp_sum = 0\n",
    "        for row in train:\n",
    "            temp_sum += calc_gaussian(row, center, kernel_width)\n",
    "        b[j] = temp_sum/np.float16(len(train))\n",
    "    # Initialize alpha vector\n",
    "    alpha = a_val * np.ones(shape=num_kernels)\n",
    "    # Begin training\n",
    "    alpha_old = np.zeros(shape=num_kernels)\n",
    "    counter = 0\n",
    "    while True:\n",
    "        alpha = np.add(alpha, lr*np.matmul(A.T, np.divide(np.ones(len(test)), np.matmul(A, alpha))))\n",
    "        alpha = np.add(alpha, np.divide(np.multiply((1-np.dot(b, alpha)), b), np.dot(b, b)))\n",
    "        alpha = np.maximum(np.zeros(num_kernels), alpha)\n",
    "        alpha = np.divide(alpha, np.dot(b, alpha))\n",
    "        # Check convergence by average deviation\n",
    "        deviation = np.linalg.norm(np.subtract(alpha, alpha_old))\n",
    "        if deviation < stop*np.linalg.norm(alpha_old):\n",
    "            print 'Converged in %s iterations!'%counter\n",
    "            importance_weights = get_importances(data=test, alpha=alpha, kc=kernel_centers, kw=kernel_width)\n",
    "            return importance_weights, alpha, kernel_centers, counter\n",
    "            break\n",
    "        else:\n",
    "            counter += 1\n",
    "            alpha_old = alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting the best model\n",
    "def get_best_KLIEP(train, test, width_list, path, n_splits=10, num_kernels=100, lr=0.001, a_val=1, stop=0.00001):\n",
    "    '''\n",
    "    Function for tuning KLIEP kernel performance\n",
    "    '''\n",
    "    # Split test set into disjoint subsets\n",
    "    split_sets = []\n",
    "    kf = KFold(n_splits=n_splits, shuffle=False, random_state=random_state)\n",
    "    print 'Splitting test set into %s disjoint subsets...'%n_splits\n",
    "    for _, test_idx in kf.split(test):\n",
    "        split_sets.append(test[test_idx, :])\n",
    "    # Evaluate each model\n",
    "    j_models = []\n",
    "    alpha_list = []\n",
    "    centers_list = []\n",
    "    counts = []\n",
    "    for idx, w in enumerate(width_list):\n",
    "        print '\\nWorking on split set %s'%idx\n",
    "        print 'Evaluating KLIEP model with Gaussian kernel width of %s...'%w\n",
    "        j_avglist = []\n",
    "        counter_list = []\n",
    "        for s in split_sets:\n",
    "            importance, alpha, center, count = train_KLIEP(train=train, test=s, num_kernels=num_kernels, \n",
    "                                                           kernel_width=w, lr=lr, a_val=a_val, stop=stop)\n",
    "            j_avglist.append(np.mean(np.log(importance)))\n",
    "            counter_list.append(count)\n",
    "        j_models.append(np.mean(j_avglist))\n",
    "        alpha_list.append(alpha)\n",
    "        centers_list.append(center)\n",
    "        counts.append(counter_list)\n",
    "    # Evalulate train set KLIEP importances for all models\n",
    "    eval_results = []\n",
    "    for i, idx in enumerate((-np.array(j_models)).argsort()):\n",
    "        importance_weights = get_importances(data=train, alpha=alpha_list[idx], kc=centers_list[idx], \n",
    "                                             kw=width_list[idx])\n",
    "        eval_results.append({'width': width_list[idx], \n",
    "                             'num_kernels': num_kernels, \n",
    "                             'j_value': j_models[idx],\n",
    "                             'place': i,\n",
    "                             'counts': counts[idx], \n",
    "                             'weights': importance_weights})\n",
    "    # Save train set KLIEP importances for all models\n",
    "    for result in eval_results:\n",
    "        save_pickle(fname=path+'%s_width%s_numk%s.pickle'%(result['place'], result['width'], result['num_kernels']), \n",
    "                    data=result)\n",
    "    \n",
    "    # Return information on best model\n",
    "    print '\\nBest width was: %s'%eval_results[0]['width']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make covariate shift weights storage folder\n",
    "if debug:\n",
    "    cs_path = './debug_cs_weights_v1/'\n",
    "else:\n",
    "    cs_path = './full_cs_weights_v1/'\n",
    "if os.path.exists(cs_path):\n",
    "    print 'Removing old covariate shift weights folder'\n",
    "    shutil.rmtree(cs_path)\n",
    "    print 'Creating new covariate shift weights folder\\n'\n",
    "    os.mkdir(cs_path)\n",
    "else:\n",
    "    print 'Creating new covariate shift weights folder\\n'\n",
    "    os.mkdir(cs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "xtrain, ytrain_log, xtest, id_test = get_input(debug)\n",
    "\n",
    "# Define width list\n",
    "if debug:\n",
    "    wlist = [75, 500]\n",
    "else:\n",
    "    wlist = [75, 100, 110, 120, 130, 140, 150, 200, 250, 300]\n",
    "\n",
    "# Remove duplicate columns\n",
    "unique = UniqueTransformer()\n",
    "unique.fit(X=xtrain)\n",
    "xtrain = unique.transform(X=xtrain)\n",
    "xtest = unique.transform(X=xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA (if necessary) and scale data\n",
    "max_features = 100\n",
    "xdata = np.concatenate([xtrain, xtest], axis=0)\n",
    "if xdata.shape[1] > max_features:\n",
    "    pca = PCA(n_components=max_features)\n",
    "    xdata = pca.fit_transform(xdata)\n",
    "scaler = StandardScaler()\n",
    "xdata_scaled = scaler.fit_transform(X=xdata)\n",
    "xtrain_scaled = xdata_scaled[:len(xtrain), :]\n",
    "xtest_scaled = xdata_scaled[len(xtrain):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of kernels\n",
    "if debug:\n",
    "    num_kernels_list = [100, 1000]\n",
    "else:\n",
    "    num_kernels_list = [100, 250, 500, 750, 1000]\n",
    "# Start training\n",
    "print 'Training KLIEP models...'\n",
    "for nk in num_kernels_list:\n",
    "    print '\\nEvaluating with number of kernels: %s'%nk\n",
    "    get_best_KLIEP(train=xtrain_scaled, \n",
    "                   test=xtest_scaled, \n",
    "                   width_list=wlist,\n",
    "                   num_kernels=nk,\n",
    "                   path=cs_path)\n",
    "print 'KLIEP models trained and weights are saved!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
