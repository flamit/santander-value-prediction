{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost with Covariate Shift Correction\n",
    "Extension of Model 0v2 that has covariate shift via Kullback-Leibler Importance Estimation Procedure integrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "import pickle as pkl\n",
    "import pdb\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin  # For making custom classes\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-level parameters\n",
    "debug=True\n",
    "random_state=17\n",
    "# KLIEP weight parameters\n",
    "num_kernels = 1000\n",
    "gw_val = 130"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading h5py file\n",
    "def load_h5py(fname):\n",
    "    with h5py.File(fname, 'r') as handle:\n",
    "        return handle['data'][:]\n",
    "# Function for loading pickle file\n",
    "def load_pickle(fname):\n",
    "    with open(fname, 'rb') as handle:\n",
    "        return pkl.load(handle)\n",
    "# Function for saving pickle file\n",
    "def save_pickle(fname, data):\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pkl.dump(obj=data, file=handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniqueTransformer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Class with fit and transform methods for removing duplicate columns from a dataset\n",
    "    **fit** finds the indexes of unique columns using numpy unique\n",
    "    **transform** returns the dataset with the indexes of unique columns\n",
    "    '''\n",
    "    def __init__(self, axis=1):\n",
    "        self.axis=axis\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print 'Finding unique indexes...'\n",
    "        _, self.unique_indexes_ = np.unique(X, axis=self.axis, return_index=True)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print 'Filtering for only unique columns...'\n",
    "        return X[:, self.unique_indexes_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierTransformer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Class describing an object that transforms datasets via estimator results\n",
    "    **_get_labels** specifies target value bins and transforms target vector into bin values\n",
    "    '''\n",
    "    def __init__(self, estimator=None, n_classes=2, cv=3):\n",
    "        self.estimator=estimator\n",
    "        self.n_classes=n_classes\n",
    "        self.cv=cv\n",
    "\n",
    "    def _get_labels(self, y):\n",
    "        y_labels = np.zeros(len(y))\n",
    "        y_us = np.sort(np.unique(y))\n",
    "        step = int(len(y_us)/self.n_classes)\n",
    "        for i_class in range(self.n_classes):\n",
    "            if i_class+1 == self.n_classes:  # Edge case where i_class is initialized at 1\n",
    "                y_labels[y >= y_us[i_class*step]] = i_class\n",
    "            else:\n",
    "                y_labels[np.logical_and(y>=y_us[i_class*step], y<y_us[(i_class+1)*step])] = i_class\n",
    "        return y_labels\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print 'Fitting random forest classifier with n_classes = %s'%self.n_classes\n",
    "        y_labels = self._get_labels(y)\n",
    "        kf = KFold(n_splits=self.cv, shuffle=True, random_state=random_state)\n",
    "        self.estimators_ = []\n",
    "        # Train individual classifiers\n",
    "        for train, _ in kf.split(X, y_labels):\n",
    "            self.estimators_.append(clone(self.estimator).fit(X[train], y_labels[train]))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print 'Applying classifier transformation with n_classes = %s'%self.n_classes\n",
    "        kf = KFold(n_splits=self.cv, shuffle=False, random_state=random_state)\n",
    "\n",
    "        X_prob = np.zeros((X.shape[0], self.n_classes))\n",
    "        X_pred = np.zeros(X.shape[0])\n",
    "\n",
    "        for estimator, (_, test) in zip(self.estimators_, kf.split(X)):\n",
    "            X_prob[test] = estimator.predict_proba(X[test])\n",
    "            X_pred[test] = estimator.predict(X[test])\n",
    "        return np.hstack([X_prob, np.array([X_pred]).T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for transforming a row into statistical values\n",
    "def apply_stats_to_row(row):\n",
    "    stats = []\n",
    "    for fun in stat_functions:\n",
    "        stats.append(fun(row))\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatsTransformer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Class describing an object for transforming datasets into statistical values row-wise\n",
    "    NOTE: This class is dependent on the function **apply_stats_to_row**\n",
    "    '''\n",
    "    def __init__(self, verbose=0, n_jobs=-1, pre_dispatch='2*n_jobs'):\n",
    "        self.verbose = verbose\n",
    "        self.n_jobs = n_jobs\n",
    "        self.pre_dispatch = pre_dispatch\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print 'Applying statistical transformation to dataset...'\n",
    "        parallel = Parallel(n_jobs=self.n_jobs, pre_dispatch=self.pre_dispatch, verbose=self.verbose)\n",
    "        # Get statistics transformation\n",
    "        stats_list = parallel(delayed(apply_stats_to_row)(X[i_smpl, :]) for i_smpl in range(len(X)))\n",
    "        return np.array(stats_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _StatFunAdaptor:\n",
    "    '''\n",
    "    Class describing an object that wraps pre-processing functions with a main statistical function\n",
    "    **__init__** sets up the object parameters\n",
    "    **__call__** describes routine steps when object is called\n",
    "    '''\n",
    "    def __init__(self, stat_fun, *funs, **stat_fun_kwargs):\n",
    "        self.stat_fun = stat_fun\n",
    "        self.funs = funs\n",
    "        self.stat_fun_kwargs = stat_fun_kwargs\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x[x != 0]  # Only look at nonzero entries\n",
    "        # Transform row with cached functions\n",
    "        for fun in self.funs:\n",
    "            x = fun(x)\n",
    "        if x.size == 0:\n",
    "            return -99999  # Edge case default\n",
    "        return self.stat_fun(x, **self.stat_fun_kwargs)  # Returns result of a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff2(x):\n",
    "    return np.diff(x, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stat_funs():\n",
    "    '''\n",
    "    Function for defining all the statistical functions used for evaluating elements in a row-wise manner\n",
    "    Functions include: length, minimum, maximum, standard deviation, skew, kurtosis, and percentile\n",
    "    '''\n",
    "    stat_funs = []\n",
    "\n",
    "    stats = [len, np.min, np.max, np.median, np.std, skew, kurtosis] + 19 * [np.percentile]\n",
    "    # Dictionary arguments (nontrivial only for percentile function)\n",
    "    stats_kwargs = [{} for i in range(7)] + [{'q': i} for i in np.linspace(0.05, 0.95, 19)]\n",
    "\n",
    "    for stat, stat_kwargs in zip(stats, stats_kwargs):\n",
    "        stat_funs.append(_StatFunAdaptor(stat, **stat_kwargs))\n",
    "        stat_funs.append(_StatFunAdaptor(stat, np.diff, **stat_kwargs))  # Apply to 1-diff of row\n",
    "        stat_funs.append(_StatFunAdaptor(stat, diff2, **stat_kwargs))  # Apply to 2-diff of row\n",
    "        stat_funs.append(_StatFunAdaptor(stat, np.unique, **stat_kwargs))  # Apply to unique vals of row\n",
    "        stat_funs.append(_StatFunAdaptor(stat, np.unique, np.diff, **stat_kwargs))  # Apply to unique, 1-diff row vals\n",
    "        stat_funs.append(_StatFunAdaptor(stat, np.unique, diff2, **stat_kwargs))  # Apply to unique, 2-diff row vals\n",
    "    return stat_funs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for retrieving a Random Forest Classifier object\n",
    "def get_rfc():\n",
    "    return RandomForestClassifier(n_estimators=100,\n",
    "                                  max_features=0.5,\n",
    "                                  max_depth=None,\n",
    "                                  max_leaf_nodes=270,\n",
    "                                  min_impurity_decrease=0.0001,\n",
    "                                  random_state=123,\n",
    "                                  n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for setting up\n",
    "def get_input(debug=False):\n",
    "    '''\n",
    "    Function for loading either debug or full datasets\n",
    "    '''\n",
    "    if debug:\n",
    "        print 'Loading debug train and test datasets...'\n",
    "        train = load_h5py('../data/compressed/debug_train.h5')\n",
    "        test = load_h5py('../data/compressed/debug_test.h5')\n",
    "        id_test = load_pickle('../data/compressed/debug_test_id.pickle')\n",
    "    else:\n",
    "        print 'Loading original train and test datasets...'\n",
    "        train = load_h5py('../data/compressed/full_train.h5')\n",
    "        test = load_h5py('../data/compressed/full_test.h5')\n",
    "        id_test = load_pickle('../data/compressed/full_test_id.pickle')\n",
    "    # Isolate target variable\n",
    "    y_train_log = np.log1p(train[:, -1])\n",
    "    # Drop unnecessary columns\n",
    "    train = np.delete(train, -1, axis=1)\n",
    "    # Find shape of loaded datasets\n",
    "    print('Shape of training dataset: {} Rows, {} Columns'.format(*train.shape))\n",
    "    print('Shape of test dataset: {} Rows, {} Columns'.format(*test.shape))\n",
    "\n",
    "    return train, y_train_log, test, id_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading debug train and test datasets...\n",
      "Shape of training dataset: 100 Rows, 4991 Columns\n",
      "Shape of test dataset: 200 Rows, 4991 Columns\n",
      "Finding unique indexes...\n",
      "Filtering for only unique columns...\n",
      "Filtering for only unique columns...\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "X_train, y_train_log, X_test, id_test = get_input(debug)\n",
    "\n",
    "# Remove constant columns\n",
    "variance_checker = VarianceThreshold(threshold=0.0)\n",
    "xtrain = variance_checker.fit_transform(X_train)\n",
    "xtest = variance_checker.transform(X_test)\n",
    "\n",
    "# Remove duplicate columns\n",
    "unique_transformer = UniqueTransformer()\n",
    "unique_transformer.fit(X_train)\n",
    "xtrain = unique_transformer.transform(X_train)\n",
    "xtest = unique_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stat functions\n",
    "stat_functions = get_stat_funs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature union\n",
    "data_union = FeatureUnion([\n",
    "    ('pca', PCA(n_components=100)),\n",
    "    ('ct-2', ClassifierTransformer(get_rfc(), n_classes=2, cv=5)),\n",
    "    ('ct-3', ClassifierTransformer(get_rfc(), n_classes=3, cv=5)),\n",
    "    ('ct-4', ClassifierTransformer(get_rfc(), n_classes=4, cv=5)),\n",
    "    ('ct-5', ClassifierTransformer(get_rfc(), n_classes=5, cv=5)),\n",
    "    ('st', StatsTransformer(verbose=2))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data\n",
    "data_union.fit(X=xtrain, y=y_train_log)\n",
    "print '\\nCreating processed training set...\\n'\n",
    "train_data = data_union.transform(xtrain)\n",
    "print '\\nCreating processed test set...\\n'\n",
    "test_data = data_union.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "xdata = np.concatenate([train_data, test_data], axis=0)\n",
    "scaler = StandardScaler()\n",
    "xdata_scaled = scaler.fit_transform(X=xdata)\n",
    "train_scaled = xdata_scaled[:len(X_train), :]\n",
    "test_scaled = xdata_scaled[len(X_train):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load KLIEP importance weights\n",
    "weights_path = './covariate_shift/cs_weights_v1/'\n",
    "weights_temp = '0_width%s_numk%s.pickle'%(gw_val, num_kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost regressor parameters\n",
    "xgb_params = {'n_estimators': 1000,\n",
    "              'objective': 'reg:linear',\n",
    "              'booster': 'gbtree',\n",
    "              'learning_rate': 0.02,\n",
    "              'max_depth': 22,\n",
    "              'min_child_weight': 57,\n",
    "              'gamma' : 1.45,\n",
    "              'alpha': 0.0,\n",
    "              'lambda': 0.0,\n",
    "              'subsample': 0.67,\n",
    "              'colsample_bytree': 0.054,\n",
    "              'colsample_bylevel': 0.50,\n",
    "              'n_jobs': -1,\n",
    "              'random_state': 456}\n",
    "# Fitting XGB Regressor parameters\n",
    "fit_params = {'early_stopping_rounds': 15,\n",
    "              'eval_metric': 'rmse',\n",
    "              'verbose': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
