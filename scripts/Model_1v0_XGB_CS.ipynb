{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost with Covariate Shift Correction\n",
    "Extension of Model 0v2 that has covariate shift via Kullback-Leibler Importance Estimation Procedure integrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "import pickle as pkl\n",
    "import pdb\n",
    "import os\n",
    "import h5py\n",
    "import copy\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin  # For making custom classes\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-level parameters\n",
    "debug=True\n",
    "random_state=17\n",
    "# Cross-validation parameters\n",
    "cv_val = 10 \n",
    "cv_counter = 0  # Global variable\n",
    "# KLIEP weight parameters\n",
    "num_kernels = 1000\n",
    "gw_val = 75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading pickle file\n",
    "def load_pickle(fname):\n",
    "    with open(fname, 'rb') as handle:\n",
    "        return pkl.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniqueTransformer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Class with fit and transform methods for removing duplicate columns from a dataset\n",
    "    **fit** finds the indexes of unique columns using numpy unique\n",
    "    **transform** returns the dataset with the indexes of unique columns\n",
    "    '''\n",
    "    def __init__(self, axis=1):\n",
    "        self.axis=axis\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print 'Finding unique indexes...'\n",
    "        _, self.unique_indexes_ = np.unique(X, axis=self.axis, return_index=True)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print 'Filtering for only unique columns...'\n",
    "        return X[:, self.unique_indexes_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierTransformer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Class describing an object that transforms datasets via estimator results\n",
    "    **_get_labels** specifies target value bins and transforms target vector into bin values\n",
    "    '''\n",
    "    def __init__(self, estimator=None, n_classes=2, cv=3):\n",
    "        self.estimator=estimator\n",
    "        self.n_classes=n_classes\n",
    "        self.cv=cv\n",
    "\n",
    "    def _get_labels(self, y):\n",
    "        y_labels = np.zeros(len(y))\n",
    "        y_us = np.sort(np.unique(y))\n",
    "        step = int(len(y_us)/self.n_classes)\n",
    "        for i_class in range(self.n_classes):\n",
    "            if i_class+1 == self.n_classes:  # Edge case where i_class is initialized at 1\n",
    "                y_labels[y >= y_us[i_class*step]] = i_class\n",
    "            else:\n",
    "                y_labels[np.logical_and(y>=y_us[i_class*step], y<y_us[(i_class+1)*step])] = i_class\n",
    "        return y_labels\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print 'Fitting random forest classifier with n_classes = %s'%self.n_classes\n",
    "        y_labels = self._get_labels(y)\n",
    "        kf = KFold(n_splits=self.cv, shuffle=True, random_state=random_state)\n",
    "        self.estimators_ = []\n",
    "        # Train individual classifiers\n",
    "        for train, _ in kf.split(X, y_labels):\n",
    "            self.estimators_.append(clone(self.estimator).fit(X[train], y_labels[train]))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print 'Applying classifier transformation with n_classes = %s'%self.n_classes\n",
    "        kf = KFold(n_splits=self.cv, shuffle=False, random_state=random_state)\n",
    "\n",
    "        X_prob = np.zeros((X.shape[0], self.n_classes))\n",
    "        X_pred = np.zeros(X.shape[0])\n",
    "\n",
    "        for estimator, (_, test) in zip(self.estimators_, kf.split(X)):\n",
    "            X_prob[test] = estimator.predict_proba(X[test])\n",
    "            X_pred[test] = estimator.predict(X[test])\n",
    "        return np.hstack([X_prob, np.array([X_pred]).T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for transforming a row into statistical values\n",
    "def apply_stats_to_row(row):\n",
    "    stats = []\n",
    "    for fun in stat_functions:\n",
    "        stats.append(fun(row))\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatsTransformer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Class describing an object for transforming datasets into statistical values row-wise\n",
    "    NOTE: This class is dependent on the function **apply_stats_to_row**\n",
    "    '''\n",
    "    def __init__(self, verbose=0, n_jobs=-1, pre_dispatch='2*n_jobs'):\n",
    "        self.verbose = verbose\n",
    "        self.n_jobs = n_jobs\n",
    "        self.pre_dispatch = pre_dispatch\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print 'Applying statistical transformation to dataset...'\n",
    "        parallel = Parallel(n_jobs=self.n_jobs, pre_dispatch=self.pre_dispatch, verbose=self.verbose)\n",
    "        # Get statistics transformation\n",
    "        stats_list = parallel(delayed(apply_stats_to_row)(X[i_smpl, :]) for i_smpl in range(len(X)))\n",
    "        return np.array(stats_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _StatFunAdaptor:\n",
    "    '''\n",
    "    Class describing an object that wraps pre-processing functions with a main statistical function\n",
    "    **__init__** sets up the object parameters\n",
    "    **__call__** describes routine steps when object is called\n",
    "    '''\n",
    "    def __init__(self, stat_fun, *funs, **stat_fun_kwargs):\n",
    "        self.stat_fun = stat_fun\n",
    "        self.funs = funs\n",
    "        self.stat_fun_kwargs = stat_fun_kwargs\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x[x != 0]  # Only look at nonzero entries\n",
    "        # Transform row with cached functions\n",
    "        for fun in self.funs:\n",
    "            x = fun(x)\n",
    "        if x.size == 0:\n",
    "            return -99999  # Edge case default\n",
    "        return self.stat_fun(x, **self.stat_fun_kwargs)  # Returns result of a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff2(x):\n",
    "    return np.diff(x, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stat_funs():\n",
    "    '''\n",
    "    Function for defining all the statistical functions used for evaluating elements in a row-wise manner\n",
    "    Functions include: length, minimum, maximum, standard deviation, skew, kurtosis, and percentile\n",
    "    '''\n",
    "    stat_funs = []\n",
    "\n",
    "    stats = [len, np.min, np.max, np.median, np.std, skew, kurtosis] + 19 * [np.percentile]\n",
    "    # Dictionary arguments (nontrivial only for percentile function)\n",
    "    stats_kwargs = [{} for i in range(7)] + [{'q': i} for i in np.linspace(0.05, 0.95, 19)]\n",
    "\n",
    "    for stat, stat_kwargs in zip(stats, stats_kwargs):\n",
    "        stat_funs.append(_StatFunAdaptor(stat, **stat_kwargs))\n",
    "        stat_funs.append(_StatFunAdaptor(stat, np.diff, **stat_kwargs))  # Apply to 1-diff of row\n",
    "        stat_funs.append(_StatFunAdaptor(stat, diff2, **stat_kwargs))  # Apply to 2-diff of row\n",
    "        stat_funs.append(_StatFunAdaptor(stat, np.unique, **stat_kwargs))  # Apply to unique vals of row\n",
    "        stat_funs.append(_StatFunAdaptor(stat, np.unique, np.diff, **stat_kwargs))  # Apply to unique, 1-diff row vals\n",
    "        stat_funs.append(_StatFunAdaptor(stat, np.unique, diff2, **stat_kwargs))  # Apply to unique, 2-diff row vals\n",
    "    return stat_funs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for retrieving a Random Forest Classifier object\n",
    "def get_rfc():\n",
    "    return RandomForestClassifier(n_estimators=100,\n",
    "                                  max_features=0.5,\n",
    "                                  max_depth=None,\n",
    "                                  max_leaf_nodes=270,\n",
    "                                  min_impurity_decrease=0.0001,\n",
    "                                  random_state=123,\n",
    "                                  n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for setting up\n",
    "def get_input(debug=False):\n",
    "    if debug:\n",
    "        print 'Loading debug train and test datasets...'\n",
    "        train = pd.read_csv('../data/train_debug.csv')\n",
    "        test = pd.read_csv('../data/test_debug.csv')\n",
    "    else:\n",
    "        print 'Loading original train and test datasets...'\n",
    "        train = pd.read_csv('../data/train.csv')\n",
    "        test = pd.read_csv('../data/test.csv')\n",
    "    y_train_log = np.log1p(train['target'])\n",
    "    id_test = test['ID']\n",
    "    # Drop unnecessary columns\n",
    "    train.drop(labels=['ID', 'target'], axis=1, inplace=True)\n",
    "    test.drop(labels=['ID'], axis=1, inplace=True)\n",
    "    # Find shape of loaded datasets\n",
    "    print('Shape of training dataset: {} Rows, {} Columns'.format(*train.shape))\n",
    "    print('Shape of test dataset: {} Rows, {} Columns'.format(*test.shape))\n",
    "\n",
    "    return train.values, y_train_log.values, test.values, id_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBRegressorCV_KLIEP(BaseEstimator, RegressorMixin):\n",
    "    '''\n",
    "    Class that describes an object that implements XGB Regressor with cross-validation capability\n",
    "    **fit** defines indexes for cross-validation purposes and fits individual XGBRegressor models\n",
    "    \n",
    "    This XGB Regressor implements a custom loss and evaluation function\n",
    "    Custom loss: KLIEP-weighted root mean squared error\n",
    "    Evaluation: Root mean squared log error (RMSLE)\n",
    "    '''\n",
    "    def __init__(self, xgb_params=None, fit_params=None):\n",
    "        self.xgb_params = xgb_params\n",
    "        self.fit_params = fit_params\n",
    "\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        feature_importances = []\n",
    "        for estimator in self.estimators_:\n",
    "            feature_importances.append(estimator.feature_importances_)\n",
    "        return np.mean(feature_importances, axis=0)\n",
    "\n",
    "    @property\n",
    "    def evals_result_(self):\n",
    "        evals_result = []\n",
    "        for estimator in self.estimators_:\n",
    "            evals_result.append(estimator.evals_result_)\n",
    "        return np.array(evals_result)\n",
    "\n",
    "    @property\n",
    "    def best_scores_(self):\n",
    "        best_scores = []\n",
    "        for estimator in self.estimators_:\n",
    "            best_scores.append(estimator.best_score)\n",
    "        return np.array(best_scores)\n",
    "\n",
    "    @property\n",
    "    def best_score_(self):\n",
    "        return np.mean(self.best_scores_)\n",
    "\n",
    "    @property\n",
    "    def best_iterations_(self):\n",
    "        best_iterations = []\n",
    "        for estimator in self.estimators_:\n",
    "            best_iterations.append(estimator.best_iteration)\n",
    "        return np.array(best_iterations)\n",
    "\n",
    "    @property\n",
    "    def best_iteration_(self):\n",
    "        return np.round(np.mean(self.best_iterations_))\n",
    "\n",
    "    def fit(self, X, y, kf_list, **fit_params):\n",
    "        print 'Fitting XGB Regressors...'\n",
    "        self.estimators_ = []\n",
    "        \n",
    "        global cv_counter\n",
    "        for train, valid in kf_list:\n",
    "            self.estimators_.append(xgb.XGBRegressor(\n",
    "                                            **self.xgb_params\n",
    "                                                    ).fit(\n",
    "                                                        X[train], y[train],\n",
    "                                                        eval_set=[(X[valid], y[valid])],\n",
    "                                                        **self.fit_params))\n",
    "            cv_counter+=1\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        print 'Making aggregate XGB Regressor predictions...'\n",
    "        y_pred = []\n",
    "        for estimator in self.estimators_:\n",
    "            y_pred.append(estimator.predict(X))\n",
    "        return np.mean(y_pred, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "X_train, y_train_log, X_test, id_test = get_input(debug)\n",
    "\n",
    "# Remove constant columns\n",
    "variance_checker = VarianceThreshold(threshold=0.0)\n",
    "xtrain = variance_checker.fit_transform(X_train)\n",
    "xtest = variance_checker.transform(X_test)\n",
    "\n",
    "# Remove duplicate columns\n",
    "unique_transformer = UniqueTransformer()\n",
    "unique_transformer.fit(X_train)\n",
    "xtrain = unique_transformer.transform(X_train)\n",
    "xtest = unique_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stat functions\n",
    "stat_functions = get_stat_funs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature union\n",
    "data_union = FeatureUnion([\n",
    "    ('pca', PCA(n_components=100)),\n",
    "    ('ct-2', ClassifierTransformer(get_rfc(), n_classes=2, cv=5)),\n",
    "    ('ct-3', ClassifierTransformer(get_rfc(), n_classes=3, cv=5)),\n",
    "    ('ct-4', ClassifierTransformer(get_rfc(), n_classes=4, cv=5)),\n",
    "    ('ct-5', ClassifierTransformer(get_rfc(), n_classes=5, cv=5)),\n",
    "    ('st', StatsTransformer(verbose=2))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data\n",
    "data_union.fit(X=xtrain, y=y_train_log)\n",
    "print '\\nCreating processed training set...\\n'\n",
    "train_data = data_union.transform(xtrain)\n",
    "print '\\nCreating processed test set...\\n'\n",
    "test_data = data_union.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "xdata = np.concatenate([train_data, test_data], axis=0)\n",
    "scaler = StandardScaler()\n",
    "xdata_scaled = scaler.fit_transform(X=xdata)\n",
    "train_scaled = xdata_scaled[:len(X_train), :]\n",
    "test_scaled = xdata_scaled[len(X_train):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load KLIEP importance weights\n",
    "if debug:\n",
    "    cs_path = './covariate_shift/debug_cs_weights_v1/'\n",
    "else:\n",
    "    cs_path = './covariate_shift/full_cs_weights_v1/'\n",
    "cs_temp = '0_width%s_numk%s.pickle'%(gw_val, num_kernels)\n",
    "\n",
    "weight_path = cs_path + cs_temp\n",
    "if os.path.exists(weight_path):\n",
    "    kliep_set = load_pickle(weight_path)\n",
    "weights = np.array(kliep_set['weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom objective function for modified ordinary least squares\n",
    "def kliep_objective(y_true, y_pred):\n",
    "    # Get split indexes\n",
    "    split_list = copy.deepcopy(kf_list)\n",
    "    target_idx = split_list[cv_counter][0]\n",
    "    # Calculate 1st and 2nd derivatives\n",
    "    grad = np.multiply(weights[target_idx], np.subtract(y_pred, y_true))\n",
    "    hess = weights[target_idx]\n",
    "    return grad, hess\n",
    "\n",
    "# Custom evaluation function for RMSLE\n",
    "def rmsle_eval(y_predicted, y_true):\n",
    "    labels = y_true.get_label()\n",
    "    pred = np.log1p(y_predicted)\n",
    "    real = np.log1p(labels)\n",
    "    err = np.subtract(pred, real)\n",
    "    return 'rmsle', np.sqrt(np.mean(np.square(err)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost regressor parameters\n",
    "xgb_params = {'n_estimators': 1000,\n",
    "              'objective': kliep_objective,\n",
    "#               'objective': 'reg:linear',\n",
    "              'booster': 'gbtree',\n",
    "              'learning_rate': 0.02,\n",
    "              'max_depth': 22,\n",
    "              'min_child_weight': 57,\n",
    "              'gamma' : 1.45,\n",
    "              'alpha': 0.0,  # No regularization\n",
    "              'lambda': 0.0,  # No regularization\n",
    "              'subsample': 0.67,\n",
    "              'colsample_bytree': 0.054,\n",
    "              'colsample_bylevel': 0.50,\n",
    "              'n_jobs': -1,\n",
    "              'random_state': 456}\n",
    "# Fitting XGB Regressor parameters\n",
    "fit_params = {'early_stopping_rounds': 15,\n",
    "              'eval_metric': rmsle_eval,\n",
    "#               'eval_metric': 'rmse',\n",
    "              'verbose': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define KFold split\n",
    "kf_split = KFold(n_splits=cv_val, shuffle=False, random_state=random_state).split(train_scaled, y_train_log)\n",
    "kf_list = list(kf_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train xgboost regressor\n",
    "reg_kliep = XGBRegressorCV_KLIEP(xgb_params=xgb_params, fit_params=fit_params)\n",
    "reg_kliep.fit(X=train_scaled, y=y_train_log, kf_list=kf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred_log = reg_kliep.predict(X=test_scaled)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "# Format submission\n",
    "submission_path = '../submissions/xgb_kliep_1v0_submit.csv'\n",
    "submission = pd.DataFrame()\n",
    "submission['ID'] = id_test\n",
    "submission['target'] = y_pred\n",
    "# Save submissions\n",
    "submission.to_csv(submission_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
