{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost with Covariate Shift Correction\n",
    "Extension of Model 0v2 that has covariate shift via Kullback-Leibler Importance Estimation Procedure integrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "import pickle\n",
    "import pdb\n",
    "import os\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin  # For making custom classes\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-level parameters\n",
    "debug=True\n",
    "random_state=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniqueTransformer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Class with fit and transform methods for removing duplicate columns from a dataset\n",
    "    **fit** finds the indexes of unique columns using numpy unique\n",
    "    **transform** returns the dataset with the indexes of unique columns\n",
    "    '''\n",
    "    def __init__(self, axis=1):\n",
    "        self.axis=axis\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print 'Finding unique indexes...'\n",
    "        _, self.unique_indexes_ = np.unique(X, axis=self.axis, return_index=True)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print 'Filtering for only unique columns...'\n",
    "        return X[:, self.unique_indexes_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierTransformer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Class describing an object that transforms datasets via estimator results\n",
    "    **_get_labels** specifies target value bins and transforms target vector into bin values\n",
    "    '''\n",
    "    def __init__(self, estimator=None, n_classes=2, cv=3):\n",
    "        self.estimator=estimator\n",
    "        self.n_classes=n_classes\n",
    "        self.cv=cv\n",
    "\n",
    "    def _get_labels(self, y):\n",
    "        y_labels = np.zeros(len(y))\n",
    "        y_us = np.sort(np.unique(y))\n",
    "        step = int(len(y_us)/self.n_classes)\n",
    "\n",
    "        for i_class in range(self.n_classes):\n",
    "            if i_class+1 == self.n_classes:  # Edge case where i_class is initialized at 1\n",
    "                y_labels[y >= y_us[i_class*step]] = i_class\n",
    "            else:\n",
    "                y_labels[np.logical_and(y>=y_us[i_class*step], y<y_us[(i_class+1)*step])] = i_class\n",
    "        return y_labels\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print 'Fitting random forest classifier with n_classes = %s'%self.n_classes\n",
    "        y_labels = self._get_labels(y)\n",
    "        kf = KFold(n_splits=self.cv, shuffle=False, random_state=random_state)\n",
    "        self.estimators_ = []\n",
    "        # Train individual classifiers\n",
    "        for train, _ in kf.split(X, y_labels):\n",
    "            self.estimators_.append(clone(self.estimator).fit(X[train], y_labels[train]))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print 'Applying classifier transformation with n_classes = %s'%self.n_classes\n",
    "        kf = KFold(n_splits=self.cv, shuffle=False, random_state=random_state)\n",
    "\n",
    "        X_prob = np.zeros((X.shape[0], self.n_classes))\n",
    "        X_pred = np.zeros(X.shape[0])\n",
    "\n",
    "        for estimator, (_, test) in zip(self.estimators_, kf.split(X)):\n",
    "            X_prob[test] = estimator.predict_proba(X[test])\n",
    "            X_pred[test] = estimator.predict(X[test])\n",
    "        return np.hstack([X_prob, np.array([X_pred]).T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for transforming a row into statistical values\n",
    "def apply_stats_to_row(row):\n",
    "    stats = []\n",
    "    for fun in stat_functions:\n",
    "        stats.append(fun(row))\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatsTransformer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Class describing an object for transforming datasets into statistical values row-wise\n",
    "    NOTE: This class is dependent on the function **apply_stats_to_row**\n",
    "    '''\n",
    "    def __init__(self, verbose=0, n_jobs=-1, pre_dispatch='2*n_jobs'):\n",
    "        self.verbose = verbose\n",
    "        self.n_jobs = n_jobs\n",
    "        self.pre_dispatch = pre_dispatch\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print 'Applying statistical transformation to dataset...'\n",
    "        parallel = Parallel(n_jobs=self.n_jobs, pre_dispatch=self.pre_dispatch, verbose=self.verbose)\n",
    "        # Get statistics transformation\n",
    "        stats_list = parallel(delayed(apply_stats_to_row)(X[i_smpl, :]) for i_smpl in range(len(X)))\n",
    "        return np.array(stats_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _StatFunAdaptor:\n",
    "    '''\n",
    "    Class describing an object that wraps pre-processing functions with a main statistical function\n",
    "    **__init__** sets up the object parameters\n",
    "    **__call__** describes routine steps when object is called\n",
    "    '''\n",
    "    def __init__(self, stat_fun, *funs, **stat_fun_kwargs):\n",
    "        self.stat_fun = stat_fun\n",
    "        self.funs = funs\n",
    "        self.stat_fun_kwargs = stat_fun_kwargs\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x[x != 0]  # Only look at nonzero entries\n",
    "        # Transform row with cached functions\n",
    "        for fun in self.funs:\n",
    "            x = fun(x)\n",
    "        if x.size == 0:\n",
    "            return -99999  # Edge case default\n",
    "        return self.stat_fun(x, **self.stat_fun_kwargs)  # Returns result of a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff2(x):\n",
    "    return np.diff(x, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stat_funs():\n",
    "    '''\n",
    "    Function for defining all the statistical functions used for evaluating elements in a row-wise manner\n",
    "    Functions include: length, minimum, maximum, standard deviation, skew, kurtosis, and percentile\n",
    "    '''\n",
    "    stat_funs = []\n",
    "\n",
    "    stats = [len, np.min, np.max, np.median, np.std, skew, kurtosis] + 19 * [np.percentile]\n",
    "    # Dictionary arguments (nontrivial only for percentile function)\n",
    "    stats_kwargs = [{} for i in range(7)] + [{'q': i} for i in np.linspace(0.05, 0.95, 19)]\n",
    "\n",
    "    for stat, stat_kwargs in zip(stats, stats_kwargs):\n",
    "        stat_funs.append(_StatFunAdaptor(stat, **stat_kwargs))\n",
    "        stat_funs.append(_StatFunAdaptor(stat, np.diff, **stat_kwargs))  # Apply to 1-diff of row\n",
    "        stat_funs.append(_StatFunAdaptor(stat, diff2, **stat_kwargs))  # Apply to 2-diff of row\n",
    "        stat_funs.append(_StatFunAdaptor(stat, np.unique, **stat_kwargs))  # Apply to unique vals of row\n",
    "        stat_funs.append(_StatFunAdaptor(stat, np.unique, np.diff, **stat_kwargs))  # Apply to unique, 1-diff row vals\n",
    "        stat_funs.append(_StatFunAdaptor(stat, np.unique, diff2, **stat_kwargs))  # Apply to unique, 2-diff row vals\n",
    "    return stat_funs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for retrieving a Random Forest Classifier object\n",
    "def get_rfc():\n",
    "    return RandomForestClassifier(n_estimators=100,\n",
    "                                  max_features=0.5,\n",
    "                                  max_depth=None,\n",
    "                                  max_leaf_nodes=270,\n",
    "                                  min_impurity_decrease=0.0001,\n",
    "                                  random_state=123,\n",
    "                                  n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for setting up datasets\n",
    "def get_input(debug=False):\n",
    "    if debug:\n",
    "        print 'Loading debug train and test datasets...'\n",
    "        train = pd.read_csv('../data/train_debug.csv')\n",
    "        test = pd.read_csv('../data/test_debug.csv')\n",
    "    else:\n",
    "        print 'Loading original train and test datasets...'\n",
    "        train = pd.read_csv('../data/train.csv')\n",
    "        test = pd.read_csv('../data/test.csv')\n",
    "    y_train_log = np.log1p(train['target'])\n",
    "    id_test = test['ID']\n",
    "    # Drop unnecessary columns\n",
    "    train.drop(labels=['ID', 'target'], axis=1, inplace=True)\n",
    "    test.drop(labels=['ID'], axis=1, inplace=True)\n",
    "    # Find shape of loaded datasets\n",
    "    print('Shape of training dataset: {} Rows, {} Columns'.format(*train.shape))\n",
    "    print('Shape of test dataset: {} Rows, {} Columns'.format(*test.shape))\n",
    "\n",
    "    return train.values, y_train_log.values, test.values, id_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for retrieving width list\n",
    "def get_width(debug=False):\n",
    "    '''\n",
    "    Function for loading either debug or full width lists\n",
    "    '''\n",
    "    if debug:\n",
    "        return [10, 1000]\n",
    "    else:\n",
    "        return [0.1, 1, 10, 100, 1000, 10000, 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating Gaussian kernel value\n",
    "def calc_gaussian(x, center, width):\n",
    "    return np.exp(-(np.square(np.linalg.norm(np.subtract(x, center))))/(2*np.square(width)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating importance weights\n",
    "def get_importances(data, alpha, kc, kw):\n",
    "    importance_weights = np.zeros(len(data))\n",
    "    for i, row in enumerate(data):\n",
    "        kernel_sum = 0\n",
    "        for j, center in enumerate(kc):\n",
    "            kernel_sum += alpha[j]*calc_gaussian(row, center, kw)\n",
    "        importance_weights[i] = kernel_sum\n",
    "    return importance_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kullback-Leibler Importance Estimation Procedure training function\n",
    "def train_KLIEP(train, test, num_kernels=100, kernel_width=10, lr=0.001, a_val=1, stop=0.00001):\n",
    "    '''\n",
    "    Function for getting KLIEP weights for a given training and test set\n",
    "    '''\n",
    "    # Instantiate kernel centers\n",
    "    kernel_idx_bag = np.random.permutation(len(test))\n",
    "    kernel_idx = np.array([np.random.choice(kernel_idx_bag) for i in range(num_kernels)])\n",
    "    kernel_centers = test[kernel_idx, :]\n",
    "    # Compute A matrix\n",
    "    A = np.zeros(shape=(len(test), len(kernel_centers)))\n",
    "    for i, row in enumerate(test):\n",
    "        for j, center in enumerate(kernel_centers):\n",
    "            A[i, j] = calc_gaussian(row, center, kernel_width)\n",
    "    # Compute b vector\n",
    "    b = np.zeros(num_kernels)\n",
    "    for j, center in enumerate(kernel_centers):\n",
    "        temp_sum = 0\n",
    "        for row in train:\n",
    "            temp_sum += calc_gaussian(row, center, kernel_width)\n",
    "        b[j] = temp_sum/np.float16(len(train))\n",
    "    # Initialize alpha vector\n",
    "    alpha = a_val * np.ones(shape=num_kernels)\n",
    "    # Begin training\n",
    "    alpha_old = np.zeros(shape=num_kernels)\n",
    "    counter = 0\n",
    "    while True:\n",
    "        alpha = np.add(alpha, lr*np.matmul(A.T, np.divide(np.ones(len(test)), np.matmul(A, alpha))))\n",
    "        alpha = np.add(alpha, np.divide(np.multiply((1-np.dot(b, alpha)), b), np.dot(b, b)))\n",
    "        alpha = np.maximum(np.zeros(num_kernels), alpha)\n",
    "        alpha = np.divide(alpha, np.dot(b, alpha))\n",
    "        # Check convergence by average deviation\n",
    "        deviation = np.linalg.norm(np.subtract(alpha, alpha_old))\n",
    "        if deviation < stop*np.linalg.norm(alpha_old):\n",
    "            print 'Converged in %s iterations!'%counter\n",
    "            importance_weights = get_importances(data=test, alpha=alpha, kc=kernel_centers, kw=kernel_width)\n",
    "            return importance_weights, alpha, kernel_centers\n",
    "            break\n",
    "        else:\n",
    "            counter += 1\n",
    "            alpha_old = alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting the best model\n",
    "def get_best_KLIEP(train, test, width_list, n_splits=10, num_kernels=100, lr=0.001, a_val=1, stop=0.00001):\n",
    "    '''\n",
    "    Function for tuning KLIEP kernel performance\n",
    "    '''\n",
    "    # Split test set into disjoint subsets\n",
    "    split_sets = []\n",
    "    kf = KFold(n_splits=n_splits, shuffle=False, random_state=random_state)\n",
    "    print 'Splitting test set into %s disjoint subsets...'%n_splits\n",
    "    for _, test_idx in kf.split(test):\n",
    "        split_sets.append(test[test_idx, :])\n",
    "    # Evaluate each model\n",
    "    j_models = []\n",
    "    alpha_list = []\n",
    "    centers_list = []\n",
    "    for idx, w in enumerate(width_list):\n",
    "        print 'Working on split set %s'%idx\n",
    "        print 'Evaluating KLIEP model with Gaussian kernel width of %s...'%w\n",
    "        j_avglist = []\n",
    "        for s in split_sets:\n",
    "            importance, alpha, center = train_KLIEP(train=train, test=s, num_kernels=num_kernels, kernel_width=w, \n",
    "                                     lr=lr, a_val=a_val, stop=stop)\n",
    "            j_avglist.append(np.mean(np.log(importance)))\n",
    "        j_models.append(np.mean(j_avglist))\n",
    "        alpha_list.append(alpha)\n",
    "        centers_list.append(center)\n",
    "    # Use best model to evaluate train set KLIEP importances\n",
    "    best_idx = np.argmax(np.array(j_models))\n",
    "    print '\\nBest width was: %s'%width_list[best_idx]\n",
    "    importance_weights = get_importances(data=train, alpha=alpha_list[best_idx], kc=centers_list[best_idx], \n",
    "                                         kw=width_list[best_idx])\n",
    "    return width_list[best_idx], importance_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading debug train and test datasets...\n",
      "Shape of training dataset: 100 Rows, 4992 Columns\n",
      "Shape of test dataset: 200 Rows, 4992 Columns\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "X_train, y_train_log, X_test, id_test = get_input(debug)\n",
    "# Load width list\n",
    "wlist = get_width(debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding unique indexes...\n",
      "Filtering for only unique columns...\n",
      "Filtering for only unique columns...\n"
     ]
    }
   ],
   "source": [
    "# Remove constant columns\n",
    "variance_checker = VarianceThreshold(threshold=0.0)\n",
    "xtrain = variance_checker.fit_transform(X_train)\n",
    "xtest = variance_checker.transform(X_test)\n",
    "\n",
    "# Remove duplicate columns\n",
    "unique_transformer = UniqueTransformer()\n",
    "unique_transformer.fit(X_train)\n",
    "xtrain = unique_transformer.transform(X_train)\n",
    "xtest = unique_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stat functions\n",
    "stat_functions = get_stat_funs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature union\n",
    "data_union = FeatureUnion([\n",
    "    ('pca', PCA(n_components=100)),\n",
    "    ('ct-2', ClassifierTransformer(get_rfc(), n_classes=2, cv=5)),\n",
    "    ('ct-3', ClassifierTransformer(get_rfc(), n_classes=3, cv=5)),\n",
    "    ('ct-4', ClassifierTransformer(get_rfc(), n_classes=4, cv=5)),\n",
    "    ('ct-5', ClassifierTransformer(get_rfc(), n_classes=5, cv=5)),\n",
    "    ('st', StatsTransformer(verbose=2))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting random forest classifier with n_classes = 2\n",
      "Fitting random forest classifier with n_classes = 3\n",
      "Fitting random forest classifier with n_classes = 4\n",
      "Fitting random forest classifier with n_classes = 5\n",
      "Creating processed training set...\n",
      "Applying classifier transformation with n_classes = 2\n",
      "Applying classifier transformation with n_classes = 3\n",
      "Applying classifier transformation with n_classes = 4\n",
      "Applying classifier transformation with n_classes = 5\n",
      "Applying statistical transformation to dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating processed test set...\n",
      "Applying classifier transformation with n_classes = 2\n",
      "Applying classifier transformation with n_classes = 3\n",
      "Applying classifier transformation with n_classes = 4\n",
      "Applying classifier transformation with n_classes = 5\n",
      "Applying statistical transformation to dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    0.8s finished\n"
     ]
    }
   ],
   "source": [
    "data_union.fit(X=xtrain, y=y_train_log)\n",
    "print 'Creating processed training set...'\n",
    "train_data = data_union.transform(xtrain)\n",
    "print 'Creating processed test set...'\n",
    "test_data = data_union.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "xdata = np.concatenate([train_data, test_data], axis=0)\n",
    "scaler = StandardScaler()\n",
    "xdata_scaled = scaler.fit_transform(X=xdata)\n",
    "train_scaled = xdata_scaled[:len(X_train), :]\n",
    "test_scaled = xdata_scaled[len(X_train):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get KLIEP importance weights\n",
    "# weights_path = './covariate_shift/kliep_weights.pickle'\n",
    "# if os.path.exists(weights_path):\n",
    "#     print 'Loading existing KLIEP importances...'\n",
    "#     with open(weights_path, 'rb') as handle:\n",
    "#         best_width, importances = pickle.load(handle)\n",
    "# else:\n",
    "#     print 'Generating new KLIEP importances...'\n",
    "#     best_width, importances = get_best_KLIEP(train_data, test_data, wlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost Regressor\n",
    "# train_KLIEP(train_scaled, test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
