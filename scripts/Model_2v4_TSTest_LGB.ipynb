{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2v4: LightGBM with Test-Only Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pdb\n",
    "import os\n",
    "import gc; gc.enable()\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Function for loading h5py file\n",
    "def load_h5py(fname):\n",
    "    with h5py.File(fname, 'r') as handle:\n",
    "        return handle['data'][:]\n",
    "# Function for loading pickle file\n",
    "def load_pickle(fname):\n",
    "    with open(fname, 'rb') as handle:\n",
    "        return pickle.load(handle)\n",
    "\n",
    "\n",
    "# Function for setting up\n",
    "def get_input(debug=False):\n",
    "    '''\n",
    "    Function for loading either debug or full datasets\n",
    "    '''\n",
    "    os.chdir('../data/compressed/')\n",
    "    print os.getcwd()\n",
    "    pkl_files = ['train_id.pickle', 'trainidx.pickle', 'target.pickle', 'test_id.pickle', 'testidx.pickle']\n",
    "    if debug:\n",
    "        print 'Loading debug train and test datasets...'\n",
    "        # h5py files\n",
    "        train = load_h5py('debug_train.h5')\n",
    "        test = load_h5py('debug_test.h5')\n",
    "        # pickle files\n",
    "        id_train, train_idx, target, id_test, test_idx = [load_pickle('debug_%s'%f) for f in pkl_files]\n",
    "    else:\n",
    "        print 'Loading original train and test datasets...'\n",
    "        # h5py files\n",
    "        train = load_h5py('full_train.h5')\n",
    "        test = load_h5py('full_test.h5')\n",
    "        # pickle files\n",
    "        id_train, train_idx, target, id_test, test_idx = [load_pickle('full_%s'%f) for f in pkl_files]\n",
    "    # Load feature names\n",
    "    fnames = load_pickle('feature_names.pickle')\n",
    "    # Find shape of loaded datasets\n",
    "    print('Shape of training dataset: {} Rows, {} Columns'.format(*train.shape))\n",
    "    print('Shape of test dataset: {} Rows, {} Columns'.format(*test.shape))\n",
    "    os.chdir('../../scripts/')\n",
    "    print os.getcwd()\n",
    "    return fnames, train, id_train, train_idx, target, test, id_test, test_idx\n",
    "\n",
    "\n",
    "# Function for getting datasets in dataframe format\n",
    "def get_dataframes(debug=False):\n",
    "    # Load data\n",
    "    fnames, train, id_train, train_idx, target, test, id_test, test_idx = get_input(debug)\n",
    "    # Format data\n",
    "    train_df = pd.DataFrame(data=train, index=train_idx, columns=fnames)\n",
    "    train_df['ID'] = id_train\n",
    "    train_df['target'] = target\n",
    "    test_df = pd.DataFrame(data=test, index=test_idx, columns=fnames)\n",
    "    test_df['ID'] = id_test\n",
    "    \n",
    "    print('\\nShape of training dataframe: {} Rows, {} Columns'.format(*train_df.shape))\n",
    "    print('Shape of test dataframe: {} Rows, {} Columns'.format(*test_df.shape))\n",
    "    return fnames, train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading leaks\n",
    "def load_leaks(leak_val):\n",
    "    leak_dir = './time_series/stats/'\n",
    "    \n",
    "    train_leak_loc = leak_dir + 'train_leak_%s.csv'%leak_val\n",
    "    train_leak = pd.read_csv(train_leak_loc).compiled_leak\n",
    "    test_leak_loc = leak_dir + 'test_leak_%s.csv'%leak_val\n",
    "    test_leak = pd.read_csv(test_leak_loc).compiled_leak\n",
    "    \n",
    "    return train_leak, test_leak\n",
    "\n",
    "\n",
    "# Function for applying statistical transformations to data\n",
    "def calculate_metadata(df):\n",
    "    '''\n",
    "    Function for calculating metadata across pandas dataframe row\n",
    "    '''\n",
    "    meta = pd.DataFrame()\n",
    "    # Calculations that disregard zeros\n",
    "    meta['nz_mean'] = df.apply(lambda x: x[x!=0].mean(), axis=1)\n",
    "    meta['nz_log_mean_exp'] = df.apply(lambda x: np.expm1(np.mean(np.log1p(x[x!=0]))), axis=1)\n",
    "    meta['nz_median'] = df.apply(lambda x: x[x!=0].median(), axis=1)\n",
    "    meta['nz_std'] = df.apply(lambda x: x[x!=0].std(), axis=1)\n",
    "    meta['nz_kurtosis'] = df.apply(lambda x: x[x!=0].kurtosis(), axis=1)\n",
    "    meta['nz_min'] = df.apply(lambda x: np.min(x[x!=0]), axis=1)\n",
    "    \n",
    "    # Calculations independent of zeros\n",
    "    meta['sum'] = df.apply(lambda x: np.sum(x), axis=1)\n",
    "    meta['max'] = df.apply(lambda x: np.max(x), axis=1)\n",
    "    \n",
    "    # Calculations factoring in zeros\n",
    "    meta['zero_count'] = df.apply(lambda x: np.count_nonzero(x==0), axis=1)\n",
    "    meta['mean'] = df.apply(lambda x: x.mean(), axis=1)\n",
    "    meta['log_mean_exp'] = df.apply(lambda x: np.expm1(np.mean(np.log1p(x))), axis=1)\n",
    "    meta['median'] = df.apply(lambda x: x.median(), axis=1)\n",
    "    meta['std'] = df.apply(lambda x: x.std(), axis=1)\n",
    "    meta['kurtosis'] = df.apply(lambda x: x.kurtosis(), axis=1)\n",
    "    \n",
    "    return meta\n",
    "\n",
    "\n",
    "# Function for feature engineering\n",
    "def format_for_training(train, test, f, trn_res, tst_res, lagval=38):\n",
    "    '''\n",
    "    - Formats train and test dataframes for training\n",
    "    '''\n",
    "    tmp_trn = train.copy(deep=True)\n",
    "    tmp_trn['log_leak'] = np.log1p(trn_res['target'].values)\n",
    "    \n",
    "    tmp_tst = test.copy(deep=True)\n",
    "    tmp_tst['log_leak'] = np.log1p(tst_res['target'].values)\n",
    "    \n",
    "    score_name = './model_data/model_2v4_featscores_test_%s.csv'%lagval\n",
    "    print 'Loading file:\\n', score_name\n",
    "    score_df = pd.read_csv(score_name, index_col=0)\n",
    "    \n",
    "    # Select good features\n",
    "    threshold = 0.017233  # lag 38\n",
    "    good_features = score_df.loc[score_df['rmse']<=threshold].index\n",
    "    \n",
    "    print '\\nLoading metadata for training set...'\n",
    "    trn_meta = pd.read_csv('./model_data/train_meta.csv')\n",
    "    print 'Loading metadata for test set...'\n",
    "    tst_meta = pd.read_csv('./model_data/test_meta.csv')\n",
    "    \n",
    "    # Format training and test datasets\n",
    "    cols = ['ID'] + list(good_features) + list(trn_meta.columns.values) + ['log_leak']\n",
    "    tmp_trn = pd.concat([tmp_trn, trn_meta], axis=1)\n",
    "    tmp_tst = pd.concat([tmp_tst, tst_meta], axis=1)\n",
    "    \n",
    "    return tmp_trn[cols], tmp_tst[cols]\n",
    "\n",
    "\n",
    "# Function for scaling datasets \n",
    "def scale_for_training(train, test, real):\n",
    "    print 'Scaling data...'\n",
    "    tmp_trn = train.copy(deep=True)\n",
    "    tmp_trn.replace(np.nan, 0, inplace=True)\n",
    "    tmp_tst = test.copy(deep=True)\n",
    "    tmp_tst.replace(np.nan, 0, inplace=True)\n",
    "    tmp_rel = real.copy(deep=True)\n",
    "    tmp_rel.replace(np.nan, 0, inplace=True)\n",
    "    \n",
    "    tmp_trn.drop(labels=['ID'], axis=1, inplace=True)\n",
    "    tmp_tst.drop(labels=['ID'], axis=1, inplace=True)\n",
    "    tmp_rel.drop(labels=['ID'], axis=1, inplace=True)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaled_train = scaler.fit_transform(tmp_trn)\n",
    "    scaled_test = scaler.transform(tmp_tst)\n",
    "    scaled_real = scaler.transform(tmp_rel)\n",
    "    \n",
    "    del tmp_trn, tmp_tst, tmp_rel; gc.collect();\n",
    "    return scaled_train, scaled_test, scaled_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Script\n",
    "try:\n",
    "    del fnames, train, test\n",
    "    print 'Clearing loaded dataframes from memory...\\n'\n",
    "except:\n",
    "    pass\n",
    "fnames, train, test = get_dataframes(debug=False)\n",
    "\n",
    "# Load leaks\n",
    "leak_val = 38\n",
    "print '\\nLoading train and test leaks...'\n",
    "train_leak, test_leak = load_leaks(leak_val)\n",
    "print 'Nonzero elements in train:', np.count_nonzero(train_leak)\n",
    "print 'Nonzero elements in test:', np.count_nonzero(test_leak)\n",
    "\n",
    "# Load good result\n",
    "print '\\nLoading good results for train and test predictions...'\n",
    "\n",
    "tst_res_name = './model_data/tstest_lgb_2v2_lag38_good_test.csv'\n",
    "trn_res_name = './model_data/tstest_lgb_2v2_lag38_good_train.csv'\n",
    "\n",
    "tst_res = pd.read_csv(tst_res_name)\n",
    "trn_res = pd.read_csv(trn_res_name)\n",
    "print 'Shape of test results import:', tst_res.shape\n",
    "print 'Shape of train results import:', trn_res.shape\n",
    "\n",
    "# Find hit and miss indexes\n",
    "trn_leak_idx = np.where(train_leak!=0)[0]\n",
    "trn_miss_idx = np.where(train_leak==0)[0]\n",
    "\n",
    "tst_leak_idx = np.where(test_leak!=0)[0]\n",
    "tst_miss_idx = np.where(test_leak==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and test in format for training booster\n",
    "\n",
    "# Real train target\n",
    "y = train['target'].values\n",
    "train_target = np.log1p(y)\n",
    "\n",
    "# Define target in terms of test set leaks\n",
    "target = test_leak[tst_leak_idx].values\n",
    "target_log = np.log1p(target)\n",
    "\n",
    "pp_flag = False\n",
    "train_name = './model_data/btrain_%s_test.csv'%leak_val\n",
    "test_name = './model_data/btest_%s_test.csv'%leak_val\n",
    "if pp_flag:\n",
    "    btrain, btest = format_for_training(train, test, fnames, trn_res, tst_res, leak_val)\n",
    "    print '\\nSaving generated datasets...'\n",
    "    btrain.to_csv(train_name, index=False)\n",
    "    btest.to_csv(test_name, index=False)\n",
    "else:\n",
    "    print '\\nLoading generated datasets...'\n",
    "    btrain = pd.read_csv(train_name)\n",
    "    btest = pd.read_csv(test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for boosting\n",
    "tst_training = btest.loc[tst_leak_idx]\n",
    "tst_testing = btest.loc[tst_miss_idx]\n",
    "\n",
    "boost_train, boost_test, real_train = scale_for_training(tst_training, tst_testing, btrain)\n",
    "\n",
    "print 'New shape of boost train:', boost_train.shape\n",
    "print 'New shape of boost test:', boost_test.shape\n",
    "print 'New shape of real train:', real_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom evaluation metric for LGBM\n",
    "def RMSLE(preds, train_data):\n",
    "    return 'RMSLE', np.sqrt(mean_squared_error(train_data.get_label(), preds)), False\n",
    "\n",
    "\n",
    "# Function for evaluating mean errors\n",
    "def average_best_scores(scores):\n",
    "    s_cols = [c for c in scores.columns if c!='index']\n",
    "    errs = scores[s_cols].apply(lambda x: np.min(x[x!=np.nan]), axis=0)\n",
    "    return np.mean(errs)\n",
    "\n",
    "\n",
    "# Function for plotting RMSLE averaged over all iterations\n",
    "def plot_scores(t_scores, v_scores):\n",
    "    s_cols = [c for c in t_scores.columns if c!='index']\n",
    "    t_mean = np.mean(t_scores[s_cols], axis=1)\n",
    "    t_mean = t_mean[~np.isnan(t_mean)]\n",
    "    v_mean = np.mean(v_scores[s_cols], axis=1)\n",
    "    v_mean = v_mean[~np.isnan(v_mean)]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(v_mean, label='validation', c='orange', alpha=0.7)\n",
    "    plt.plot(t_mean, label='training', c='orange', alpha=0.7)\n",
    "    plt.title('Averaged Training and Validation Error')\n",
    "    plt.xlabel('Training Iteration')\n",
    "    plt.ylabel('RMSLE')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "\n",
    "# Function for training a LGB Regressor\n",
    "def train_lgb_regressor(train, target, test, real, params, n_boost=500):\n",
    "    num_boosting = n_boost\n",
    "    valid_result = pd.DataFrame(data=np.arange(num_boosting), columns=['index'])\n",
    "    train_result = pd.DataFrame(data=np.arange(num_boosting), columns=['index'])\n",
    "    test_predictions = np.zeros(test.shape[0])\n",
    "    train_predictions = np.zeros(train.shape[0])\n",
    "    real_predictions = np.zeros(real.shape[0])\n",
    "    kfold = KFold(n_splits=4)\n",
    "    \n",
    "    for i, (trn, val) in enumerate(kfold.split(train)):\n",
    "        print '\\nTraining on fold:', i\n",
    "        round_name = 'round_%s'%i\n",
    "        xtrain = train[trn, :]\n",
    "        ytrain = target[trn]\n",
    "\n",
    "        xval = train[val, :]\n",
    "        yval = target[val]\n",
    "\n",
    "        lgb_train = lgb.Dataset(xtrain, ytrain)\n",
    "        lgb_eval = lgb.Dataset(xval, yval, reference=lgb_train)\n",
    "\n",
    "        evals_result = {}\n",
    "        gbm = lgb.train(params,\n",
    "                        lgb_train,\n",
    "                        num_boost_round=num_boosting,\n",
    "                        valid_sets= (lgb_train, lgb_eval),\n",
    "                        verbose_eval=500,\n",
    "                        feval=RMSLE, \n",
    "                        evals_result=evals_result,\n",
    "                        early_stopping_rounds=50)\n",
    "\n",
    "        # Get evaluation results\n",
    "        valid_res_df = pd.DataFrame.from_dict(evals_result['valid_1'], orient='columns')\n",
    "        valid_res_df.rename(columns={'RMSLE': round_name}, inplace=True)\n",
    "        valid_res_df.reset_index(inplace=True)\n",
    "        train_res_df = pd.DataFrame.from_dict(evals_result['training'], orient='columns')\n",
    "        train_res_df.rename(columns={'RMSLE': round_name}, inplace=True)\n",
    "        train_res_df.reset_index(inplace=True)\n",
    "\n",
    "        valid_result = pd.merge(valid_result, valid_res_df, how='outer', on='index')\n",
    "        train_result = pd.merge(train_result, train_res_df, how='outer', on='index')\n",
    "\n",
    "        # Make predictions\n",
    "        test_predictions += (gbm.predict(test, num_iteration=gbm.best_iteration))/kfold.n_splits\n",
    "        train_predictions += (gbm.predict(train, num_iteration=gbm.best_iteration))/kfold.n_splits\n",
    "        real_predictions += (gbm.predict(real, num_iteration=gbm.best_iteration))/kfold.n_splits\n",
    "    \n",
    "    return train_result, valid_result, test_predictions, train_predictions, real_predictions\n",
    "\n",
    "\n",
    "# Function for calculating real scores for training set\n",
    "def get_real_scores(trn_preds, train_target, trn_miss_idx):\n",
    "    '''\n",
    "    Splits predictions into training leak and miss\n",
    "    Gets scores for both sets\n",
    "    '''\n",
    "    train_miss_preds = trn_preds[trn_miss_idx]\n",
    "    train_target_miss = train_target[trn_miss_idx]\n",
    "    \n",
    "    # Calculate scores\n",
    "    full_score = np.sqrt(mean_squared_error(trn_preds, train_target))\n",
    "    miss_score = np.sqrt(mean_squared_error(train_miss_preds, train_target_miss))\n",
    "    \n",
    "    return full_score, miss_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGB Regressor Parameters\n",
    "params = {'task': 'train',\n",
    "          'boosting': 'gbdt',\n",
    "          'objective': 'regression',\n",
    "          'metric': 'RMSLE',\n",
    "          'num_leaves': 7,\n",
    "          'learning_rate': 0.08,\n",
    "          'max_depth': -1,\n",
    "          'feature_fraction': 0.9,\n",
    "          'bagging_fraction': 0.73,\n",
    "          'bagging_freq': 5,\n",
    "          'min_sum_hessian_in_leaf': 1e-3,\n",
    "          'lambda_l2': 30,\n",
    "          'verbose': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tune LGB Regressor Hyper-Parameters\n",
    "tune_flag = False\n",
    "if tune_flag:\n",
    "    cv_results = {}\n",
    "\n",
    "#     cv_entry = 'num_leaves'\n",
    "#     cv_set = [5, 8, 11]\n",
    "\n",
    "#     cv_entry = 'learning_rate'\n",
    "#     cv_set = [0.6, 0.8, 0.9]\n",
    "\n",
    "#     cv_entry = 'max_depth'\n",
    "#     cv_set = [-1, 6, 10, 16]\n",
    "\n",
    "#     cv_entry = 'feature_fraction'\n",
    "#     cv_set = [0.8, 0.99]\n",
    "\n",
    "#     cv_entry = 'bagging_fraction'\n",
    "#     cv_set = [0.70, 0.73, 0.75]\n",
    "\n",
    "#     cv_entry = 'min_sum_hessian_in_leaf'\n",
    "#     cv_set = [1e-4, 1e-3, 1e-2]\n",
    "\n",
    "    cv_entry = 'lambda_l2'\n",
    "    cv_set = [27, 30, 33]\n",
    "\n",
    "    for cv_val in cv_set:\n",
    "        print '\\nCross validating with %s: %s'%(cv_entry, cv_val)\n",
    "        params[cv_entry] = cv_val\n",
    "        # Get predictions\n",
    "        train_res, valid_res, tst_preds, trn_preds, rel_preds = train_lgb_regressor(train=boost_train, \n",
    "                                                                                    target=target_log, \n",
    "                                                                                    test=boost_test,\n",
    "                                                                                    real=real_train,\n",
    "                                                                                    params=params, \n",
    "                                                                                    n_boost=30000)\n",
    "        # Get real and train / val scores\n",
    "        trn_full, trn_miss = get_real_scores(trn_preds=rel_preds,\n",
    "                                             train_target=train_target,\n",
    "                                             trn_miss_idx=trn_miss_idx)\n",
    "        train_res_score = average_best_scores(train_res)\n",
    "        valid_res_score = average_best_scores(valid_res)\n",
    "        # Compile all scores\n",
    "        cv_results[cv_entry+'_full_miss_train_val_%s'%cv_val] = (trn_full, trn_miss,\n",
    "                                                                 train_res_score, valid_res_score)\n",
    "        \n",
    "    # Show cv results\n",
    "    print '\\nCV Results:'\n",
    "    for key in cv_results.keys():\n",
    "        print key, cv_results[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Confirm current parameter settings\n",
    "train_res, valid_res, tst_preds, trn_preds, rel_preds = train_lgb_regressor(train=boost_train, \n",
    "                                                                            target=target_log, \n",
    "                                                                            test=boost_test,\n",
    "                                                                            real=real_train,\n",
    "                                                                            params=params, \n",
    "                                                                            n_boost=30000)\n",
    "# Get real and train / val scores\n",
    "train_res_score = average_best_scores(train_res)\n",
    "valid_res_score = average_best_scores(valid_res)\n",
    "\n",
    "trn_full, trn_miss = get_real_scores(trn_preds=rel_preds,\n",
    "                                     train_target=train_target,\n",
    "                                     trn_miss_idx=trn_miss_idx)\n",
    "\n",
    "print '\\nTraining score:', train_res_score\n",
    "print 'Validation score:', valid_res_score\n",
    "\n",
    "print '\\nReal full training score:', trn_full\n",
    "print 'Real training missed score:', trn_miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_target(tst_leak, tst_preds):\n",
    "    exp_preds = np.expm1(tst_preds)\n",
    "    \n",
    "    fill_idx = np.where(tst_leak==0)[0]\n",
    "    \n",
    "    tmp_leak = tst_leak.copy()\n",
    "    tmp_leak[fill_idx] = exp_preds\n",
    "    return tmp_leak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make submission\n",
    "\n",
    "tst_target = make_test_target(test_leak, tst_preds)\n",
    "\n",
    "save_flag=True\n",
    "\n",
    "sub_name_tst = '../submissions/tstest_lgb_2v4_lag%s_submit.csv'%leak_val\n",
    "tst_df = pd.DataFrame()\n",
    "tst_df['ID'] = test.ID\n",
    "tst_df['target'] = tst_target\n",
    "\n",
    "if save_flag:\n",
    "    tst_df.to_csv(sub_name_tst, index=False)\n",
    "\n",
    "tst_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_square_log_diff(sub1, sub2):\n",
    "    return np.sqrt(np.mean(np.square(np.subtract(np.log1p(sub1), np.log1p(sub2)))))\n",
    "\n",
    "def sum_abs_diff(sub1, sub2):\n",
    "    return np.sum(np.abs(np.subtract(sub1, sub2)))\n",
    "\n",
    "fri_df = pd.read_csv('../submissions/tstest_lgb_2v2_lag36_submit_0.52.csv')\n",
    "sat_df = pd.read_csv('../submissions/tstest_lgb_2v2_lag38_bad_submit_0.52.csv')\n",
    "sun_df = pd.read_csv('../submissions/tstest_lgb_2v2_lag38_good_submit.csv')\n",
    "v5_df = pd.read_csv('../submissions/tstest_lgb_2v5_lag38_submit.csv')\n",
    "\n",
    "right_idx = np.where(test_leak!=0)[0]\n",
    "wrong_idx = np.where(test_leak==0)[0]\n",
    "\n",
    "sub_right = tst_df['target'].values[right_idx]\n",
    "sun_right = sun_df['target'].values[right_idx]\n",
    "fri_right = fri_df['target'].values[right_idx]\n",
    "sat_right = sat_df['target'].values[right_idx]\n",
    "v5_right = v5_df['target'].values[right_idx]\n",
    "\n",
    "sub_wrong = tst_df['target'].values[wrong_idx]\n",
    "sun_wrong = sun_df['target'].values[wrong_idx]\n",
    "fri_wrong = fri_df['target'].values[wrong_idx]\n",
    "sat_wrong = sat_df['target'].values[wrong_idx]\n",
    "v5_wrong = v5_df['target'].values[wrong_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Sum abs diff:'\n",
    "print sum_abs_diff(sub_right, sun_right)\n",
    "print sum_abs_diff(sub_wrong, sun_wrong)\n",
    "\n",
    "print '\\nRoot mean square log diff:'\n",
    "print root_mean_square_log_diff(sub_right, sun_right)\n",
    "print root_mean_square_log_diff(sub_wrong, sun_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Sum abs diff:'\n",
    "print sum_abs_diff(sub_right, v5_right)\n",
    "print sum_abs_diff(sub_wrong, v5_wrong)\n",
    "\n",
    "print '\\nRoot mean square log diff:'\n",
    "print root_mean_square_log_diff(sub_right, v5_right)\n",
    "print root_mean_square_log_diff(sub_wrong, v5_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
