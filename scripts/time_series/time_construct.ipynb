{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Series Reconstruction\n",
    "After verifying the Kaggle community's selection of important features, I will proceed to reconstruct the time-series dataset. Reconstruction will be based on this public kernel: https://www.kaggle.com/johnfarrell/breaking-lb-fresh-start-with-lag-selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pdb\n",
    "import os\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Function for loading h5py file\n",
    "def load_h5py(fname):\n",
    "    with h5py.File(fname, 'r') as handle:\n",
    "        return handle['data'][:]\n",
    "# Function for loading pickle file\n",
    "def load_pickle(fname):\n",
    "    with open(fname, 'rb') as handle:\n",
    "        return pickle.load(handle)\n",
    "\n",
    "\n",
    "# Function for setting up\n",
    "def get_input(debug=False):\n",
    "    '''\n",
    "    Function for loading either debug or full datasets\n",
    "    '''\n",
    "    os.chdir('../../data/compressed/')\n",
    "    print os.getcwd()\n",
    "    pkl_files = ['train_id.pickle', 'trainidx.pickle', 'target.pickle', 'test_id.pickle', 'testidx.pickle']\n",
    "    if debug:\n",
    "        print 'Loading debug train and test datasets...'\n",
    "        # h5py files\n",
    "        train = load_h5py('debug_train.h5')\n",
    "        test = load_h5py('debug_test.h5')\n",
    "        # pickle files\n",
    "        id_train, train_idx, target, id_test, test_idx = [load_pickle('debug_%s'%f) for f in pkl_files]\n",
    "    else:\n",
    "        print 'Loading original train and test datasets...'\n",
    "        # h5py files\n",
    "        train = load_h5py('full_train.h5')\n",
    "        test = load_h5py('full_test.h5')\n",
    "        # pickle files\n",
    "        id_train, train_idx, target, id_test, test_idx = [load_pickle('full_%s'%f) for f in pkl_files]\n",
    "    # Load feature names\n",
    "    fnames = load_pickle('feature_names.pickle')\n",
    "    # Find shape of loaded datasets\n",
    "    print('Shape of training dataset: {} Rows, {} Columns'.format(*train.shape))\n",
    "    print('Shape of test dataset: {} Rows, {} Columns'.format(*test.shape))\n",
    "    os.chdir('../../scripts/time_series/')\n",
    "    print os.getcwd()\n",
    "    return fnames, train, id_train, train_idx, target, test, id_test, test_idx\n",
    "\n",
    "\n",
    "# Function for getting datasets in dataframe format\n",
    "def get_dataframes(debug=False):\n",
    "    # Load data\n",
    "    fnames, train, id_train, train_idx, target, test, id_test, test_idx = get_input(debug)\n",
    "    # Format data\n",
    "    train_df = pd.DataFrame(data=train, index=train_idx, columns=fnames)\n",
    "    train_df['ID'] = id_train\n",
    "    train_df['target'] = target\n",
    "    test_df = pd.DataFrame(data=test, index=test_idx, columns=fnames)\n",
    "    test_df['ID'] = id_test\n",
    "    \n",
    "    print('\\nShape of training dataframe: {} Rows, {} Columns'.format(*train_df.shape))\n",
    "    print('Shape of test dataframe: {} Rows, {} Columns'.format(*test_df.shape))\n",
    "    return fnames, train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting predictions with certain lag assumption\n",
    "def get_leak(df, cols, extras, lag=0):\n",
    "    cols_1 = cols[:((lag + 2)*-1)]\n",
    "    cols_2 = cols[(lag + 2):]\n",
    "    for ext in extras:\n",
    "        cols_1 += ext[:((lag + 2)*-1)]\n",
    "        cols_2 += ext[(lag + 2):]\n",
    "\n",
    "    # All columns except last two + lag into tuple\n",
    "    d1 = df[cols_1].apply(tuple, axis=1).to_frame().rename(columns={0: 'key'})\n",
    "    # All columns except first two + lag into tuple\n",
    "    d2 = df[cols_2].apply(tuple, axis=1).to_frame().rename(columns={0: 'key'})\n",
    "    d2['pred'] = df[cols[lag]]\n",
    "    \n",
    "    # Remove duplicate keys so that join operation will work\n",
    "    d3 = d2[~d2.duplicated(subset=['key'], keep=False)]\n",
    "    d4 = d1[~d1.duplicated(subset=['key'], keep=False)]\n",
    "    d5 = d3.merge(d4, on='key', how='inner')\n",
    "    \n",
    "    d6 = d1.merge(d5, on='key', how='left')\n",
    "    \n",
    "    return d6['pred'].fillna(0)\n",
    "\n",
    "# Function for rewriting leaky dataset UP TO best leak value\n",
    "def rewrite_compiled_leak(leak_df, lag):\n",
    "    # Reset compiled_leak field\n",
    "    leak_df['compiled_leak'] = 0\n",
    "    for i in range(lag):\n",
    "        c = 'leaked_target_%s'%str(i)\n",
    "        zeroleak = leak_df['compiled_leak']==0\n",
    "        leak_df.loc[zeroleak, 'compiled_leak'] = leak_df.loc[zeroleak, c]\n",
    "    return leak_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading singh sets\n",
    "def load_singh(train):\n",
    "    exclude = ['target', 'value_count']\n",
    "    \n",
    "    set_loc = './pattern_singh/'\n",
    "    file_names = os.listdir(set_loc)\n",
    "    file_names = [set_loc+f for f in file_names if '.csv' in f]\n",
    "\n",
    "    singh_sets = []\n",
    "    singh_cols = []\n",
    "    for name in file_names:\n",
    "        tmp_df = pd.read_csv(name, index_col=0)\n",
    "        tmp_df.insert(0, 'target', train.target.values[tmp_df.index.values])\n",
    "        if name==set_loc+'pattern_1166666.66.csv':\n",
    "            tmp_df.rename(columns={'8.50E+43': '850027e38'},inplace=True)\n",
    "        singh_sets.append(tmp_df)\n",
    "        singh_cols.append([c for c in tmp_df.columns.values if c not in exclude])\n",
    "    return singh_sets, singh_cols\n",
    "\n",
    "# Function for loading Aaron test sets v0\n",
    "def load_aaron_v0(count=10):\n",
    "    set_name = './aaron_test_v0.pickle'\n",
    "    \n",
    "    aaron_features = load_pickle(set_name)\n",
    "    return aaron_features[:count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Script\n",
    "try:\n",
    "    del fnames, train, test\n",
    "    print 'Clearing loaded dataframes from memory...\\n'\n",
    "except:\n",
    "    pass\n",
    "fnames, train, test = get_dataframes(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load important features\n",
    "cols = load_pickle('./important.pickle')\n",
    "\n",
    "# Load extra_sets\n",
    "a_count = 55\n",
    "extra_features= load_aaron_v0(count=a_count)\n",
    "# extra_sets, extra_features = load_singh(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format target\n",
    "y = np.log1p(train['target']).values\n",
    "log_mean = y.mean()\n",
    "test['target'] = log_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leak compilation for training set\n",
    "extra_cols = ['compiled_leak', 'nonzero_mean']\n",
    "\n",
    "# Function for compiling leak results over many lag values\n",
    "def compiled_leak_result():\n",
    "    # Define number of lag values to consider\n",
    "    max_nlags = len(cols)-2\n",
    "    # Define leaky train set\n",
    "    train_leak = train[['ID', 'target'] + list(cols)]\n",
    "    # Initialize compiled_leak as zeros\n",
    "    train_leak['compiled_leak'] = 0\n",
    "    train_leak['nonzero_mean'] = train[fnames].apply(lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1)\n",
    "    # Initialize empty lists\n",
    "    scores = []\n",
    "    leaky_value_counts = []\n",
    "    leaky_value_corrects = []\n",
    "    leaky_cols = []\n",
    "    \n",
    "    for i in range(max_nlags):\n",
    "        c = 'leaked_target_%s'%str(i)\n",
    "        \n",
    "        print '\\nProcessing Lag:', i\n",
    "        # Get predictions for current lag and store in new column\n",
    "        train_leak[c] = get_leak(train, list(cols), extra_features, i)\n",
    "        \n",
    "        # Update leaky_cols with latest lag label\n",
    "        leaky_cols.append(c)\n",
    "        # Get \"grounding\" by joining with original training dataset\n",
    "        train_leak = train.join(train_leak.set_index('ID')[leaky_cols + extra_cols], \n",
    "                                on='ID', how='left')[['ID', 'target'] + list(cols) + leaky_cols + extra_cols]\n",
    "        # Iteratively fill in compiled_leak values for increasing lag\n",
    "        zeroleak = train_leak['compiled_leak'] == 0\n",
    "        train_leak.loc[zeroleak, 'compiled_leak'] = train_leak.loc[zeroleak, c]\n",
    "        \n",
    "        # Number of leaky values found so far\n",
    "        leaky_value_counts.append(np.sum(train_leak['compiled_leak']>0))\n",
    "        # Number of correct discovered leaky values\n",
    "        _correct_counts = np.sum(train_leak['compiled_leak']==train_leak['target'])\n",
    "        # Percentage of correct discovered leaky values\n",
    "        leaky_value_corrects.append(1.0*_correct_counts/leaky_value_counts[-1])\n",
    "        \n",
    "        print 'Number of leak values found in train:', leaky_value_counts[-1]\n",
    "        print 'Percentage of correct leak values in train:', leaky_value_corrects[-1]\n",
    "        \n",
    "        # Find score of current compilation iteration\n",
    "        tmp = train_leak.copy()  # Temporary dataframe\n",
    "        tmp.loc[zeroleak, 'compiled_leak'] = tmp.loc[zeroleak, 'nonzero_mean']\n",
    "        scores.append(np.sqrt(mean_squared_error(y, np.log1p(tmp['compiled_leak']).fillna(log_mean))))\n",
    "        \n",
    "        print 'Score (filled with nonzero mean):', scores[-1]\n",
    "    \n",
    "    # End of iterations\n",
    "    result = dict(score=scores,\n",
    "                  leaky_count = leaky_value_counts,\n",
    "                  leaky_correct = leaky_value_corrects)\n",
    "    \n",
    "    return train_leak, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get leaked training data and result\n",
    "train_leak, result = compiled_leak_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format results \n",
    "result = pd.DataFrame.from_dict(result, orient='columns')\n",
    "result.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best score and lag value\n",
    "best_score = np.min(result['score'])\n",
    "best_lag = np.argmin(result['score'])\n",
    "print 'Best score:', best_score\n",
    "print 'Best lag value:', best_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite leaky training set in terms of best lag\n",
    "leaky_cols = [c for c in train_leak.columns if 'leaked_target_' in c]\n",
    "train_leak = rewrite_compiled_leak(train_leak, best_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train leak\n",
    "train_leak_name = './stats/train_leak_%s.csv'%best_lag\n",
    "train_leak.to_csv(train_leak_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leak compilation for test set\n",
    "# Function for compiling leaky values for test set\n",
    "def compiled_leak_result_test():\n",
    "    max_nlags = len(cols)-2\n",
    "    \n",
    "    test_leak = test[['ID', 'target'] + list(cols)]\n",
    "    test_leak['compiled_leak'] = 0\n",
    "    test_leak['nonzero_mean'] = test[fnames].apply(lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1)\n",
    "    \n",
    "    leaky_value_counts = []\n",
    "    leaky_cols = []\n",
    "    \n",
    "    for i in range(max_nlags):\n",
    "        c = 'leaked_target_%s'%str(i)\n",
    "        \n",
    "        print '\\nProcessing Lag:', i\n",
    "        test_leak[c] = get_leak(test, list(cols), extra_features, i)\n",
    "        leaky_cols.append(c)\n",
    "        \n",
    "        test_leak = test.join(test_leak.set_index('ID')[leaky_cols + extra_cols], \n",
    "                              on='ID', how='left')[['ID', 'target'] + list(cols) + leaky_cols + extra_cols]\n",
    "        zeroleak = test_leak['compiled_leak']==0\n",
    "        test_leak.loc[zeroleak, 'compiled_leak'] = test_leak.loc[zeroleak, c]\n",
    "        leaky_value_counts.append(np.sum(test_leak['compiled_leak']>0))\n",
    "        \n",
    "        print 'Number of leaky values found in test:', leaky_value_counts[-1]\n",
    "        \n",
    "    # End iterations\n",
    "    result = dict(leaky_count = leaky_value_counts)\n",
    "    \n",
    "    return test_leak, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get leaked test data and result\n",
    "test_leak, test_result = compiled_leak_result_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format test results\n",
    "test_result = pd.DataFrame.from_dict(test_result, orient='columns')\n",
    "test_result.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite leaky test set in terms of best lag\n",
    "test_leak = rewrite_compiled_leak(test_leak, best_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test leak\n",
    "test_leak_name = './stats/test_leak_%s.csv'%best_lag\n",
    "test_leak.to_csv(test_leak_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(test_leak.compiled_leak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_flag = False\n",
    "if submit_flag:\n",
    "    # Replace zeros in compiled_leak field\n",
    "    test_leak.loc[test_leak['compiled_leak']==0, 'compiled_leak'] = test_leak.loc[test_leak['compiled_leak']==0, \n",
    "                                                                                  'nonzero_mean']\n",
    "\n",
    "    submit_name = '../../submissions/recon_a%s_lag%s_submit.csv'%(a_count, best_lag)\n",
    "    # Make and save submission\n",
    "    sub = pd.DataFrame()\n",
    "    sub['ID'] = test['ID']\n",
    "    sub['target'] = test_leak['compiled_leak']\n",
    "    sub.to_csv(submit_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
