{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Feature Similarity with Target Variable\n",
    "Given that the time-series nature of the Santander datasets has become exposed to the Kaggle community, I will be conducting an exploration of various features' correlation with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading h5py file\n",
    "def load_h5py(fname):\n",
    "    with h5py.File(fname, 'r') as handle:\n",
    "        return handle['data'][:]\n",
    "# Function for loading pickle file\n",
    "def load_pickle(fname):\n",
    "    with open(fname, 'rb') as handle:\n",
    "        return pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for setting up\n",
    "def get_input(debug=False):\n",
    "    '''\n",
    "    Function for loading either debug or full datasets\n",
    "    '''\n",
    "    os.chdir('../../data/compressed/')\n",
    "    print os.getcwd()\n",
    "    pkl_files = ['train_id.pickle', 'trainidx.pickle', 'target.pickle', 'test_id.pickle', 'testidx.pickle']\n",
    "    if debug:\n",
    "        print 'Loading debug train and test datasets...'\n",
    "        # h5py files\n",
    "        train = load_h5py('debug_train.h5')\n",
    "        test = load_h5py('debug_test.h5')\n",
    "        # pickle files\n",
    "        id_train, train_idx, target, id_test, test_idx = [load_pickle('debug_%s'%f) for f in pkl_files]\n",
    "    else:\n",
    "        print 'Loading original train and test datasets...'\n",
    "        # h5py files\n",
    "        train = load_h5py('full_train.h5')\n",
    "        test = load_h5py('full_test.h5')\n",
    "        # pickle files\n",
    "        id_train, train_idx, target, id_test, test_idx = [load_pickle('full_%s'%f) for f in pkl_files]\n",
    "    # Load feature names\n",
    "    fnames = load_pickle('feature_names.pickle')\n",
    "    # Find shape of loaded datasets\n",
    "    print('Shape of training dataset: {} Rows, {} Columns'.format(*train.shape))\n",
    "    print('Shape of test dataset: {} Rows, {} Columns'.format(*test.shape))\n",
    "    os.chdir('../../scripts/time_series/')\n",
    "    print os.getcwd()\n",
    "    return fnames, train, id_train, train_idx, target, test, id_test, test_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "fnames, train, id_train, train_idx, target, test, id_test, test_idx = get_input(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properties of target values\n",
    "# Number of values\n",
    "print 'Number of target values:', len(target)\n",
    "\n",
    "# Unique values\n",
    "unique_target = np.unique(target)\n",
    "print 'Number of unique target values:', len(unique_target)\n",
    "\n",
    "# Whole numbers\n",
    "float_target = np.asarray(target, dtype=float)\n",
    "print 'Number of whole numbers in target values:', np.sum(np.array([x.is_integer() for x in float_target]))\n",
    "\n",
    "# Values with decimal precision\n",
    "decimal_target = target[np.invert(np.array([x.is_integer() for x in float_target]))]\n",
    "decimal_target_str = [str(x) for x in decimal_target]\n",
    "avg_decimals = np.mean(np.array([len(x.split('.')[1]) for x in decimal_target_str]))\n",
    "print 'Average number of decimals values in non-whole numbers: %0.4f'%avg_decimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find length of intersections for features and target\n",
    "intersect_lengths = np.zeros(train.shape[1])\n",
    "for i in range(train.shape[1]):\n",
    "    intersect_lengths[i] = len(np.intersect1d(train[:, i], target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature-target intersection histogram\n",
    "plt.figure(figsize=(12, 7))\n",
    "n, bins, patches = plt.hist(intersect_lengths, 100, alpha=0.5)\n",
    "plt.title('Feature-Target Intersection Histogram')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Number of Common Entities')\n",
    "plt.savefig('./images/feat_target_inter.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histogram shown above, it can be seen that a majority of the features do not share any common values with the target. That being said, there seem to be a few features that share a startling number of values with the target.\n",
    "\n",
    "Around the middle of July (2018), there was a massive upset of the Santander competition leaderboard where it was revealed that the datasets provided by Santander were actually scrambled time-series data both along the rows and columns. From that point onwards, many of the public Kaggle kernels focused on locating features in the training and test datasets that could help in the reconstruction of this time-series dataset. Proposed feature sets have been made public-knowledge on these forums.\n",
    "\n",
    "To validate these publicly posted features, I will be comparing them with the features shown in the sparse end of the long-tail distribution shown above. The exact features are taken from: https://www.kaggle.com/johnfarrell/breaking-lb-fresh-start-with-lag-selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important features as identified by Kaggle\n",
    "kags = ['f190486d6', '58e2e02e6', 'eeb9cd3aa', '9fd594eec', '6eef030c1', '15ace8c9f', \n",
    "        'fb0f5dbfe', '58e056e12', '20aa07010', '024c577b9', 'd6bb78916', 'b43a7cfd5', \n",
    "        '58232a6fb', '1702b5bf0', '324921c7b', '62e59a501', '2ec5b290f', '241f0f867', \n",
    "        'fb49e4212', '66ace2992', 'f74e8f13d', '5c6487af1', '963a49cdc', '26fc93eb7', \n",
    "        '1931ccfdd', '703885424', '70feb1494', '491b9ee45', '23310aa6f', 'e176a204a', \n",
    "        '6619d81fc', '1db387535', 'fc99f9426', '91f701ba2', '0572565c2', '190db8488', \n",
    "        'adb64ff71', 'c47340d97', 'c5a231d81', '0ff32eb98'\n",
    "       ]\n",
    "kags = np.array(kags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_idx = np.where(intersect_lengths>300)[0]\n",
    "important_cols = fnames[important_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Kaggle indexes are equivalent to indexes I discovered\n",
    "np.array_equal(np.sort(kags), np.sort(important_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this result, it is determined that the Kaggle community most likely decided upon these features through an analysis similar to my own - that is, selecting the features that share the most similarites to the target variable.\n",
    "\n",
    "One last thing to note is that while I've verified the features, I haven't verified the proper order in which they are presented in within the Kaggle community. The order of these features is extremely significant since the proper sequence is required to reconstruct the time-series nature of the dataset. I will not be double-checking this facet of the Kaggle community's findings and will proceed using their proposed ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save important columns into pickle file\n",
    "cols_fname = './important.pickle'\n",
    "with open(cols_fname, 'wb') as handle:\n",
    "    pickle.dump(kags, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
