{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pdb\n",
    "import os\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Function for loading h5py file\n",
    "def load_h5py(fname):\n",
    "    with h5py.File(fname, 'r') as handle:\n",
    "        return handle['data'][:]\n",
    "# Function for loading pickle file\n",
    "def load_pickle(fname):\n",
    "    with open(fname, 'rb') as handle:\n",
    "        return pickle.load(handle)\n",
    "\n",
    "\n",
    "# Function for setting up\n",
    "def get_input(debug=False):\n",
    "    '''\n",
    "    Function for loading either debug or full datasets\n",
    "    '''\n",
    "    os.chdir('../../data/compressed/')\n",
    "    print os.getcwd()\n",
    "    pkl_files = ['train_id.pickle', 'trainidx.pickle', 'target.pickle', 'test_id.pickle', 'testidx.pickle']\n",
    "    if debug:\n",
    "        print 'Loading debug train and test datasets...'\n",
    "        # h5py files\n",
    "        train = load_h5py('debug_train.h5')\n",
    "        test = load_h5py('debug_test.h5')\n",
    "        # pickle files\n",
    "        id_train, train_idx, target, id_test, test_idx = [load_pickle('debug_%s'%f) for f in pkl_files]\n",
    "    else:\n",
    "        print 'Loading original train and test datasets...'\n",
    "        # h5py files\n",
    "        train = load_h5py('full_train.h5')\n",
    "        test = load_h5py('full_test.h5')\n",
    "        # pickle files\n",
    "        id_train, train_idx, target, id_test, test_idx = [load_pickle('full_%s'%f) for f in pkl_files]\n",
    "    # Load feature names\n",
    "    fnames = load_pickle('feature_names.pickle')\n",
    "    # Find shape of loaded datasets\n",
    "    print('Shape of training dataset: {} Rows, {} Columns'.format(*train.shape))\n",
    "    print('Shape of test dataset: {} Rows, {} Columns'.format(*test.shape))\n",
    "    os.chdir('../../scripts/time_series/')\n",
    "    print os.getcwd()\n",
    "    return fnames, train, id_train, train_idx, target, test, id_test, test_idx\n",
    "\n",
    "\n",
    "# Function for getting datasets in dataframe format\n",
    "def get_dataframes(debug=False):\n",
    "    # Load data\n",
    "    fnames, train, id_train, train_idx, target, test, id_test, test_idx = get_input(debug)\n",
    "    # Format data\n",
    "    train_df = pd.DataFrame(data=train, index=train_idx, columns=fnames)\n",
    "    train_df['ID'] = id_train\n",
    "    train_df['target'] = target\n",
    "    test_df = pd.DataFrame(data=test, index=test_idx, columns=fnames)\n",
    "    test_df['ID'] = id_test\n",
    "    \n",
    "    print('\\nShape of training dataframe: {} Rows, {} Columns'.format(*train_df.shape))\n",
    "    print('Shape of test dataframe: {} Rows, {} Columns'.format(*test_df.shape))\n",
    "    return fnames, train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_in_batches(trn, f):\n",
    "    set_size = 100\n",
    "    threshold = 50\n",
    "    \n",
    "    # Row non-zero counts without target and ID\n",
    "    nz_row_count = np.count_nonzero(trn[f], axis=1)\n",
    "    filtered_len = len(np.where(nz_row_count>=threshold)[0])\n",
    "    \n",
    "    row_idx = np.argsort(-nz_row_count)\n",
    "    \n",
    "    counter = 0\n",
    "    sample_sets = []\n",
    "    while set_size < filtered_len:\n",
    "        row_set_idx = row_idx[counter*set_size:(counter+1)*set_size]\n",
    "        tmp_trn = trn.iloc[row_set_idx]\n",
    "        tmp_trn['row_nz'] = nz_row_count[row_set_idx]\n",
    "        sample_sets.append(tmp_trn)\n",
    "        filtered_len -= set_size\n",
    "        counter += 1\n",
    "    \n",
    "    return sample_sets\n",
    "\n",
    "def calc_prob(c_row, df):\n",
    "    '''\n",
    "    Calculates probability that a row in a dataframe is part of a sequence\n",
    "    belonging to c_row by the value of (intersection / union)\n",
    "    '''\n",
    "    # Get count information for current row\n",
    "    c_vals, c_counts = np.unique(c_row, return_counts=True)\n",
    "    c_dict = {'vals': c_vals, 'c_counts': c_counts}\n",
    "    c_df = pd.DataFrame.from_dict(c_dict, orient='columns').set_index('vals')\n",
    "    \n",
    "    # Get count information for rows of dataframe\n",
    "    probs = np.zeros(df.shape[0])\n",
    "    for i, (_, row) in enumerate(df.iterrows()):\n",
    "        row_vals, row_counts = np.unique(row, return_counts=True)\n",
    "        row_dict = {'vals': row_vals, 'row_counts': row_counts}\n",
    "        row_df = pd.DataFrame.from_dict(row_dict, orient='columns').set_index('vals')\n",
    "        joint_df = c_df.join(row_df, how='outer').reset_index().replace(np.nan, 0)\n",
    "        joint_df.drop([0], inplace=True)\n",
    "        joint_df['abs_diff'] = np.abs(joint_df['c_counts'] - joint_df['row_counts'])\n",
    "        probs[i] = joint_df['c_counts'].sum() / (joint_df['c_counts'].sum() + joint_df['abs_diff'].sum())\n",
    "    return probs\n",
    "\n",
    "def initialize_by_target(df, f):\n",
    "    '''\n",
    "    Chooses row with target that has highest occurrences amongst\n",
    "    dataframe rows\n",
    "    '''\n",
    "    target_occurrences = np.zeros(df.shape[0])\n",
    "    target_idx = np.zeros(df.shape[0])\n",
    "    \n",
    "    for i, (idx, row) in enumerate(df.iterrows()):\n",
    "        contain_idx = (df[f]==row['target']).any(axis=1)\n",
    "        target_occurrences[i] = contain_idx.sum()\n",
    "        target_idx[i] = idx\n",
    "    \n",
    "    sorted_occ = np.argsort(-target_occurrences)\n",
    "    best_row_idx = target_idx[sorted_occ][:10]\n",
    "    \n",
    "    return best_row_idx.astype(int)\n",
    "\n",
    "def filter_rows(df, target):\n",
    "    '''\n",
    "    Filters dataframe for rows containing target\n",
    "    '''\n",
    "    contain_idx = (df==target).any(axis=1)\n",
    "    true_idx = contain_idx[contain_idx==True].index.values\n",
    "    return true_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_dataset(data, f, idx_to_use, density_df):\n",
    "    '''\n",
    "    Mines for structure in given data sample\n",
    "    '''\n",
    "    # Initialize parameters\n",
    "    ignore_mask = np.ones(data.shape[0], dtype=bool)\n",
    "    f_list = []\n",
    "    r_list = []\n",
    "    idx_list = data.index.values\n",
    "    \n",
    "    # Initialize search set\n",
    "    current_idx = idx_to_use\n",
    "    \n",
    "    current_row = data[f].loc[current_idx]\n",
    "    current_target = data['target'].loc[current_idx]\n",
    "    \n",
    "    while True:\n",
    "        # Update ignore mask\n",
    "        print 'Working on index:', current_idx\n",
    "        ignore_mask[np.where(idx_list==current_idx)[0]] = False\n",
    "        masked_idx = idx_list[ignore_mask]\n",
    "        \n",
    "        # Get filtered dataset\n",
    "        filtered_idx = filter_rows(data[f].loc[masked_idx], current_target)\n",
    "        if len(filtered_idx)==0:\n",
    "            print 'No more rows are available for filtering. Exiting...'\n",
    "            break\n",
    "        subset_df = data[f].loc[filtered_idx]\n",
    "        subset_idx = subset_df.index.values\n",
    "        \n",
    "        probs = calc_prob(current_row, subset_df)\n",
    "        \n",
    "        new_idx = subset_idx[np.argmax(probs)]\n",
    "        new_row = subset_df.loc[new_idx]\n",
    "        feature_mask = new_row==current_target\n",
    "        new_feature = feature_mask[feature_mask==True].index.values\n",
    "        # Prioritize feature if necessary\n",
    "        if len(new_feature)>1:\n",
    "            f_list.append(density_df.loc[new_feature].sort_values(by=['density']).index[-1])\n",
    "        else:\n",
    "            f_list.append(new_feature[0])\n",
    "        # Add new row\n",
    "        r_list.append(current_idx)\n",
    "\n",
    "        # Set next iteration parameters\n",
    "        current_idx = new_idx\n",
    "        current_row = new_row\n",
    "        current_target = data['target'].loc[current_idx]\n",
    "        \n",
    "    return f_list, r_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Script\n",
    "try:\n",
    "    del fnames, train, test\n",
    "    print 'Clearing loaded dataframes from memory...\\n'\n",
    "except:\n",
    "    pass\n",
    "fnames, train, test = get_dataframes(debug=False)\n",
    "\n",
    "# Column density\n",
    "col_density = np.count_nonzero(train[fnames], axis=0)\n",
    "density_dict = {'name': fnames, 'density': col_density}\n",
    "density_df = pd.DataFrame.from_dict(density_dict, orient='columns').set_index('name')\n",
    "\n",
    "# Get samples\n",
    "samples = samples_in_batches(train, fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get orderings\n",
    "results = {}\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    print 'Working with sample:', i\n",
    "    nz_min = np.min(sample['row_nz'])\n",
    "    nz_max = np.max(sample['row_nz'])\n",
    "    c = 'Sample_%s_%s'%(nz_max, nz_min)\n",
    "    \n",
    "    starting_idx_list = initialize_by_target(sample, fnames)\n",
    "    \n",
    "    sample_lists = {}\n",
    "    for s_idx in starting_idx_list:\n",
    "        s = 'index_%s'%s_idx\n",
    "        f_list, r_list = order_dataset(sample, fnames, s_idx, density_df)\n",
    "        s_dict = {'features': f_list, 'rows': r_list}\n",
    "        s_df = pd.DataFrame.from_dict(s_dict, orient='columns')\n",
    "        sample_lists[s] = s_df\n",
    "        \n",
    "    results[c] = sample_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_name = './candidate_sets.pickle'\n",
    "with open(results_name, 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
