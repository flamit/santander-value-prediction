{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pdb\n",
    "import os\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Function for loading h5py file\n",
    "def load_h5py(fname):\n",
    "    with h5py.File(fname, 'r') as handle:\n",
    "        return handle['data'][:]\n",
    "# Function for loading pickle file\n",
    "def load_pickle(fname):\n",
    "    with open(fname, 'rb') as handle:\n",
    "        return pickle.load(handle)\n",
    "\n",
    "\n",
    "# Function for setting up\n",
    "def get_input(debug=False):\n",
    "    '''\n",
    "    Function for loading either debug or full datasets\n",
    "    '''\n",
    "    os.chdir('../../data/compressed/')\n",
    "    print os.getcwd()\n",
    "    pkl_files = ['train_id.pickle', 'trainidx.pickle', 'target.pickle', 'test_id.pickle', 'testidx.pickle']\n",
    "    if debug:\n",
    "        print 'Loading debug train and test datasets...'\n",
    "        # h5py files\n",
    "        train = load_h5py('debug_train.h5')\n",
    "        test = load_h5py('debug_test.h5')\n",
    "        # pickle files\n",
    "        id_train, train_idx, target, id_test, test_idx = [load_pickle('debug_%s'%f) for f in pkl_files]\n",
    "    else:\n",
    "        print 'Loading original train and test datasets...'\n",
    "        # h5py files\n",
    "        train = load_h5py('full_train.h5')\n",
    "        test = load_h5py('full_test.h5')\n",
    "        # pickle files\n",
    "        id_train, train_idx, target, id_test, test_idx = [load_pickle('full_%s'%f) for f in pkl_files]\n",
    "    # Load feature names\n",
    "    fnames = load_pickle('feature_names.pickle')\n",
    "    # Find shape of loaded datasets\n",
    "    print('Shape of training dataset: {} Rows, {} Columns'.format(*train.shape))\n",
    "    print('Shape of test dataset: {} Rows, {} Columns'.format(*test.shape))\n",
    "    os.chdir('../../scripts/time_series/')\n",
    "    print os.getcwd()\n",
    "    return fnames, train, id_train, train_idx, target, test, id_test, test_idx\n",
    "\n",
    "\n",
    "# Function for getting datasets in dataframe format\n",
    "def get_dataframes(debug=False):\n",
    "    # Load data\n",
    "    fnames, train, id_train, train_idx, target, test, id_test, test_idx = get_input(debug)\n",
    "    # Format data\n",
    "    train_df = pd.DataFrame(data=train, index=train_idx, columns=fnames)\n",
    "    train_df['ID'] = id_train\n",
    "    train_df['target'] = target\n",
    "    test_df = pd.DataFrame(data=test, index=test_idx, columns=fnames)\n",
    "    test_df['ID'] = id_test\n",
    "    \n",
    "    print('\\nShape of training dataframe: {} Rows, {} Columns'.format(*train_df.shape))\n",
    "    print('Shape of test dataframe: {} Rows, {} Columns'.format(*test_df.shape))\n",
    "    return fnames, train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading Aaron test sets v0\n",
    "def load_aaron_v0(count=10):\n",
    "    set_name = './aaron_test_v0.pickle'\n",
    "    \n",
    "    aaron_features = load_pickle(set_name)\n",
    "    return aaron_features[:count]\n",
    "\n",
    "# Function for loading leaks\n",
    "def load_leaks(leak_val):\n",
    "    leak_dir = './stats/'\n",
    "    \n",
    "    train_leak_loc = leak_dir + 'train_leak_%s.csv'%leak_val\n",
    "    train_leak = pd.read_csv(train_leak_loc).compiled_leak\n",
    "    test_leak_loc = leak_dir + 'test_leak_%s.csv'%leak_val\n",
    "    test_leak = pd.read_csv(test_leak_loc).compiled_leak\n",
    "    \n",
    "    return train_leak, test_leak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del fnames, train, test\n",
    "    print 'Clearing loaded dataframes from memory...\\n'\n",
    "except:\n",
    "    pass\n",
    "fnames, train, test = get_dataframes(debug=False)\n",
    "\n",
    "# Load leaks\n",
    "leak_val = 38\n",
    "print '\\nLoading train and test leaks...'\n",
    "train_leak, test_leak = load_leaks(leak_val)\n",
    "print 'Nonzero elements in train:', np.count_nonzero(train_leak)\n",
    "print 'Nonzero elements in test:', np.count_nonzero(test_leak)\n",
    "\n",
    "# Load important features\n",
    "print '\\nLoading important (Giba) features...\\n'\n",
    "cols = load_pickle('./important.pickle')\n",
    "\n",
    "# Load extra_sets\n",
    "print 'Loading extra features...\\n'\n",
    "extra_features= load_aaron_v0(count=55)\n",
    "\n",
    "# Load unmatched test indexes\n",
    "print 'Loading unmatched test indexes...\\n'\n",
    "public_test_idx = load_pickle('../model_data/unmatched_public_idxs.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_right_idx = np.where(train_leak!=0)[0]\n",
    "tst_right_idx = np.where(test_leak!=0)[0]\n",
    "\n",
    "trn_wrong_idx = np.where(train_leak==0)[0]\n",
    "tst_wrong_idx = np.where(test_leak==0)[0]\n",
    "\n",
    "matched_train = train.loc[trn_right_idx]\n",
    "matched_test = test.loc[tst_right_idx]\n",
    "\n",
    "unmatched_train = train.loc[trn_wrong_idx]\n",
    "unmatched_test = test.loc[tst_wrong_idx]\n",
    "\n",
    "public_unmatched_test = test.loc[public_test_idx]\n",
    "\n",
    "# Check accuracy of matches\n",
    "if np.array_equal(matched_train['target'], train_leak[trn_right_idx]):\n",
    "    print 'Train targets match train leaks!'\n",
    "\n",
    "# Setting targets and log targets\n",
    "matched_test['target'] = test_leak[tst_right_idx]\n",
    "\n",
    "matched_train['log_target'] = np.log1p(matched_train['target'])\n",
    "matched_test['log_target'] = np.log1p(matched_test['target'])\n",
    "\n",
    "\n",
    "# Format data\n",
    "flat_tst_public = np.log1p(public_unmatched_test[fnames].values.reshape(-1))\n",
    "flat_tst_public = flat_tst_public[np.where(flat_tst_public!=0)[0]]\n",
    "\n",
    "# Train leak samples' feature values without zeros\n",
    "flat_trn_leak = np.log1p(matched_train[fnames].values.reshape(-1))\n",
    "flat_trn_leak = flat_trn_leak[np.where(flat_trn_leak!=0)[0]]\n",
    "# Test leak samples' feature values without zeros\n",
    "flat_tst_leak = np.log1p(matched_test[fnames].values.reshape(-1))\n",
    "flat_tst_leak = flat_tst_leak[np.where(flat_tst_leak!=0)[0]]\n",
    "\n",
    "# Train missed samples' feature values without zeros\n",
    "flat_trn_miss = np.log1p(unmatched_train[fnames].values.reshape(-1))\n",
    "flat_trn_miss = flat_trn_miss[np.where(flat_trn_miss!=0)[0]]\n",
    "# Test missed samples' feature values without zeros\n",
    "flat_tst_miss = np.log1p(unmatched_test[fnames].values.reshape(-1))\n",
    "flat_tst_miss = flat_tst_miss[np.where(flat_tst_miss!=0)[0]]\n",
    "\n",
    "all_data = np.concatenate([flat_trn_leak, flat_tst_leak])\n",
    "all_data_min = np.min(all_data)\n",
    "all_data_max = np.max(all_data)\n",
    "\n",
    "all_targets = np.concatenate([matched_train['log_target'].values, matched_test['log_target'].values])\n",
    "all_targ_min = np.min(all_targets)\n",
    "all_targ_max = np.max(all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scaler(set_x, set_y):\n",
    "    xlen = len(set_x)\n",
    "    ylen = len(set_y)\n",
    "    \n",
    "    correct_using = np.argmin([xlen, ylen])\n",
    "    \n",
    "    new_set = []\n",
    "    if correct_using == 0:\n",
    "        scaling = np.ceil(ylen/xlen)\n",
    "        for i in range(int(scaling)):\n",
    "            new_set.append(set_x)\n",
    "        combined = np.concatenate(new_set)\n",
    "        return combined, set_y\n",
    "    else:\n",
    "        scaling = np.ceil(xlen/ylen)\n",
    "        for i in range(int(scaling)):\n",
    "            new_set.append(set_y)\n",
    "        combined = np.concatenate(new_set)\n",
    "        return set_x, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, tst = data_scaler(matched_train['log_target'], matched_test['log_target'])\n",
    "\n",
    "bins = np.linspace(all_targ_min*1.1, all_targ_max*1.1, 100)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.hist(trn, bins, alpha=0.5, label='train')\n",
    "plt.hist(tst, bins, alpha=0.5, label='test')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Matched Train and Test Target Overlaps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match, unmatch = data_scaler(flat_trn_leak, flat_trn_miss)\n",
    "\n",
    "bins = np.linspace(all_targ_min*1.1, all_targ_max*1.1, 100)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.hist(match, bins, alpha=0.5, label='train match')\n",
    "plt.hist(unmatch, bins, alpha=0.5, label='train unmatch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Matched Train and Unmatched Train Data Overlaps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, tst = data_scaler(flat_trn_miss, flat_tst_miss)\n",
    "\n",
    "bins = np.linspace(all_targ_min*1.1, all_targ_max*1.1, 100)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.hist(trn, bins, alpha=0.5, label='train unmatch')\n",
    "plt.hist(tst, bins, alpha=0.5, label='test unmatch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Unmatched Train and Test Data Overlaps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, tst = data_scaler(flat_trn_leak, flat_tst_leak)\n",
    "\n",
    "bins = np.linspace(all_data_min*1.1, all_data_max*1.1, 100)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.hist(trn, bins, alpha=0.5, label='train matched')\n",
    "plt.hist(tst, bins, alpha=0.5, label='test matched')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Test to Train Leak Overlaps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leak, miss = data_scaler(flat_tst_leak, flat_tst_miss)\n",
    "\n",
    "bins = np.linspace(all_data_min*1.1, all_data_max*1.1, 100)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.hist(leak, bins, alpha=0.5, label='test matched')\n",
    "plt.hist(miss, bins, alpha=0.5, label='all test unmatched')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Test Data Overlaps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leak, public = data_scaler(flat_tst_leak, flat_tst_public)\n",
    "\n",
    "bins = np.linspace(all_data_min*1.1, all_data_max*1.1, 100)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.hist(leak, bins, alpha=0.5, label='test matched')\n",
    "plt.hist(public, bins, alpha=0.5, label='public test unmatched')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Test Data (Leak and Public) Overlaps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find mistakes made in the training leak\n",
    "target = train['target']\n",
    "target_log = np.log1p(target)\n",
    "\n",
    "zero_leak_trn = np.where(train_leak==0)[0]\n",
    "wrong_trn_idx = np.where(train_leak!=target)[0]\n",
    "print 'Zero leak idx equivalent to wrong train idx?:', np.array_equal(zero_leak_trn, wrong_trn_idx)\n",
    "print 'Number of zero-value leaks in training leaks:', len(zero_leak_trn)\n",
    "\n",
    "zero_leak_tst = np.where(test_leak==0)[0]\n",
    "print '\\nNumber of zero-value leaks in test leaks:', len(zero_leak_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for matching samples to predictions with 2-lag assumption\n",
    "def two_get_leak(df, cols, extras, lag=0):\n",
    "    cols_1 = cols[:((lag + 2)*-1)]\n",
    "    cols_2 = cols[(lag + 2):]\n",
    "    for ext in extras:\n",
    "        cols_1 += ext[:((lag + 2)*-1)]\n",
    "        cols_2 += ext[(lag + 2):]\n",
    "\n",
    "    # All columns except last two + lag into tuple\n",
    "    d1 = df[cols_1].apply(tuple, axis=1).to_frame().rename(columns={0: 'key'})\n",
    "    d1['index_one'] = d1.index.values\n",
    "    # All columns except first two + lag into tuple\n",
    "    d2 = df[cols_2].apply(tuple, axis=1).to_frame().rename(columns={0: 'key'})\n",
    "    d2['index_two'] = d2.index.values\n",
    "    d2['pred'] = df[cols[lag]]\n",
    "    \n",
    "    # Remove duplicate keys for accurate matching\n",
    "    d3 = d2[~d2.duplicated(subset=['key'], keep=False)]\n",
    "    d4 = d1[~d1.duplicated(subset=['key'], keep=False)]\n",
    "    d5 = d3.merge(d4, on='key', how='inner')\n",
    "    d5.drop(labels='index_one', axis=1, inplace=True)\n",
    "    \n",
    "    d6 = d1.merge(d5, on='key', how='left')\n",
    "    d6['matches'] = d6.apply(lambda x: (x['index_one'], x['index_two']), axis=1)\n",
    "    \n",
    "    return d6['pred'].fillna(0), d6['matches']\n",
    "\n",
    "\n",
    "# Function for matching samples with 1-lag assumption\n",
    "def one_get_leak(df, cols, extras, lag=0):\n",
    "    cols_1 = cols[:((lag + 1)*-1)]\n",
    "    cols_2 = cols[(lag + 1):]\n",
    "    for ext in extras:\n",
    "        cols_1 += ext[:((lag + 1)*-1)]\n",
    "        cols_2 += ext[(lag + 1):]\n",
    "\n",
    "    # All columns except last one + lag into tuple\n",
    "    d1 = df[cols_1].apply(tuple, axis=1).to_frame().rename(columns={0: 'key'})\n",
    "    d1['index_one'] = d1.index.values\n",
    "    # All columns except first one + lag into tuple\n",
    "    d2 = df[cols_2].apply(tuple, axis=1).to_frame().rename(columns={0: 'key'})\n",
    "    d2['index_two'] = d2.index.values\n",
    "    d2['pred'] = df[cols[lag]]\n",
    "    \n",
    "    # Remove duplicate keys for accurate matching\n",
    "    d3 = d2[~d2.duplicated(subset=['key'], keep=False)]\n",
    "    d4 = d1[~d1.duplicated(subset=['key'], keep=False)]\n",
    "    d5 = d3.merge(d4, on='key', how='inner')\n",
    "    d5.drop(labels='index_one', axis=1, inplace=True)\n",
    "    \n",
    "    d6 = d1.merge(d5, on='key', how='left')\n",
    "    d6['matches'] = d6.apply(lambda x: (x['index_one'], x['index_two']), axis=1)\n",
    "    \n",
    "    return d6['pred'].fillna(0), d6['matches']\n",
    "\n",
    "\n",
    "# Function for storing row indexes while making Giba-like matches (2-off predictions)\n",
    "def compile_leak_index(data, f, cols, extras, pair=False):\n",
    "    extra_cols = ['compiled_leak', 'compiled_idx', 'nonzero_mean']\n",
    "    max_nlags = len(cols)-2\n",
    "    \n",
    "    train_leak = data[['ID']]\n",
    "    train_leak['compiled_leak'] = 0\n",
    "    train_leak['compiled_idx'] = 0\n",
    "    train_leak['nonzero_mean'] = data[f].apply(lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1)\n",
    "    \n",
    "    leaky_cols = []\n",
    "    \n",
    "    if pair:\n",
    "        print '\\nMatching with one-lag assumption:'\n",
    "    else:\n",
    "        print '\\nMatching with two-lag assumption:'\n",
    "    \n",
    "    for i in range(max_nlags):\n",
    "        c = 'leaked_target_%s'%i\n",
    "        n = 'leaked_index_%s'%i\n",
    "        \n",
    "        print 'Processing Lag:', i\n",
    "        # Get predictions for current lag and store in new column\n",
    "        if pair:\n",
    "            train_leak[c], train_leak[n] = one_get_leak(data, list(cols), extras, i)\n",
    "        else:\n",
    "            train_leak[c], train_leak[n] = two_get_leak(data, list(cols), extras, i)\n",
    "        leaky_cols.append(c)\n",
    "        leaky_cols.append(n)\n",
    "        # Get \"grounding\" by joining with original training dataset\n",
    "        train_leak = train.join(train_leak.set_index('ID')[leaky_cols + extra_cols], \n",
    "                                on='ID', how='left')[['ID', 'target'] + leaky_cols + extra_cols]\n",
    "    \n",
    "    return train_leak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find leak values and leak indexes\n",
    "master_two_leak = compile_leak_index(train, fnames, cols, extra_features, pair=False)\n",
    "master_one_leak = compile_leak_index(train, fnames, cols, extra_features, pair=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leak_compiler(data, leak_val):\n",
    "    data['compiled_leak'] = 0\n",
    "    for i in range(leak_val):\n",
    "        c = 'leaked_target_%s'%i\n",
    "        t = 'leaked_index_%s'%i\n",
    "        zero_target = data['compiled_leak']==0\n",
    "        zero_index = data['compiled_idx']==0\n",
    "        data.loc[zero_target, 'compiled_leak'] = data.loc[zero_target, c]\n",
    "        data.loc[zero_index, 'compiled_idx'] = data.loc[zero_index, t]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat leak data\n",
    "two_leak = leak_compiler(master_two_leak, leak_val)\n",
    "one_leak = leak_compiler(master_one_leak, leak_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find tails\n",
    "coords_two = two_leak['compiled_idx'].values.tolist()\n",
    "tail_df = pd.DataFrame(data=coords_two, columns=['coord_1', 'coord_2'])\n",
    "tail_df = tail_df.loc[tail_df['coord_2'].notnull()]\n",
    "tail_intersect = np.intersect1d(tail_df['coord_1'], tail_df['coord_2'])\n",
    "\n",
    "tail_idx = tail_df['coord_2'].values[np.where(tail_df['coord_2'].values!=tail_intersect)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nonzero counts across rows\n",
    "nzr_count_train = np.count_nonzero(train[fnames], axis=1)\n",
    "nzr_count_test = np.count_nonzero(test[fnames], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for finding a unique value if it exists\n",
    "def row_checker(x):\n",
    "    uni_val, uni_cnt = np.unique(x, return_counts=True)\n",
    "    # Drop zero index\n",
    "    uni_val = uni_val[1:]\n",
    "    uni_cnt = uni_cnt[1:]\n",
    "    # Check if there's only 1 unique value\n",
    "    if uni_val.shape[0]==1:\n",
    "        return uni_cnt[0]\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-Level Metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of nonzero values in features\n",
    "f_nonzeros = np.count_nonzero(test[fnames], axis=0)\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 7))\n",
    "n, bins, patches = plt.hist(f_nonzeros, 100, alpha=0.5)\n",
    "plt.title('Test Set Nonzero Features')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Number of Non-Zero Entities')\n",
    "plt.savefig('./images/test_nonzero_count.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on the tail-end\n",
    "dense_features = fnames[np.where(f_nonzeros>=4000)[0]]\n",
    "print 'Number of dense features:', len(dense_features)\n",
    "print dense_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find densest rows amongst test set\n",
    "dense_test = test.loc[:, dense_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_nonzeros = np.count_nonzero(dense_test, axis=1)\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 7))\n",
    "n, bins, patches = plt.hist(r_nonzeros, 100, alpha=0.5)\n",
    "plt.title('Test Set Nonzero Rows')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Number of Non-Zero Entities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on tail-end\n",
    "dense_rows = dense_test.index.values[np.where(r_nonzeros>=50)[0]]\n",
    "print 'Number of dense rows:', len(dense_rows)\n",
    "print dense_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dense features and rows in a dataframe\n",
    "candidate_df = test.loc[dense_rows, dense_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Unique Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_in_dense(data, decimal=True, head_val=10):\n",
    "    vals = data.values.reshape((-1))\n",
    "    unique_vals, unique_counts = np.unique(vals, return_counts=True)\n",
    "    unique_dict = {'vals': unique_vals, 'counts': unique_counts}\n",
    "    unique_df = pd.DataFrame.from_dict(unique_dict, orient='columns')\n",
    "    unique_df.sort_values(by='counts', ascending=False, inplace=True)\n",
    "    if decimal:\n",
    "        return unique_df.head(head_val)\n",
    "    else:\n",
    "        return unique_df[np.invert(np.equal(np.mod(unique_df['vals'].values, 1), 0))].head(head_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at most common values in the dense test dataframe\n",
    "unique_in_dense(dense_test, decimal=False, head_val=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at most common values in the candidate dataframe\n",
    "unique_in_dense(candidate_df, decimal=True, head_val=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
